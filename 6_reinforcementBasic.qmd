---
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
jupyter: python3
---

# Basics of Reinforcement Learning(RL)
In this section, we outline the main ideas behind reinforcement learning and how they can be applied in the context of this thesis. The reader familiar with the material may skip this section. 



## A non mathematical, yet delicious example!

In Reinforcement Learning tasks, we are training an  *agent* that interacts with its *environment* by taking decisions. In this example, we are the agent, and the environment is the kitchen. Suppose we want to cook a delicious meal. At any point in time, we are making decisions such as;

- Which ingredients we use. Do we use tofu or seitan? Do we add spice or more chili pepper? When do we incorporate the sauce?
- Which cookware we use? Cast iron, or non-stick pan?
- Whether to put the oven to $200^\circ C$ or $220^\circ C$.
- Or simply do nothing!

All of these decisions, which we will call *actions* from now on, are taken based on the current *state* of the cooking process, following a certain *policy*, which is shaped by our previous cooking experience. 

After each action, the cooking process get to a new *state* and we get a *reward* that depend on how well we did. Maybe the food started to burn in which case we get a negative reward, or maybe we made the food better, in which case we get a positive reward. In this example, there is also a *terminal state*, in which we finished cooking and get to enjoy the meal. 

But how do we learn how to cook, how do we know what *action* to take at a specific *state*? That is, how do we learn the *policy*? We learn it by trying to make the food as flavorful as possible, which is defined by the *reward* we get after each action. Some of those rewards are immediate, for example, if we add some spices to our food and it tastes better.  We want to have a *policy* that maximizes the total *rewards* we get over a entire cooking session. This also mean that we have to balance how we prefer the immediate rewards against the future rewards. For example, adding a spice may make the meal taste better in the short term, for which we get a reward, but it may clash later when we add other ingredients, leading to a worse meal and worse *rewards* down the line.

Each time we cook, we learn what works and what doesn't, and remember that for any future time we cook. But, if we want to get better at cooking, we must not just repeat the *actions* that worked! We also have to take some risks, and *explore* the potential actions we can take at each state! On the other hand, we still need to rely and *exploit* what we know. There is a balance to find between *exploitation* and *exploration* so as to learn as fast as possible.




## Finite Markov decision process



We formalize the above example by defining a Markov decision process (MDP) @reinforcementBookShiyuZhao.

:::{#def-Markov_decision_process}
**(Markov decision process)**. 
A finite Markov decision process (MDP) is defined as a discrete time process, where we have:

- A finite state set $\mathcal{S}$.
- An finite action set $\mathcal{A}$, containing all possible actions.
- A reward set $\mathcal{R}(s,a)$, which contains the potential rewards received after taking  any action $a\in\mathcal{A}$ from any state $s\in\mathcal{S}$.

We use the notation $S_t, A_t$ as the state and action of the process at time $t$. The reward $R_t$ is the reward received at time t. $S_t,A_t$ and $R_t$ are random variables.


A Markov decision process also has a model, which consists of:

- The probability of getting from state $s$ to the state $s'$ by taking action $a$, which we call the state transition probability $p(s'|s,a) = \Pr(S_{t+1} = s' | S_t = s, A_t = a)$.
- The probability of getting reward $r$ by taking the action $a$ at a state $s$ $p(r|s,a) = \Pr(R_{t+1} = r | S_t = s, A_t = a)$. 

Furthermore, a Markov decision process has a policy that governs, for any state $s\in\mathcal{S}$, the probability of taking action $a\in\mathcal{A}$, that probability is $\pi(a|s) = \Pr(A_t = a|S_t = s)$.

Finally, a Markov decision process has the Markov property, or lack of memory. The action taken at instant t $A_t$ is only dependent on the current state $S_t$ and not the state before. Mathematically, $p(A_t|S_t, S_{t-1}, \dots , S_0) = p(A_t|S_t)$.

:::

An example of Markov decision process with two states can be seen in @fig-MDP_example.

![An example of Markov decision process with two states and two possible actions for each state. The dashed lines represent the model. After each action, a reward is given, here in dark red.](images/MDP.pdf){#fig-MDP_example width="40%"}


:::{.remark}

The state space $\mathcal{S}$ and the action space $\mathcal{A}$ can be finite or not. We only consider the case of finite Markov decision process to make matters easier. This also mean that the model is finite. 

The model in a Markov decision process is often impossible to define in advance. This problem is remedied by using *model free* algorithms.
:::

:::{#exm-grid_world}
A Markov decision process is the mathematical formalization of an agent interacting with its environment @Sutton1998. Consider the following grid, in which Leonardo the rabbit is in. Leonardo wants to get to the carrot.  

:::


## State Value and Bellman Equation

The Markov decision process are the mathematical formalization of an agent (for example a chess player) interacting with its environment (the chess board)@Sutton1998. 

We first define a trajectory. We denote by $S_t$ the state of an agent at instant $t$. Then, according to the policy, this agent takes the action $A_t$. After taking this action, the agent is now at the state $S_{t+1}$, and it gets the rewards $R_{t+1}$. Then the agent takes action $A_{t+1}$, and gets to a new state $S_{t+2}$ with reward $R_{t+2}$. This can continues indefinitely. We define the trajectory of an agent with starting state $S_t = s$ as the chain of state, actions and rewards from time $t$ to time $t+\tau$:

$$
S_t,A_t \to R_{t+1},S_{t+1},A_{t+1} \to R_{t+2},S_{t+2},A_{t+2} \to \cdots \to R_{t+\tau},S_{t+\tau},A_{t+\tau},
$$

where $\tau$ can either be finite or infinite. Note that due to the Markov property, the value of $t$ is not important. 


:::{.remark}
In some environments, it is natural for the agent to have a task that has a starting state and a finishing states (for example, beginning a cooking session and finishing it, or starting a game and winning/losing at it.) We call these tasks *episodic tasks* and in these cases, a finite trajectory $S_0,A_0 \to \dots \to S_T$ is also called an *episode*. In the cases where the task is such that no such state can be defined, a trajectory is not finite and we call these tasks *continuing tasks*, which will be the case in this thesis.

:::

The goal of any reinforcement learning algorithm is to maximize the rewards the agent get, which we elaborate on in the next sections. 
We now define the discounted return along a trajectory,


:::{#def-discount}
Let $t = 0, 1, \dots$. The (discounted) return along the trajectory $S_t,A_t \to S_{t+1},A_{t+1} \to S_{t+2},A_{t+2} \to \dots$ is the random variable given by

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{+\infty}\gamma^k R_{t+1+k},
$$

where $\gamma \in [0,1)$ is called the discount rate. 
:::

:::{.remark}
By setting a discount rate that is less than 1 in continuing tasks, we make sure that the discounted return is well defined in the case of bounded rewards. Indeed, if, for any $t$, $|R_t|\leq M$, then $\sum_{k=0}^{+\infty}|\gamma^k R_{t+1+k}| \leq  \sum_{t=0}^{+\infty}\gamma^t M = \frac{M}{1-\gamma}$, so the series converges absolutely.
:::

The discounted return is thus the sum of rewards along a trajectory, with a penalty for rewards far in the future. The discount rate is chosen depending on whether we want the agent to favor short term rewards, in which case a discount rate closer to $0$ can be chosen, or long term rewards, with a discount rate closer to $1$.

Since the return is a random variable, we can look at its expectation, in particular, we are interested in its conditional expectation, given a starting state $S_t = s$. This expectation is called the state value.




:::{#def-state_value}
**State value**@Sutton1998 The state value of a state $s$ is the function, defined for any $s\in\mathcal{S}$ as:

$$
v_\pi(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t],
$$

where $\pi$ is a given policy. 
:::

:::{.remark}
The Markov property of the MDP means that the state value does not depend on time.
:::

The objective is thus to find a policy $\pi$ that maximizes the state values. We next derive the Bellman equation. 

We have


\begin{align}   
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \nonumber\\
&=R_{t+1} + \gamma \left(R_{t+2} + \gamma R_{t+3}+ \dots \right) \nonumber\\
&=R_{t+1} + \gamma G_{t+1}.
\end{align}



 
This expression of the return can be used in conjunction with the definition of the state value above to get

$$
v_\pi(s) = E[G_t|S_t = s]= E[R_{t+1}| S_t = s] + \gamma E[G_{t+1} | S_t = s].
$${#eq-state_value_part1}

The first term is the expectation of immediate reward, following a certain policy $\pi$, the second is the expectation of future rewards. Let us expand on that formula a bit more. We use the law of total expectations on the first part of the RHS to get

$$
E[R_{t+1}| S_t = s] = E\big[E[R_{t+1}|S_t = s,A_t]\big] = \sum_{a\in\mathcal{A}}\pi(a,s)\sum_{r\in\mathcal{R}}rp(r|s,a),
$$

where $\mathcal{R} = \mathcal{R}(s,a)$ is the set of possible rewards one can get by taking action $a$ at state $s$. 

We now develop the second part of the RHS of the equation to get

$$
E[G_{t+1} | S_t = s] = E\big[E[G_{t+1} | S_t = s , S_{t+1}]\big] = \sum_{s'\in\mathcal{S}}p(s'|s)E[G_{t+1}|S_t = s, S_{t+1} = s'],
$$

where $p(s'|s) = \sum_{a\in\mathcal{A}} p(s'|s,a)\pi(a,s)$ is the probability of the next state being $s'$ if the current state is $s$. Because of the Markov property of the MDP, we can remove the conditioning $S_t = s$ and thus, $E[G_{t+1}|S_t = s, S_{t+1} = s'] = E[G_{t+1}|S_{t+1} = s] = v_\pi(s')$. Then 
$$
E[G_{t+1} | S_t = s] = \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A}}v_\pi(s')\pi(a|s)p(s'|s,a).
$${#eq-state_value_part2}

Putting @eq-state_value_part1 and @eq-state_value_part2 together, we get Bellman's equation:


$$
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a,s)\left[\sum_{r\in\mathcal{R}}rp(r|s,a) + \gamma\sum_{s'\in\mathcal{S}}v_\pi(s')p(s'|s,a) \right].
$$ {#eq-Bellman}

:::{.remark}
The Bellman equation is dependent on the given policy and gives a recursive relation for the state values. Solving this equation is called policy evaluation which involves fixed point iterations(see example below). 
:::


:::{#exm-the_example}

We can derive directly the state values in the MDP in @fig-MDP_example. In particular, for the state $s_2$. There are two possible actions $a_0$ and $a_1$ we can take. The policy is to take action $a_0$ with a probability $0.6$, and action $a_1$ with a probability $0.4$. When we take  for example action $a_0$, the probability of the next state being $s_1$ is $0.3$, in which case the reward is $5$. Proceeding similarly for all the possible actions and rewards, we get

$$
v_\pi(s_2) = 0.6 \left[0.7\times-2 + 0.3 \times 5 + \gamma(0.3v_\pi(s_1) + 0.7v_\pi(s_2))\right] + 0.4 \times [\dots].
$$

After some computations, we end up with 

$$
v_\pi(s_2) = 1.5 + \gamma (0.5,0.5)\begin{pmatrix}
v_\pi(s_1) \\
v_\pi(s_2)
\end{pmatrix}.
$$

Similarly $v_\pi(s_1) = 4.1 + \gamma(0.9,0.1)(v_\pi(s_1),v_\pi(s_2))^\intercal$. This leads to the system:

$$
\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)
\end{pmatrix} = \begin{pmatrix}
4.1\\
1.5
\end{pmatrix} + \gamma \begin{pmatrix}
0.9 & 0.1\\
0.5 & 0.5
\end{pmatrix}\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)
\end{pmatrix}.
$$

We stop here to remark that this equation is of the form $v_\pi = r_\pi + \gamma P_\pi v_\pi$. $P_\pi$ is row stochastic, and is the transition matrix of the Markov chain we get when we consider a given policy. Furthermore, since $\gamma<1$, we motivate solving the equation by using a fixed point iteration. This is the main idea behind *dynamic programming* @bellman1957dynamic. In this case, we can simply solve the system directly. With $\gamma= 0.5$, we have the state values $v_\pi(s_1) = 7.875$, $v_\pi(s_2) = 4.625$. There is, for the given policy, more value in being in the first state than the second. 

```{python}
#| echo: false
#| output: false
import numpy as np
gamma = 0.5

matrix = np.array([[0.9*gamma-1,0.1*gamma],[0.5*gamma,0.5*gamma-1]])
vector2 = np.array([-4.1,-1.5])

v = np.linalg.solve(matrix,vector2)
print(v)


```


:::





## Action Value

The state value gives information about a specific state, however, we are also often interested in knowing how much we stand to gain by taking a particular action at a particular state. This lead to the definition of the action value.


:::{#def-action_value}
**Action value** 
The action value is defined as 

$$
q_\pi(a,s) = E\left[G_t|A_t=a,S_t=s\right].
$$
:::


We also have, from @def-state_value, and the law of total expectation,

$$
v_\pi(s) = E[G_t|S_t = s] = E\left[E[G_t|S_t = s, A_t = a]\right].
$$

Then,
$$
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a,s)E\left[G_t|S_t=s,A_t =a\right],
$$

and we can get the relation between state value and action value:

$$
v_\pi(s) = \sum_{a\in\mathcal{A}} \pi(a,s)q_\pi(a,s).
$${#eq-state_value_action_relation}


