## Abstract {.unnumbered}

Under some stability condition on a matrix $A$, solving the linear system $Ax = b$ can be done by solving the initial value problem $x' = Ax-b$. In this bachelor thesis, we study the use of this method to solve linear systems which arise from the discretization via finite differences of the one dimensional, steady state convection diffusion equation. These systems depend on two problem parameters.
To solve these initial value problems, we use a two stages explicit Runge-Kutta method with two parameters to chose, which we aim to optimize using reinforcement learning. 

Using policy gradient methods, and in particular the REINFORCE algorithm, we find that with some care, reinforcement learning can be used to learn the 
solver parameters as a function of the two problem parameters. These results are however tempered by some limitations, as the solver can diverge in certain cases, and convergence speed remains low in general. Some different approach to using reinforcement learning are thus suggested.

## Acknowledgements {.unnumbered}


I would like to express my deepest thanks to my supervisor Philipp Birken at the university of Lund for the regular discussion sessions, and without which I could not have written this thesis.

I would also like to thank my partner Sarah (who also fixed an issue I struggled with for weeks in a matter of minutes), for being an amazing partner who is always there to help and encourage me in all aspects of my life. My gratitude also goes to our cat Alyx for her emotional support and the deep talks we have. 

Finally, I would like to thank Paulina Ibek, who worked on a similar thesis at the same time, for the discussions which led to exchanging ideas.