---
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
jupyter: python3
---

# Policy gradient methods

We introduce policy based method in this chapter.

## The test problem, in a RL framework

As a reminder, we have a test problem with the following problem parameters:

- a parameter $b \in [0,1]$ in the steady-state convection diffusion equation, and

- a discretization parameter $n\in\mathbb{N}$ defining the number of points in the linear grid used to solve numerically the equation.

We end up with a linear equation to solve for, which can be solver using the method highlighted before. We wish to find the solver parameters $\Delta t$ and $\alpha$ that will minimize the residual of REF as fast as possible. To simplify future computation, we will be interested in minimizing the residual ratio after 10 iteration of the Runge-Kutta solver $c_{10}$. We define this ratio as $c_{b,n}(\Delta t, \alpha)$, a function parametrized by $b$ and $n$, with arguments $\Delta t$ and $\alpha$. We are faced with the following optimization problem:


For any $b$, $n$, find
$$
(\Delta t^*, \alpha^*) =  \arg \min_{\Delta t, \alpha} c_{b,n}(\Delta t, \alpha).
$$


We are interested in using reinforcement learning to solve this problem. The last section provided an overview of the elements of reinforcement learning, and we can now translate our problem in a RL setting.

- A individual state can be defined as a pair $s = (b,n)$.
- An individual action can be defined as a pair $a = (\Delta t, \alpha)$.
- Given a state $s$, the action chosen depend on the policy $\pi(a = (\Delta t, \alpha) |s = (b,n))$. This policy can be deterministic or stochastic.
- Once a state-action pair is chosen, the residual ratio is computed. The reward can then be defined as a function of calculated residual ratio, which is defined in the next section. 
 
The state transition model is more difficult to find a direct translation for. For the purpose of this thesis, the next state is chosen at random after computing an action and a reward. This is not ideal.  

There are still several challenges that need to be addressed.

- State transition being random makes for a poor model to apply reinforcement learning to. In a typical RL scenario, the discount rate is usually set close to 1 as the agent need to take into account the future states it will be in. Here, the next state is independent on the action taken, so it makes no sense to set the discount rate high. As a consequence, we set it low.
- In our problem, the State-Action space is continuous. We previously assumed finite spaces.
- In the definition of a MDP, the reward is a modeled random variable. This is not the case here, as we do not know in advance how the solver will behave.

The first challenge is inherent to the way we defined the problem. We answer the last two challenges in the next sections. 

## Computing the reward.

Once a state and action is chosen, the reward need to be computed. We said before that, for each state and action, we compute the residual ratio after 10 iterations $c_{10}$. With that ratio, we need to define an appropriate reward metrics. We design the reward such that:

- The lower the ratio $c_{10}$, the better the convergence rate and the better the reward should be.
- It appears natural to have a positive reward when $c_{10}<1$, which implies convergence, and a negative reward otherwise.

 The reward is 

$$
r(c_{10}) = \begin{cases}
100\times (1-c_{10}) \;\;\;\;\;\qquad \text{ if } c_{10}<1\\
\max(-10,1 - c_{10}) \qquad \text{ if } c_{10}\geq1
\end{cases}
$$

When $c_{10}<1$, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. Because the ratio tends to be very close to $1$, we multiply everything by $100$, adding more contrast to the rewards. 

When, on the other hand $c_{10}\geq 1$, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at $-10$.



## Model based, model free

One problem we are faced with is the problem of the model. In the last section, we assume that both $p(s'|s,a)$ and $p(r|s,a)$ are known. Depending on the problem, this is not straightforward to define. Thankfully, the model can be empirically estimated via Monte Carlo methods. 

In particular, we often have to compute expectation of random variables. The most basic method is simply to sample the desired random variable and to use the empirical mean as an estimator of the desired expectation. Stochastic estimation is also used in numerous reinforcement learning algorithm. 


## Dealing with a large state-action space.

In the last chapter, we made the assumption that the every space, be it state, action, or reward is finite. However, this is in practice not always the case, as some state may be continuously defined for example. Even if those spaces are discrete, the *curse of dimensionality* (TODO, should something be cited) may not allow us to efficiently represent every state or action. 

We take our problem as formulated before. The state is defined as the problem parameters, that is $b\in[0,1]$ and $n = 1 , 2, \dots$. Without any adjustment, the state space is of the form $[0,1] \times \mathbb{N}$, and is not finite. 

Similarly, the policy is defined by choosing the values $(\alpha,\Delta t) \in [0,1]\times \mathbb{R}^+$, depending on the state. Once again, the action space is continuous. 

One approach would be to discretize the entire state $\times$ action space, and then to apply classical dynamic programming algorithm to get some results. Then, after an optimal policy is found, do some form of interpolation for problem parameters outside of the discretized space. This approach has its own merit, as there is 3 dimensions that need to be discretized, and $n$ can be chosen within a finite range. The main issue is that since there are no relationship between the states, solving the resulting Bellman optimal equation is effectively close to brute forcing the problem. (//TODO, this need a stronger argument instead of "My intuition said so".)

Another approach is to use approximation function. A common approach is to approximate the value function $v(s)$ by some parametrization $v(s) \approx \hat{v}(s,\omega)$ where $\omega \in \mathbb{R}^d$ are $d$ parameters. Such methods are called *value based*. The method we use in this thesis, on the other hand, use an approximation of the policy function defined as $\pi(a|s,\theta)$, where $\theta\in \mathbb{R}^d$ is a parameter vector is dimension $d$. Such method are called *policy based*. The reason to chose from this class of algorithm is two-fold.

- When thinking about the test problem, one approach which appears natural is to chose the solver parameters as a linear function of the problem parameters. A policy based approach allow us to do exactly this. 

- A challenge that we are faced with is the poor model of state transition. Choosing such a linear policy allow us to find some relations between the states. 


:::{.remark}

Approximation is usually done using neural networks, building on the universal approximation theorem(@HORNIK1989359). In our case, a linear approximation is used.

:::

## Policy gradient methods.




### Objective function.

We apply a policy gradient method to our problem. Let $\theta \in \mathbb{R}^d$ be a parameter vector and $\pi(a|s,\theta) = p(A_t = a | S_t = s , \theta)$ an approximate policy that is derivable w.r.t $\theta$. We want to define an objective function $J(\theta)$ that we want to maximize in order to find the best value of $\theta$.

 To this end, we make the following assumptions, which are specific to our problem. For simplicity, we restrict ourselves to the discrete case. 

- The states are uniformly distributed. That is, for any $s\in\mathcal{S}, p(S_t = s) = 1 / |\mathcal{S}|$, where $|\mathcal{S}|$ is the number of element of $S$. This correspond to the idea of taking a new state at random in our problem. 

We define the objective function

$$
J(\theta) = \overline{v_\pi(S)} = \frac{1}{|\mathcal{S}|}\sum_{s\in S}v_\pi(s)
$$

that is, $J(\theta)$ is the average, (non weighted, as per assumption) state value.

We want to maximize this objective function. To this end, we use a gradient ascend algorithm of the form

$$
\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta).
$$

We are faced with the immediate issue that the algorithm requires knowing the gradient of the objective function.  

### Policy gradient theorem

We prove, using the aforementioned assumptions the policy gradient theorem. This proof is adapted from [@Sutton1998,chap 13.2]. 

Using the expression //TODO: REF, the expression for a specific state value can be written as

$$
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(a,s)
$$

We take the gradient w.r.t $\theta$ to get
$$
\nabla v_\pi(s) = \sum_{a\in\mathcal{A}}\nabla\pi(a|s)q_\pi(a,s) + \pi(a|s)\nabla q_\pi(a,s)
$$

Using the expression //REF for the action value we get 

$$
\nabla q_\pi(a,s) = \nabla \left[ \sum_{r}p(r|s,a)r + \gamma\sum_{s'}p(s'|a,s)v_\pi(s')\right]
$$

It is apparent that the first part of the RHS is not dependent on $\theta$, therefore, and neither is the state transition probability, the gradient becomes

$$
\nabla q_\pi(a,s) =  \gamma\sum_{s'}p(s'|a,s)\nabla v_\pi(s').
$$

By assumption $p(s'|a,s) = 1/|\mathcal{S}|$, and thus 

$$
\nabla q_\pi(a,s) =  \gamma\sum_{s'}\frac{1}{|\mathcal{S}|}\nabla v_\pi(s').
$$

We recognize the expression for the gradient of the metric to get $\nabla q_\pi(a,s) = \gamma \nabla J(\theta)$.
$$
\nabla v_\pi(s) = \sum_{a\in\mathcal{A}}\nabla\pi(a|s)q_\pi(a,s) + \gamma\pi(a|s)\nabla J(\theta)
$$

Since the policy $\pi(a|s)$ is a probability over the action space, it sums to $1$ and we can get the second part of the RHS out of the sum

$$
\nabla v_\pi(s) = \gamma J(\theta) + \sum_{a\in\mathcal{A}}\nabla\pi(a|s)q_\pi(a,s)
$$

Using $\nabla J(\theta) = \frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}\nabla v_\pi(s)$, we get


\begin{align}
\nabla J(\theta) &= \frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}\left[\gamma J(\theta) + \sum_{a\in\mathcal{A}}\nabla\pi(a|s)q_\pi(a,s)\right] \\
&= \gamma \nabla J(\theta) + \sum_{s\in\mathcal{S}}\frac{1}{|\mathcal{S}|}\sum_{a\in\mathcal{A}}\nabla\pi(a|s)q_\pi(a,s)
\end{align}


And after a small rearrangement of the terms

$$
\nabla J(\theta) = \frac{1}{1-\gamma}\sum_{s\in \mathcal{S}}\frac{1}{|\mathcal{S}|}\sum_{a\in\mathcal{A}}\nabla\pi(a|S)q_\pi(a,S)
$$

This is an expression of the policy gradient theorem.

### REINFORCE algorithm 

Here is the pseudocode.