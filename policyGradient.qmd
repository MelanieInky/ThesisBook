---
title: "Policy gradient methods"
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
        include-in-header: 
            text: |
                \usepackage{amsmath}
                \usepackage{easy-todo}
                \usepackage{amsthm}
        documentclass: article
        fontsize: "14"
jupyter: python3
---

## The test problem, in a RL framework

Here's the rundown

Several problem 

- State transition being random defeats the point of RL.
- State-Action is continuous
- 


## Model based, model free

One problem we are faced with is the problem of the model. In the last section, we assume that both $p(s'|s,a)$ and $p(r|s,a)$ are known. Depending on the problem, this is not straightforward to define. Thankfully, these model can be empirically estimated via Monte Carlo methods. 

In particular, we often have to compute expectation of random variables. The most basic method is simply to sample the desired random variable and to use the empirical mean as an estimator of the desired expectation. Stochastic estimation is also used in numerous reinforcement learning algorithm. 


## From discrete to continous.

In the last chapter, we made the assumption that the every space, be it state, action, or reward is finite. However, this is in practice not always the case, as some state may be continuously defined for example. Even if those spaces are discrete, the *curse of dimensionality* may not allow us to efficiently represent every state or action. 

We take our problem as formulated before. The state is defined as the problem parameters, that is $b\in[0,1]$ and $n = 1 , 2, \dots$. Without any adjustment, the state space is of the form $[0,1] \times \mathbb{N}$, and is not finite. 

Similarly, the policy is defined by choosing the values $(\alpha,\Delta t) \in [0,1]\times \mathbb{R}^+$, depending on the state. Once again, the action space is continuous. 

One approach would be to discretize the entire state $\times$ action space, and then to apply classical dynamic programming algorithm to get some results. Then, after an optimal policy is found, do some form of interpolation for problem parameters outside of the discretized space. 

Another approach is to use approximation function. A common approach is to approximate the value function $v(s)$ by some parametrization $v(s) \approx \hat{v}(s,\omega)$ where $\omega \in \mathbb{R}^n$ are $n$ parameters. Such methods are called *value based*. The method we use in this thesis, on the other hand, use an approximation of the policy function defined as $\pi(a|s,\theta)$, where $\theta\in \mathbb{R}^n$ is a parameter vector is dimension $n$. Such method are called *policy based*. The reason to chose from this class of algorithm is two-fold.

- When thinking about the test problem, one approach which appears natural is to chose the solver parameters as a linear function of the problem parameters. A policy based approach allow us to do exactly this. 

- I know I had other reasons, but it's easy to implement may not be a good one.

- Using approximation allow use to extrapolate better. 


:::{.remark}

Approximation is usually done using neural networks, building on the universal approximation theorem(@HORNIK1989359). 

:::