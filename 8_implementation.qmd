---
jupyter: python3
---


# Implementation



## A Linear Approximation of the Policy


We want to have a policy of the form $(\Delta t, \alpha) =A(b,n)' + \text{c}$, where $A$ is a two by two matrix and $c$ a 2-vector.

We define the following stochastic policy, define first

$$
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} = 
\begin{pmatrix}
\theta_0 & \theta_1\\
\theta_2 & \theta_3
\end{pmatrix} 
\begin{pmatrix}
b\\
n
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix}
$$

Then we chose the random policy $\alpha \sim \mathcal{N}(\mu_\alpha,\sigma^2)$,and similarly $\Delta t \sim \mathcal{N}(\mu_{\Delta t}, \sigma^2)$. The term $\sigma^2$, which is the variance of the policy is chosen fixed, and will help us balance exploration vs exploitation. Since $\alpha$ and $\Delta t$ are chosen independently, the joint probability density of both parameters is the product of both marginal pdf, that is 

$$
f(\alpha,\Delta t) = f_{1}(\alpha)\cdot f_{2}(\Delta t)
$$

where 
$$
f_1(\alpha) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}\right)
$$

and similarly,

$$
f_2(\Delta t) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)^2}{2\sigma^2}\right)
$$.

Taking the logarithm, we get $\ln(f(\alpha,\Delta t) = \ln(f_1(\alpha)) + \ln(f_2(\Delta t))$. Thus,

$$
\ln(f_1(\alpha)) = \ln(\frac{1}{\sqrt{2\pi}\sigma}) - \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2} 
$$.

We now take the gradient w.r.t $\theta$ to get

$$
\nabla_\theta \ln(f_1(\alpha)) = \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T
$${#eq-nabla_xi_alpha}

where $\xi_\alpha = \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)}{\sigma^2}$.

Doing a similar thing with $\Delta t$, we get the gradient,

$$
\nabla_\theta \ln(f_2(\Delta t)) = \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
$${#eq-nabla_xi_delta_t}

where $\xi_{\Delta t} = \frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)}{\sigma^2}$
We now add both gradients together to we get the gradient of the policy, for a specific action $a = (\alpha, \Delta t)$ and state $s=(b,n)$,

$$
\nabla_\theta \ln\pi(a|s,\theta) =  \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T+ \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
$$ {#eq-policyGradient}

We used here the standard notation. That is, $\pi(a|s,\theta) = f(\alpha,\Delta t)$.






:::{.remark}
One may remark that the REINFORCE algorithm uses a discrete policy space. This is not an issue. Instead of using the probability mass function of the policy, we will instead use the probability density function as a substitute(@lillicrap2019continuous). The fact that the pdf admits values $>1$ is not an issue as we adjust the learning rate accordingly.
:::

## State Space

In the test problem, the $b$ parameter is a physical parameter and can take any value in $[0,1]$, while the value of $n$ depends on the discretization of the differential equation, and can be any integer value. As the value of $n$ get bigger, computation of a single time step in the RK solver can take quite a bit more time, and we will be doing a lot of those computations in the coming section! Therefore, we decide to limit the value of $n$ to an arbitrary maximum of $200$. We also cap the minimum value of $n$ to an arbitrary minimum of $5$ as those values are simply too low to get a acceptable discretization error, and we do not want to train an agent to solve theses states. 

In the last chapter, we wrote that the state transitions were random, more precisely, this now means that a state transition is defined as

- choosing a new value of $b\sim \mathcal{U}(0,1)$, and 
- choosing a new value of $n$ randomly, between $5$ and $200$.  

## Computing the reward.

Once a state and action is chosen, the reward need to be computed. We said before that, for each state and action, we compute the residual ratio after 10 iterations $c_{10}$. With that ratio, we need to define an appropriate reward metrics. We design the reward such that:

- The lower the ratio $c_{10}$, the better the convergence rate and the better the reward should be.
- It appears natural to have a positive reward when $c_{10}<1$, which implies convergence, and a negative reward otherwise.

 The reward is 

$$
r(c_{10}) = \begin{cases}
500\times (1-c_{10}) \;\;\;\;\;\qquad \text{ if } c_{10}<1\\
\max(-3,1 - c_{10}) \qquad \text{ if } c_{10}\geq1
\end{cases}
$$

When $c_{10}<1$, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. Because the ratio tends to be very close to $1$, we multiply everything by $500$, adding more contrast to the rewards. 

When, on the other hand $c_{10}\geq 1$, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at $-3$.



## Implementation of the REINFORCE algorithm.

Now that everything as been defined, the REINFORCE algorithm can be applied to find an optimal policy.

### A first experiment

We implement the REINFORCE algorithm to the test problem. There are a few hyperparameters to set.

- The learning rate is set to $\alpha=2\times10^{-8}$.
- The discount rate is set to a low value of $\gamma = 0.1$, as the state transitions are completely random, there is no reason to prefer long term rewards.
- Because the discount rate is so low, the episodes length is set to $20$ as we want to use the updated policy as often as possible.
- The standard deviation of the policy parameters is set to $\sigma = 0.1$.

This leaves the choice of the initial value for $\theta$. While it is possible for the parameters to be random, or all set to 0, we use the experiment done in chapter 4 to use. In @fig-resratio10, it seems that a policy of $\alpha = 0.3$ and $\Delta t = 2$ is reasonable. Since this was done only for a single set of problem parameters, we have no idea of the relationship between problem parameters and optimal solver parameters. Therefore, we only set the parameter $\theta_4 = 0.2$, and $\theta_5=2$, the other parameters are set to 0.

The algorithm is run for 40000 episodes, and we observe the evolution of the theta parameters(@fig-experimentreward). The objective function $J(\theta)$ can not be observed. We have access, however to the average reward over an episode in @fig-experimenttheta. Note that in the latter plot, the rolling average of the average reward over the last $k=1000$ episodes is used as a smoother.

```{python}
#| label: fig-experimenttheta
#| echo: false
#| fig-cap: Evolution of the theta parameters with episode number. The hyperparameters are $\sigma$=0.1, learning rate 2e-8.
#| warning: false

import polars as pl
import matplotlib.pyplot as plt

import polars as pl
import seaborn as sns
from plotnine import *



fileName = 'experiment_data/exp1/log.csv'
df = pl.read_csv(fileName)
df2 = df.melt( 
    id_vars=['ep_number','avg_ep_reward'], 
    value_vars=['theta_0', 'theta_1', 'theta_2', 'theta_3', 'theta_4', 'theta_5']
    ).with_columns(
        (pl.col('ep_number')/1000).alias('episode_no_1000')
    )

df3 = df.select('ep_number', 'avg_ep_reward').with_columns(pl.col('avg_ep_reward').rolling_mean(window_size=1000).alias('rolling reward'))



p1 = (ggplot(df2, aes('episode_no_1000','value'))
+ geom_line()
+ facet_wrap('~variable', scales='free_y')
+ theme_bw()
+ theme(panel_spacing_x= 0.7)
+ xlab('Episode no. / 1000')
+ ylab('Theta')
)

fig = p1.draw()
fig.show()
```

```{python}
#| label: fig-experimentreward
#| echo: false
#| fig-cap: Evolution of the rolling average (k=1000) of the average episode reward.
#| warning: false


p2 = (ggplot(df3, aes('ep_number','rolling reward'))
+ geom_line()
+ theme_bw()
+ xlab('Episode number'))

fig = p2.draw()
fig.show()
```


The reward in @fig-experimentreward is trending upward, which is the intended behavior of the algorithm. However, there are certain problems that have been made apparent.

- Despite running the algorithm for a long time(approximately 20 minutes on a typical computer), the $\theta$ parameters have not changed very much, and it is clear that we are far from any convergence of the reward function. 
- Even with smoothing, it is apparent that the method has a high variance.
- It seems that $\theta_1$ and $\theta_5$ varies quite a bit over time whereas the other parameters have a steady rate of change.

The slow apparent convergence rate can not be mitigated by a higher learning rate, as this empirically lead to divergences issues. The high variance is typical of reinforcement learning tasks, as well as Monte Carlo based methods, which REINFORCE is a part of. That being said, there exists much better methods that can reduce this variance, at the expense of introducing some bias, such as for example actor-critics methods [@Sutton1998, chap. 13.5], or proximal policy optimization(PPO) @DBLP:journals/corr/SchulmanWDRK17. Both of these methods are not explored in this thesis.



### Gradient ascent with different learning parameters. 

#### An example of gradient descent. 

Consider the objective function $f(x,y) = x^2 + 3y^2$. The function admits a global minimum at $x = y = 0$, and its gradient is 

$$
\nabla f(x,y) = (2x,6y)'
$$

Therefore, the gradient descent iteration, with learning rate $\alpha>0$ is the iteration

$$
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \alpha \begin{pmatrix}
2x_t\\
6y_t
\end{pmatrix}
$$.

That is $x_{t+1} = (1-2\alpha)x_t$ and $y_{t+1} = (1-6\alpha)y_t$. The algorithms converge to $x=y=0$ if and only if $\alpha<1/3$. If however, $\frac{1}{3}<\alpha<1$, we will have convergence for $x$, but not for $y$.

The reason for this is that the gradient is steeper in the $y$ direction than the $x$ direction, which leads to comparatively bigger change in $y$ than $x$ in the gradient descent iterations.

To remedy this, we can set a variable learning rate that is different depending on the direction. Set $\alpha_x>0$ and $\alpha_y>0$ as the learning rate in the respective gradient direction $x$ and $y$. The gradient ascent iteration is then defined as

$$
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \begin{pmatrix}
\alpha_x & 0\\
0 & \alpha_y
\end{pmatrix}
\begin{pmatrix}
2x_t\\
6y_t
\end{pmatrix}.
$$

That is, $x_{t+1} = (1-2\alpha_x)x_t$ and $y_{t+1} = (1-6\alpha_y)y_t$. With $\alpha_x = \frac{1}{2}$ and $\alpha_y = \frac{1}{6}$, the gradient descent algorithm can now converge to $0$ in a single iteration!

#### Implementation.

We have an expression for the policy of the gradient of the policy in @eq-policyGradient. In particular, in @eq-nabla_xi_alpha and @eq-nabla_xi_delta_t, we remark that the gradient value in the directions of $\theta_1$ and $\theta_3$ have a similar expression. Namely, it depends strongly on $n$. Similarly, $\theta_0$ and $\theta_2$ depends on $b$, and $\theta_4$ and $\theta_5$ depend on neither. However, $b$ can only take values between $0$ and $1$, which motivates the idea that the gradient may be in practice way steeper in the $\theta_1$ and $\theta_3$ directions than the other one. This idea would also explain the oscillations that happen in @fig-experimenttheta. Therefore, we decide to add a variable learning rate depending on the direction. The update step in REINFORCE algorithm with fixed learning rate is

$$
\theta \leftarrow \theta + \alpha \nabla_\theta \pi(a|s,\theta).
$$


We change this update operation to 

$$
\theta \leftarrow \theta + \alpha_{\text{var}} \odot \nabla_\theta \pi(a|s,\theta)
$$

where $\alpha_{\text{var}} = (\alpha_{0}, \alpha_{1}, \dots , \alpha_{5})'$ and $\odot$ represent the termwise vector multiplication. Based on this, we set 

$$
\alpha_{\text{var}} = (\alpha_0,\alpha_1, \dots, \alpha_5)' = (1,1/50,1,1/50,1,1)
$$

so as to slow down the learning of the parameters "dependent" on $n$. The 

## Exploration vs exploitation tradeoff.

Changing $\sigma^2$ to a higher value makes for a flatter gradient, so not only are we taking more risks, we can actually afford a higher learning rate without exploding!




## Impact of initial condition.

Gradient based algorithm have a tendency to converge to local minima. (in our case maxima, but same thing really), therefore, it would be interesting to see how initial policy impacts the learned policy now that we have convergence.
(Not done yet, but I suspect some nasty surprises!)


## Moving beyond the basics.

(Some discussion on what to do next, etc... ) Example include.

- Better algorithm, with less sample variance and more sample efficiency.
- Moving beyond a linear policy. (approximation using NN).
- Meta learning, maybe?
- Applying the concept learned here to a more varied set of problem, instead of just confining ourselves to steady state diffusion-convection equation.


