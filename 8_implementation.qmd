---
jupyter: python3
---


# Implementation

```{python}
#| echo: false


##IMPORTS
import polars as pl
import matplotlib.pyplot as plt

import polars as pl
import seaborn as sns
from plotnine import *
```


## A linear approximation of the policy {#sec-linear_policy}


We want to have a policy of the form $(\Delta t, \alpha) =A(b,n)' + \text{c}$, where $A$ is a two by two matrix and $c$ a 2-vector.

We first define the deterministic policy 

$$
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} = 
\begin{pmatrix}
\theta_0 & \theta_1\\
\theta_2 & \theta_3
\end{pmatrix} 
\begin{pmatrix}
b\\
n
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix}.
$${#eq-policy_deterministic_part}

As we need a stochastic policy in the REINFORCE algorithm, we add some Gaussian noise to the policy $\alpha \sim \mu_\alpha + \mathcal{N}(0,\sigma^2)$,and similarly $\Delta t \sim \mu_{\Delta t} +  \mathcal{N}(0, \sigma^2)$. The term $\sigma^2$, which is the variance of the policy is chosen fixed in this thesis. Since $\alpha$ and $\Delta t$ are chosen independently, the joint probability density of both parameters is the product of both marginal pdf, that is 

$$
f(\alpha,\Delta t) = f_{1}(\alpha)\cdot f_{2}(\Delta t),
$$

where 
$$
f_1(\alpha) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}\right),
$$

and similarly,

$$
f_2(\Delta t) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)^2}{2\sigma^2}\right).
$$.

Taking the logarithm, we get $\ln(f(\alpha,\Delta t) = \ln(f_1(\alpha)) + \ln(f_2(\Delta t))$. Thus,

$$
\ln(f_1(\alpha)) = \ln(\frac{1}{\sqrt{2\pi}\sigma}) - \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}.
$$.

We now take the gradient w.r.t $\theta$ to get

$$
\nabla_\theta \ln(f_1(\alpha)) = \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^\intercal,
$${#eq-nabla_xi_alpha}

where $\xi_\alpha = \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)}{\sigma^2}$.

Doing a similar thing with $\Delta t$, we get the gradient,

$$
\nabla_\theta \ln(f_2(\Delta t)) = \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^\intercal,
$${#eq-nabla_xi_delta_t}

where $\xi_{\Delta t} = \frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)}{\sigma^2}$.
We now add both gradients together to we get the gradient of the policy, for a specific action $a = (\alpha, \Delta t)$ and state $s=(b,n)$:

$$
\nabla_\theta \ln\pi(a|s,\theta) =  \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T+ \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^\intercal.
$$ {#eq-policyGradient}








:::{.remark}
One may remark that the REINFORCE algorithm uses a discrete policy space. This is not an issue. Instead of using the probability mass function of the policy, we will instead use the probability density function as a substitute(@lillicrap2019continuous). The fact that the probability density function admits values $>1$ is not an issue as we can adjust the learning rate accordingly. 
:::

## State Space

In the test problem, the $b$ parameter is a physical parameter and can take any value in $[0,1]$, while the value of $n$ depends on the discretization of the differential equation, and can be any integer value. As the value of $n$ get bigger, computation of a single time step in the RK solver can take quite a bit more time, and we will be doing a lot of those computations in the coming section! Therefore, we decide to limit the value of $n$ to an arbitrary maximum of $200$. We also cap the minimum value of $n$ to an arbitrary minimum of $5$ as those values are simply too low to get a acceptable discretization error, and we do not want to train an agent to solve for these states. 

In the last chapter, we wrote that the state transitions were random, more precisely, this now means that a state transition is defined as

- choosing a new value of $b\sim \mathcal{U}(0,1)$, and 
- choosing a new value of $n$ randomly, between $5$ and $200$.  

## Computing the reward.

Once a state and action is chosen, the reward need to be computed. For each state and action, we compute the residual ratio after 10 iterations $\rho_{10}$. With that ratio, we need to define an appropriate reward metrics. We design the reward such that:

- The lower the ratio $\rho_{10}$, the better the convergence rate and the better the reward should be.
- It appears natural to have a positive reward when $\rho_{10}<1$, which implies convergence, and a negative reward otherwise.

The reward is then set to be

$$
r(\rho_{10}) = 1-\rho_{10}.
$$

When $\rho_{10}<1$, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. When, on the other hand $\rho_{10}\geq 1$, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at $-3$.



## Implementation of the REINFORCE algorithm.

Now that everything as been defined, the REINFORCE algorithm can be applied to find an optimal policy.

### A first experiment

We implement the REINFORCE algorithm to the test problem. There are a few hyperparameters to set.

- The learning rate is set to $\alpha=1\times10^{-8}$.
- The discount rate is set to $\gamma = 0$, as the state transitions are completely random, there is no reason to prefer long term rewards.
- Because the discount rate is so low, there is no bias added by estimating the returns at the end of the episodes. The episodes length is set to $20$ as we want to use the updated policy as often as possible.
- The standard deviation of the policy parameters is set to $\sigma = 0.1$.

This leaves the choice of the initial value for $\theta$. While it is possible for the parameters to be random, or all set to 0, we use the experiment done in chapter 4 to use. In @fig-res_ratio10, it seems that a policy of $\alpha = 0.3$ and $\Delta t = 2$ is a reasonable choice. Since this was done only for a single set of problem parameters, we have no idea of the relationship between problem parameters and optimal solver parameters. Therefore, we only set the parameter $\theta_4 = 0.3$, and $\theta_5=2$, the other parameters are set to 0.

The algorithm is run for 50000 episodes, and we observe the evolution of the theta parameters(@fig-theta_evolution_exp1). 

Since the discount rate is set to $0$, in any state, the return is the instant reward received by the agent over a single episode. So, for an episode of length $l$, we have the rewards $r_1, r_2, \dots, r_l$. Then, we can plot the average reward $r_{av} = \frac{r_1 + r_2 +\dots + r_l}{l}$ over each episode. Because the variance of $r_{av}$ is still high, we use the rolling average of $r_{av}$ over the last $k = 50$ episodes as a smoother. 

The average reward is the no scaling reward in @fig-experimentreward and is trending upward with successive episodes, which is the intended behavior of the algorithm. However, there are certain problems that have been made apparent by the two plots:

- Despite running the algorithm for a long time, some of the $\theta$ parameters have barely changed, and it is clear that we are far from any convergence of the reward function. 
- Even with smoothing, it is apparent that the method has a high variance.
- After the full It seems that $\theta_1$ and $\theta_3$ vary quite a bit over time whereas the other parameters have a steady rate of change.

```{python}
#| echo: false
#| warning: false
#| fig-cap: Evolution of the $\theta$ parameters in a first experiment.
#| label: fig-theta_evolution_exp1
#| out-width: "50%"

exp2_file = 'experiment_data/exp2/log.csv'
exp1_file = 'experiment_data/exp1/log.csv'
#lbjal
df_exp1 = pl.read_csv(exp1_file)
df_exp2 = pl.read_csv(exp2_file)

## Add the experiment number
df_exp1 = df_exp1.with_columns(pl.lit('1').alias('experiment_number'))
df_exp2 = df_exp2.with_columns(pl.lit('2').alias('experiment_number'))

df_exp = pl.concat([df_exp1,df_exp2],rechunk=True)

#Make it tidy + ep/1000
df_exp_tidy = df_exp.melt(
    id_vars=['ep_number','avg_ep_reward','experiment_number'],
    value_vars=['theta_0', 'theta_1', 'theta_2', 'theta_3', 'theta_4', 'theta_5']
    ).with_columns(
        (pl.col('ep_number')/1000).alias('episode_no_1000')
    )





df_theta_exp1 = df_exp_tidy.filter(pl.col('experiment_number') == "1")

p1 = (ggplot(df_theta_exp1, aes(x = 'episode_no_1000', y = 'value'))
+ geom_line()
+ facet_wrap('~variable', scales='free_y')
+ theme_seaborn()
+ theme(panel_spacing_x= 0.8)
+ xlab('Episode no. / 1000')
+ ylab('Theta')
+ theme(figure_size=(6,3))
)

fig = p1.draw()
fig.show()
```


```{python}
#| label: fig-experimentreward
#| echo: false
#| fig-cap: Evolution of the rolling average (k=50) of the average episode reward, with or without scaling.
#| warning: false
#| cache: true
#| out-width: "50%"


#Don't use the tidy data 
df_exp_reward = df_exp.select('ep_number', 'avg_ep_reward','experiment_number').with_columns(pl.col('avg_ep_reward').rolling_mean(window_size=50).alias('rolling reward'))

p2 = (ggplot(df_exp_reward, aes(x = 'ep_number', y = 'rolling reward', color = 'experiment_number'))
+ geom_line()
+ theme_seaborn()
+ xlab('Episode number')
+ xlim(0,50000)
+ labs(color = 'Re-scaling?')
+ scale_color_discrete(labels = ['No scaling','Scaling for $n$']))
# + coord_cartesian(ylim=(-0.1,0.01)))

fig = p2.draw()
fig.show()
```


The slow apparent convergence rate can not be mitigated by a higher learning rate, as this empirically leads to divergence issues. 

The high variance is typical of reinforcement learning tasks, and in particular Monte Carlo based methods, which REINFORCE is a part of. That being said, there exists much better methods that can reduce this variance, at the expense of introducing some bias, such as for example actor-critics methods [@Sutton1998, chap. 13.5], or proximal policy optimization(PPO) @DBLP:journals/corr/SchulmanWDRK17. Both of these methods are not explored in this thesis.


## Scaling the parameters 

### An example of gradient descent

Consider the objective function $f(x,y) = x^2 + 9y^2$. The function admits a global minimum at $x = y = 0$, and its gradient given by 

$$
\nabla f(x,y) = (2x,18y)^\intercal.
$$

Therefore, the gradient descent iteration, with learning rate $\alpha>0$, is the iteration

$$
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \alpha \begin{pmatrix}
2x_t\\
18y_t
\end{pmatrix}.
$$

That is $x_{t+1} = (1-2\alpha)x_t$ and $y_{t+1} = (1-18\alpha)y_t$. The algorithms converge to $x=y=0$ if and only if $\alpha<1/9$. If however, $\frac{1}{9}<\alpha<1$, we will have convergence for $x$, but not for $y$.

The reason for this is that the gradient is steeper in the $y$ direction than the $x$ direction, which leads to comparatively bigger change in $y$ than $x$ in the gradient descent iterations.

To remedy this, we can use a change of variable $z = 3y$. Then $f(x,z) = x^2 + z^2$. The gradient descent iteration is then given by 

$$
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \alpha 
\begin{pmatrix}
2x_t\\
2y_t
\end{pmatrix}.
$$

That is, $x_{t+1} = (1-2\alpha_x)x_t$ and $z_{t+1} = (1-2\alpha_y)y_t$. This converges to $0$ if and only if $0<\alpha<\frac{1}{2}$, which means we can afford a much bigger learning rate. With $\alpha = \frac{1}{2}$, the gradient descent algorithm can now converge to $0$ in a single iteration! 

### Changing the variable

We have an expression for the policy of the gradient of the policy in @eq-policyGradient. In particular, in @eq-nabla_xi_alpha and @eq-nabla_xi_delta_t, we remark that the gradient value in the directions of $\theta_1$ and $\theta_3$ have a similar expression. Namely, it "depends" strongly on $n$, which can get up to values of $200$. Similarly, $\theta_0$ and $\theta_2$ depends on $b$, and $\theta_4$ and $\theta_5$ depend on neither. However, $b$ can only take values between $0$ and $1$. This motivates the idea that the gradient may be in practice way steeper in the $\theta_1$ and $\theta_3$ directions than the other ones.

Therefore, instead of using $n$ directly, we implement the scaled variable

$$
n' = \frac{n-5}{200},
$$

which can now vary  between $0$ and $1$. Everything then follows by simply replacing $n$ by $n'$ in @sec-linear_policy. The new deterministic policy is 

$$
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} = 
\begin{pmatrix}
\theta_0 & \theta_1\\
\theta_2 & \theta_3
\end{pmatrix} 
\begin{pmatrix}
b\\
n'
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix},
$${#eq-policy2_deterministic_part}


and the expression of the gradient is unchanged, with the exception of replacing $n$ by $n'$ everywhere. 

With this change implemented, we rerun the first experiment. All the parameters are the same, except that the learning rate can is now set to $\alpha = 2\times 10^{-4}$ without divergence. Compared to the first experiment, the evolution of the policy parameters is much higher(see @fig-variable_theta)! The average episode reward is also much better, as seen in @fig-experimentreward.


```{python}
#| echo: false
#| warning: false
#| fig-cap: Evolution of the theta parameters with different initial parameters.
#| label: fig-variable_theta
#| cache: true

exp2_file = 'experiment_data/exp2/log.csv'
exp_initial_param1_file = 'experiment_data/exp_initial_param1/log.csv'
exp_initial_param2_file = 'experiment_data/exp_initial_param2/log.csv'

#flksjdfl
df_exp2 = pl.read_csv(exp2_file)
df_exp_p1 = pl.read_csv(exp_initial_param1_file)
df_exp_p2 = pl.read_csv(exp_initial_param2_file)


initial_theta_exp2 = '[0,0,0,0,0.3,2]'
initial_theta_p2 = '[0.5,0.5,0.5,0.5,0.5,0.5]'
initial_theta_p1 = '[0,0,0,0,0,0]'

## Add the experiment number
df_exp2 = df_exp2.with_columns(pl.lit(initial_theta_exp2).alias('Initial theta'))
df_exp_p1 = df_exp_p1.with_columns(pl.lit(initial_theta_p1).alias('Initial theta'))
df_exp_p2 = df_exp_p2.with_columns(pl.lit(initial_theta_p2).alias('Initial theta'))


df_exp = pl.concat([df_exp2,df_exp_p1, df_exp_p2],rechunk=True)

#Make it tidy + ep/1000
df_exp_tidy = df_exp.melt(
    id_vars=['ep_number','avg_ep_reward','Initial theta'],
    value_vars=['theta_0', 'theta_1', 'theta_2', 'theta_3', 'theta_4', 'theta_5']
    ).with_columns(
        (pl.col('ep_number')/10000).alias('episode_no_10000')
    )


p1 = (ggplot(df_exp_tidy, aes(x = 'episode_no_10000', y = 'value', colour = 'Initial theta'))
+ geom_line()
+ facet_wrap('~variable', scales='free_y')
+ theme_seaborn()
+ theme(panel_spacing_x= 0.7)
+ xlab('Episode no. / 10000')
+ ylab('Theta')
)

fig = p1.draw()
fig.show()
```

```{python}
#| echo: false
#| warning: false
#| fig-cap: Evolution of the rolling average($k=1000$) of the average episode reward for different initial parameters. 
#| label: fig-reward_initial_theta

#Don't use the tidy data 
df_exp_reward = df_exp.select('ep_number', 'avg_ep_reward','Initial theta').with_columns(pl.col('avg_ep_reward').rolling_mean(window_size=1000).alias('rolling reward'))

p2 = (ggplot(df_exp_reward, aes(x = 'ep_number', y = 'rolling reward', color = 'Initial theta'))
+ geom_line()
+ theme_seaborn()
+ xlab('Episode number')
+ coord_cartesian(ylim=(0,0.0075)))

fig = p2.draw()
fig.show()

```




## Impact of initial conditions

Gradient descent algorithms use the local information about an objective function $J(\theta)$ to compute the update $\theta \rightarrow \theta - \alpha \nabla J(\theta)$. This local behavior also means that any convergence of gradient descent is to a local minimum, and we can't be certain that this mimimum is a global mimimum.

 Therefore, it is interesting to test whether the algorithm converges to the same values regardless of initial conditions. The third experiment is then to run the algorithm with the same parameters as before, but with varied initial conditions for the $\theta$ vector, and to visualize the results, both in the average rewards and the evolution of $\theta$ parameters over 200000 episodes. 

 The evolution of the $\theta$ parameters is in @fig-variable_theta, and the rolling average of the average episode reward is plotted in @fig-reward_initial_theta for different initial value for $\theta$. It turns out that while convergence in reward is to the same values, the $\theta_3$ parameter does not seem to converge to the same value. Furthermore, even with such a large amount of episodes, it is not clear if the other parameters converged. 






## Further results

The average reward of the episode is a nice way to report on the performance of the method. However, it is difficult to interpret how exactly the model is performing once we have some found some optimal parameters $\theta^*$. 

In particular, while the policy function is stochastic during training, the actual policy we choose can be deterministic. So, at the risk of adding some bias, we simply remove the noise $\sigma$ in the policy and chose to use the  deterministic policy $\alpha = \mu_\alpha$, $\Delta t = \mu_{\Delta t}$, as in @eq-policy_deterministic_part, and we note this policy $\pi_d(a|s,\theta^*)$. For the value of the parameters, we use their last values in the second experiment, which are(with some rounding off)

$$
\theta^* = (-3.606 \times 10^{-3},4.476\times 10^{-3},-3.598\times 10^{-4},-0.3542 ,0.2435,1.1305)^\intercal.
$$

Then, we compute the value of $\rho_{10}$, for this policy and for different values of $n$ and $b$, the results are as below in @fig-learned_policy. While we have convergence at any point, the convergence is slow, and the maximum value for $\rho_{10}$ is $0.99917$. Referring back to the grid search experiment(see @fig-res_ratio_comparison), this slow convergence is also partly an issue with the solver itself.

Since we trained the policy on $\rho_{10}$, it may be a good idea to check if the convergence still holds when we compute more iterations of the solver. The result are in @fig-learned_policy. There are some points where the solver diverges, which is a problem in particular because the point where it diverges are for small values of $b$, which is often the case physically. 

This divergence indicates that it may be a good idea, as a further study to further train the learned policy by computing $\rho_{100}$, and having a reward of $1-\rho_{100}$ instead of $1-\rho_{10}$. This of course mean that the training time will be longer. 

```{python}
#| echo: false
#| warning: false
#| eval: false

import numpy as np
from testProblemClass import TestProblem
from plotnine import *
import polars as pl



def res_ratio(res_norm_list):
    return res_norm_list[-1] / res_norm_list[-2]

problem = TestProblem()

### Setting the policy
problem.policy.theta = np.array([-3.60642577e-03,4.47643004e-03,-3.59883026e-04,-3.54240053e-01,2.43498188e-01,1.13051680e+00])
problem.policy.version = 2
l = 50
n_grid = np.arange(5,200,1)
b_grid = np.linspace(0,1,l)


n_grid , b_grid = np.meshgrid(n_grid, b_grid)
ratio_grid1 = np.zeros((len(b_grid), len(n_grid[0])))
ratio_grid2 = np.zeros((len(b_grid), len(n_grid[0])))

for i in range(len(n_grid[0])):
    for j in range(len(b_grid)):
        n = n_grid[j,i]
        b = b_grid[j,i]
        problem.update(b = b, n = n)
        
        _ , res_ratio_var1 = problem.main_solver(n_iter = 10, deterministic_policy=True)
        #For rho10, then rho100
        _ , res_ratio_var2 = problem.main_solver(n_iter = 100, deterministic_policy=True)
        ratio_grid1[j,i] = res_ratio_var1
        ratio_grid2[j,i] = res_ratio_var2


### Useful binning function that could have been np.bins
def continuous_to_level(res_ratio):
    if(res_ratio < 0.99):
        return 0
    elif(res_ratio < 0.992):
        return 1
    elif(res_ratio < 0.994):
        return 2 
    elif(res_ratio < 0.996):
        return 3
    elif(res_ratio < 0.998):
        return 4
    elif(res_ratio < 1):
        return 5
    return 6 
#Vectorized
vec_continuous_to_level = np.vectorize(continuous_to_level)


##Prepare tiny data, make it one dimensional
flat_ratio_grid1 = ratio_grid1.flatten()
flat_ratio_grid2 = ratio_grid2.flatten()
flat_b_grid = b_grid.flatten()
flat_n_grid = n_grid.flatten().astype('float')

## And now put everything in bins, so it looks better
flat_ratio_grid1_factor = vec_continuous_to_level(flat_ratio_grid1)
flat_ratio_grid2_factor = vec_continuous_to_level(flat_ratio_grid2)
#print(np.max(flat_ratio_grid))

#Make the dataframe
data = {"n":flat_n_grid, "b":flat_b_grid,             "res_ratio_10":flat_ratio_grid1_factor,
"res_ratio_100":flat_ratio_grid2_factor}

df = pl.DataFrame(data)

#Just to label the legend, instead of the hideous factors
level_order1 = ["<0.99", "[0.99,0.992)", "[0.992,0.994)", "[0.994,0.996)","[0.996,0.998)", "[0.998,1)"]

plot1 = ggplot(df, aes(x = "b", y = "n", fill ="factor(res_ratio_10)")) \
    + geom_tile() \
    + xlab('b') \
    + ylab('n') \
    + labs(fill = 'Residual ratio.') \
    + theme_seaborn()\
    + scale_fill_manual(values = ['#ffffb2','#fed976','#feb24c','#fd8d3c','#fc4e2a','#e31a1c','#b10026'], labels = level_order1) \
    + theme(legend_background=element_rect(fill='#EBEBEB'))


level_order2 = ["<0.99", "[0.99,0.992)", "[0.992,0.994)", "[0.994,0.996)","[0.996,0.998)", "[0.998,1)", ">1"]



plot2 = ggplot(df, aes(x = 'b', y = 'n', fill ='factor(res_ratio_100)')) \
    + geom_tile() \
    + xlab('b') \
    + ylab('n') \
    + labs(fill = 'Residual ratio.') \
    + scale_fill_manual(values = ['#ffffb2','#fed976','#feb24c','#fd8d3c','#fc4e2a','#e31a1c','black'], labels = level_order2)\
    + theme_seaborn() \
    + theme(legend_background=element_rect(fill='#EBEBEB'))


plot2.save(filename='images/learned_policy_100.pdf')
plot1.save(filename='images/learned_policy_10.pdf')
```


::: {#fig-learned_policy layout-nrow=2}

![$\rho_{10}$. Maximum residual ratio: 0.99922.](images/learned_policy_10.pdf){#fig-learned_policy_a}

![$\rho_{100}$. Note the divergence in black, for low values of $b$.](images/learned_policy_100.pdf){#fig-learned_policy_b height=200}

Evolution of the residual ratio $\rho_{10}$ and $\rho_{100}$, with the learned policy, depending on the problem parameters $n$ and $b$.

:::



## (Possible section with comparison of results).

This possible section is to see if we can compare results.
