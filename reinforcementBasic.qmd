---
title: "Reinforcement learning - Basic"
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
        include-in-header: 
            text: |
                \usepackage{amsmath}
                \usepackage{easy-todo}
                \usepackage{amsthm}
        documentclass: article
        fontsize: "14"
jupyter: python3
---


In this section, we outline the main ideas behind reinforcement learning and how they can be applied in the context of this thesis. 


## A non mathematical, but delicious example.

Suppose we want to cook a delicious meal. At any point in time, we are making decisions such as

- which ingredients we use. Do we use tofu or seitan? Do we add spice more chili pepper? When do we incorporate the sauce?
- which cookware we use? Cast iron, or non-stick pan?
- whether to stir or not. It may stick and burn at the bottom of the pan if we don't, but we are lazy and our laziness has to be weighted in.
- Or simply do nothing!

All of these decisions, which we will call *actions* from now on, are taken in reaction to the current *state* of the cooking process, following a certain *policy*, which is shaped by our previous cooking experience. 

After each action, the cooking process get to a new *state* and we get a *reward* that depend on how we did. Maybe the food started to burn in which case we get a negative reward, or maybe we made the food better, in which case we get a positive reward. In this example, there is also a terminal state, in which we finished cooking and get to eat the meal. 

But how do we learn how to cook, that is, how do we learn the *policy*? We learn it by trying to make the food as good as possible, which is defined by the *reward* we get after each action. Some of those rewards are immediate. For example, if we add some spices to our food and it tastes better, we may be inclined to do it again the next time we cook a meal.  We want to have a *policy* that maximize the total *rewards* we get, which also mean that we have to balance our decision between the immediate reward and the future rewards. Adding a spice may make the meal taste better in the short term, but it may clash later when we add other ingredients, leading to a worse meal and bad *rewards*.

Each time we cook, we learn what works and what doesn't, and remember that for the future time we cook. But, if we want to get better at cooking, we must not just repeat the *actions* that worked! We also have to take some risks, and *explore* the potential actions we can take at each state! On the other hand, we still need to rely and *exploit* what we know, so there is a balance between *exploitation* and *exploration* to find so we can learn as fast as possible.

## Finite Markov decision process

Before introducing reinforcement learning, we first need to define a Markov decision process (MDP).





:::{#def-Markov_decision_process}
(Markov decision process). 
A finite Markov decision process is defined as a discrete time process, where we have

- a state set $\mathbfcal{S}$,
- an action set $\mathcal{A}$, containing all possible actions,
- for each state and each action, we have a reward set $\mathcal{R}(s,a)$, which contain the potential rewards received after taking action $a\in\mathcal{A}$ from the state $s\in\mathcal{S}$.

A Markov decision process has a model, which consist of 

- the probability of getting from state $s$ to the state $s'$ by taking action $a$, which we call the state transition probability $p(s'|s,a) = P(s_{t+1} = s' | s_t = s, a_t = a)$.
- the probability of getting reward $r$ by taking the action $a$ at a state $s$ $p(r|s,a) = P(r_{t+1} = r | s_t = s, a_t = a)$. 

Furthermore, a policy function is also given that governs for a given state $s\in\mathcal{S}$, the probability of taking action $a\in\mathcal{A}$, that probability is $\pi(a|s) = Pr(a_{t+1} = a|s_t = s)$.

:::


:::{.remark}

We have implicitly defined the random variables designing the state, action, reward at a time $t$, those are respectively $s_t,a_t,r_t$. A diagram of the process is as follow



:::


:::{#exm-fmdpex}
(A more mathematical example, adorable)



:::