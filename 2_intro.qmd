# Introduction {.unnumbered}

Numerical methods for differential equations are amongst the most important methods in numerical analysis. All of these methods have specific strengths and weaknesses. They all have, however, some parameters that need to be chosen, if only for the step size.

These parameters have to be chosen to maximize performances, and depend on the problem. In some cases they are taken using some heuristics, but they can also be searched for computationally. 

In this thesis, we do the latter, on a specific, restricted set of problems. We first motivate the use of numerical ODE solvers to solve linear systems. As a case study, we have a specific type of linear systems, which appears when discretizing the steady state, one dimensional convection diffusion equation $u_{x} = bu_{xx} +1$. Doing so, we end up with two problem parameters, $b$, which is a physical constant, and $n$, stemming from the discretization. The studied numerical solver is an explicit Runge-Kutta method, and has two parameters, a (pseudo-) time step $\Delta t$ and another parameter $\alpha$, which need to be chosen. We then explore how to optimize these two parameters as a function of the problem parameters $b$ and $n$. In particular, we use reinforcement learning (RL) to do so. 

The method of reinforcement learning can claim its origin from both the fields of animal learning theory and optimal control theory @towardsdatascienceReinforcementLearning. The general idea is to train an agent to make decisions based on an environment that the agent can interact with, and rewarding, or punishing this agent when it makes good, or bad decisions. Over time, the agent then learn to make better and better decisions by using its past experiences to maximize the rewards it gets. This approach has been used successfully in a number of problems, such as for example playing games at a high level @Silver2016, or discovering matrix multiplication algorithms @Fawzi2022. 

In this thesis, we introduce the main concepts of reinforcement learning, such as Markov decision processes, states, actions and rewards. We then introduce policy gradient methods and use it to optimize the solver parameters for the studied  linear systems. The results, while positive, are hampered mainly by the fact that the method used in this thesis makes poor use of what make reinforcement learning powerful in some cases. A discussion on how to redefine the problem to make better use of reinforcement learning follows.


