
# Introduction {.unnumbered}

Numerical methods for differential equations are among the most important and numerous methods in numerical analysis, all with specific strengths and weaknesses. They all have, however, some parameters that need to be chosen, if only for the step size.

These parameters have to be chosen to maximize performances, and depends on the problem. In some cases they are taken using some heuristics, but they can be searched for computationally. 

In this thesis, we do the latter, on a specific, restricted set of problems. We first motivate the use of numerical ODE solvers to solve linear systems. As a case study, we have a specific type of linear systems, which appears when discretizing the steady state, one dimensional convection diffusion equation $u_{x} = bu_{xx} +1$, with $b$ being a physical constant. Doing so, we end up with two problem parameters, $b$ and $n$, which is a parameter in the discretization. The studied numerical solver is an explicit Runge-Kutta method, and has two parameters, a pseudo time step $\Delta t$ and another parameter $\alpha$, which need to be chosen. We then explore how to optimize these two parameters, as a function of the problem parameters $b$ and $n$. In particular, we use reinforcement learning(RL) to do so. 

The method of reinforcement learning can claim its origin from both the field of animal learning (learning by trial and error) and optimal control theory @towardsdatascienceReinforcementLearning. The general idea is to train an agent to make decisions based on an environment that the agent can interact with, and rewarding, or punishing this agent when it makes good, or bad decisions. Over time, the agent then learn to make better and better decisions by using its past experiences to maximize the reward it gets. This approach has been used successfully in a number of problems, such as for example playing games at a high level @Silver2016, or discovering matrix multiplication algorithms @Fawzi2022. 

In this thesis, we introduce reinforcement learning main concepts, such as Markov decision processes, state, action and rewards. We then introduce policy gradient methods and use it to optimize the solver parameters for the studied  linear systems. The results, while positive, are hampered mainly by the fact that the method used in this thesis makes poor use of what make reinforcement learning powerful in some cases. A discussion on how to redefine the problem to make better use of reinforcement learning follows.


