<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bachelor thesis - Reinforcement learning for something. - 7&nbsp; Implementation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./9_summary.html" rel="next">
<link href="./7_policyGradient.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bachelor thesis - Reinforcement learning for something.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Abstract</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_motivation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Motivation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_convecDiff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A test problem - Convection diffusion equation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_solverExploration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Runge-Kutta solver applied to the test problem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_reinforcementBasic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning(RL)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_policyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_implementation.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-linear-approximation-of-the-policy" id="toc-a-linear-approximation-of-the-policy" class="nav-link active" data-scroll-target="#a-linear-approximation-of-the-policy"><span class="toc-section-number">7.1</span>  A Linear Approximation of the Policy</a></li>
  <li><a href="#state-space" id="toc-state-space" class="nav-link" data-scroll-target="#state-space"><span class="toc-section-number">7.2</span>  State Space</a></li>
  <li><a href="#computing-the-reward." id="toc-computing-the-reward." class="nav-link" data-scroll-target="#computing-the-reward."><span class="toc-section-number">7.3</span>  Computing the reward.</a></li>
  <li><a href="#implementation-of-the-reinforce-algorithm." id="toc-implementation-of-the-reinforce-algorithm." class="nav-link" data-scroll-target="#implementation-of-the-reinforce-algorithm."><span class="toc-section-number">7.4</span>  Implementation of the REINFORCE algorithm.</a>
  <ul class="collapse">
  <li><a href="#a-first-experiment" id="toc-a-first-experiment" class="nav-link" data-scroll-target="#a-first-experiment"><span class="toc-section-number">7.4.1</span>  A first experiment</a></li>
  <li><a href="#gradient-ascent-with-different-learning-parameters." id="toc-gradient-ascent-with-different-learning-parameters." class="nav-link" data-scroll-target="#gradient-ascent-with-different-learning-parameters."><span class="toc-section-number">7.4.2</span>  Gradient ascent with different learning parameters.</a></li>
  </ul></li>
  <li><a href="#exploration-vs-exploitation-tradeoff." id="toc-exploration-vs-exploitation-tradeoff." class="nav-link" data-scroll-target="#exploration-vs-exploitation-tradeoff."><span class="toc-section-number">7.5</span>  Exploration vs exploitation tradeoff.</a></li>
  <li><a href="#impact-of-initial-condition." id="toc-impact-of-initial-condition." class="nav-link" data-scroll-target="#impact-of-initial-condition."><span class="toc-section-number">7.6</span>  Impact of initial condition.</a></li>
  <li><a href="#moving-beyond-the-basics." id="toc-moving-beyond-the-basics." class="nav-link" data-scroll-target="#moving-beyond-the-basics."><span class="toc-section-number">7.7</span>  Moving beyond the basics.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="a-linear-approximation-of-the-policy" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="a-linear-approximation-of-the-policy"><span class="header-section-number">7.1</span> A Linear Approximation of the Policy</h2>
<p>We want to have a policy of the form <span class="math inline">\((\Delta t, \alpha) =A(b,n)' + \text{c}\)</span>, where <span class="math inline">\(A\)</span> is a two by two matrix and <span class="math inline">\(c\)</span> a 2-vector.</p>
<p>We define the following stochastic policy, define first</p>
<p><span class="math display">\[
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} =
\begin{pmatrix}
\theta_0 &amp; \theta_1\\
\theta_2 &amp; \theta_3
\end{pmatrix}
\begin{pmatrix}
b\\
n
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix}
\]</span></p>
<p>Then we chose the random policy <span class="math inline">\(\alpha \sim \mathcal{N}(\mu_\alpha,\sigma^2)\)</span>,and similarly <span class="math inline">\(\Delta t \sim \mathcal{N}(\mu_{\Delta t}, \sigma^2)\)</span>. The term <span class="math inline">\(\sigma^2\)</span>, which is the variance of the policy is chosen fixed, and will help us balance exploration vs exploitation. Since <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\Delta t\)</span> are chosen independently, the joint probability density of both parameters is the product of both marginal pdf, that is</p>
<p><span class="math display">\[
f(\alpha,\Delta t) = f_{1}(\alpha)\cdot f_{2}(\Delta t)
\]</span></p>
<p>where <span class="math display">\[
f_1(\alpha) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}\right)
\]</span></p>
<p>and similarly,</p>
<p><span class="math display">\[
f_2(\Delta t) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)^2}{2\sigma^2}\right)
\]</span>.</p>
<p>Taking the logarithm, we get <span class="math inline">\(\ln(f(\alpha,\Delta t) = \ln(f_1(\alpha)) + \ln(f_2(\Delta t))\)</span>. Thus,</p>
<p><span class="math display">\[
\ln(f_1(\alpha)) = \ln(\frac{1}{\sqrt{2\pi}\sigma}) - \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}
\]</span>.</p>
<p>We now take the gradient w.r.t <span class="math inline">\(\theta\)</span> to get</p>
<p><span id="eq-nabla_xi_alpha"><span class="math display">\[
\nabla_\theta \ln(f_1(\alpha)) = \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T
\tag{7.1}\]</span></span></p>
<p>where <span class="math inline">\(\xi_\alpha = \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)}{\sigma^2}\)</span>.</p>
<p>Doing a similar thing with <span class="math inline">\(\Delta t\)</span>, we get the gradient,</p>
<p><span id="eq-nabla_xi_delta_t"><span class="math display">\[
\nabla_\theta \ln(f_2(\Delta t)) = \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\tag{7.2}\]</span></span></p>
<p>where <span class="math inline">\(\xi_{\Delta t} = \frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)}{\sigma^2}\)</span> We now add both gradients together to we get the gradient of the policy, for a specific action <span class="math inline">\(a = (\alpha, \Delta t)\)</span> and state <span class="math inline">\(s=(b,n)\)</span>,</p>
<p><span id="eq-policyGradient"><span class="math display">\[
\nabla_\theta \ln\pi(a|s,\theta) =  \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T+ \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\tag{7.3}\]</span></span></p>
<p>We used here the standard notation. That is, <span class="math inline">\(\pi(a|s,\theta) = f(\alpha,\Delta t)\)</span>.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>One may remark that the REINFORCE algorithm uses a discrete policy space. This is not an issue. Instead of using the probability mass function of the policy, we will instead use the probability density function as a substitute(<span class="citation" data-cites="lillicrap2019continuous"><a href="10_references.html#ref-lillicrap2019continuous" role="doc-biblioref">[1]</a></span>). The fact that the pdf admits values <span class="math inline">\(&gt;1\)</span> is not an issue as we adjust the learning rate accordingly.</p>
</div>
</section>
<section id="state-space" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="state-space"><span class="header-section-number">7.2</span> State Space</h2>
<p>In the test problem, the <span class="math inline">\(b\)</span> parameter is a physical parameter and can take any value in <span class="math inline">\([0,1]\)</span>, while the value of <span class="math inline">\(n\)</span> depends on the discretization of the differential equation, and can be any integer value. As the value of <span class="math inline">\(n\)</span> get bigger, computation of a single time step in the RK solver can take quite a bit more time, and we will be doing a lot of those computations in the coming section! Therefore, we decide to limit the value of <span class="math inline">\(n\)</span> to an arbitrary maximum of <span class="math inline">\(200\)</span>. We also cap the minimum value of <span class="math inline">\(n\)</span> to an arbitrary minimum of <span class="math inline">\(5\)</span> as those values are simply too low to get a acceptable discretization error, and we do not want to train an agent to solve theses states.</p>
<p>In the last chapter, we wrote that the state transitions were random, more precisely, this now means that a state transition is defined as</p>
<ul>
<li>choosing a new value of <span class="math inline">\(b\sim \mathcal{U}(0,1)\)</span>, and</li>
<li>choosing a new value of <span class="math inline">\(n\)</span> randomly, between <span class="math inline">\(5\)</span> and <span class="math inline">\(200\)</span>.</li>
</ul>
</section>
<section id="computing-the-reward." class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="computing-the-reward."><span class="header-section-number">7.3</span> Computing the reward.</h2>
<p>Once a state and action is chosen, the reward need to be computed. We said before that, for each state and action, we compute the residual ratio after 10 iterations <span class="math inline">\(c_{10}\)</span>. With that ratio, we need to define an appropriate reward metrics. We design the reward such that:</p>
<ul>
<li>The lower the ratio <span class="math inline">\(c_{10}\)</span>, the better the convergence rate and the better the reward should be.</li>
<li>It appears natural to have a positive reward when <span class="math inline">\(c_{10}&lt;1\)</span>, which implies convergence, and a negative reward otherwise.</li>
</ul>
<p>The reward is</p>
<p><span class="math display">\[
r(c_{10}) = \begin{cases}
500\times (1-c_{10}) \;\;\;\;\;\qquad \text{ if } c_{10}&lt;1\\
\max(-3,1 - c_{10}) \qquad \text{ if } c_{10}\geq1
\end{cases}
\]</span></p>
<p>When <span class="math inline">\(c_{10}&lt;1\)</span>, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. Because the ratio tends to be very close to <span class="math inline">\(1\)</span>, we multiply everything by <span class="math inline">\(500\)</span>, adding more contrast to the rewards.</p>
<p>When, on the other hand <span class="math inline">\(c_{10}\geq 1\)</span>, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at <span class="math inline">\(-3\)</span>.</p>
</section>
<section id="implementation-of-the-reinforce-algorithm." class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="implementation-of-the-reinforce-algorithm."><span class="header-section-number">7.4</span> Implementation of the REINFORCE algorithm.</h2>
<p>Now that everything as been defined, the REINFORCE algorithm can be applied to find an optimal policy.</p>
<section id="a-first-experiment" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="a-first-experiment"><span class="header-section-number">7.4.1</span> A first experiment</h3>
<p>We implement the REINFORCE algorithm to the test problem. There are a few hyperparameters to set.</p>
<ul>
<li>The learning rate is set to <span class="math inline">\(\alpha=2\times10^{-8}\)</span>.</li>
<li>The discount rate is set to a low value of <span class="math inline">\(\gamma = 0.1\)</span>, as the state transitions are completely random, there is no reason to prefer long term rewards.</li>
<li>Because the discount rate is so low, the episodes length is set to <span class="math inline">\(20\)</span> as we want to use the updated policy as often as possible.</li>
<li>The standard deviation of the policy parameters is set to <span class="math inline">\(\sigma = 0.1\)</span>.</li>
</ul>
<p>This leaves the choice of the initial value for <span class="math inline">\(\theta\)</span>. While it is possible for the parameters to be random, or all set to 0, we use the experiment done in chapter 4 to use. In <a href="5_solverExploration.html#fig-resratio10">Figure&nbsp;<span>4.1</span></a>, it seems that a policy of <span class="math inline">\(\alpha = 0.3\)</span> and <span class="math inline">\(\Delta t = 2\)</span> is reasonable. Since this was done only for a single set of problem parameters, we have no idea of the relationship between problem parameters and optimal solver parameters. Therefore, we only set the parameter <span class="math inline">\(\theta_4 = 0.2\)</span>, and <span class="math inline">\(\theta_5=2\)</span>, the other parameters are set to 0.</p>
<p>The algorithm is run for 40000 episodes, and we observe the evolution of the theta parameters(<a href="#fig-experimentreward">Figure&nbsp;<span>7.2</span></a>). The objective function <span class="math inline">\(J(\theta)\)</span> can not be observed. We have access, however to the average reward over an episode in <a href="#fig-experimenttheta">Figure&nbsp;<span>7.1</span></a>. Note that in the latter plot, the rolling average of the average reward over the last <span class="math inline">\(k=1000\)</span> episodes is used as a smoother.</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-experimenttheta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="8_implementation_files/figure-html/fig-experimenttheta-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.1: Evolution of the theta parameters with episode number. The hyperparameters are <span class="math inline">\(\sigma\)</span>=0.1, learning rate 2e-8.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-experimentreward" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="8_implementation_files/figure-html/fig-experimentreward-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.2: Evolution of the rolling average (k=1000) of the average episode reward.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The reward in <a href="#fig-experimentreward">Figure&nbsp;<span>7.2</span></a> is trending upward, which is the intended behavior of the algorithm. However, there are certain problems that have been made apparent.</p>
<ul>
<li>Despite running the algorithm for a long time(approximately 20 minutes on a typical computer), the <span class="math inline">\(\theta\)</span> parameters have not changed very much, and it is clear that we are far from any convergence of the reward function.</li>
<li>Even with smoothing, it is apparent that the method has a high variance.</li>
<li>It seems that <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_5\)</span> varies quite a bit over time whereas the other parameters have a steady rate of change.</li>
</ul>
<p>The slow apparent convergence rate can not be mitigated by a higher learning rate, as this empirically lead to divergences issues. The high variance is typical of reinforcement learning tasks, as well as Monte Carlo based methods, which REINFORCE is a part of. That being said, there exists much better methods that can reduce this variance, at the expense of introducing some bias, such as for example actor-critics methods <span class="citation" data-cites="Sutton1998"><a href="10_references.html#ref-Sutton1998" role="doc-biblioref">[2, Ch. 13.5]</a></span>, or proximal policy optimization(PPO) <span class="citation" data-cites="DBLP:journals/corr/SchulmanWDRK17"><a href="10_references.html#ref-DBLP:journals/corr/SchulmanWDRK17" role="doc-biblioref">[3]</a></span>. Both of these methods are not explored in this thesis.</p>
</section>
<section id="gradient-ascent-with-different-learning-parameters." class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="gradient-ascent-with-different-learning-parameters."><span class="header-section-number">7.4.2</span> Gradient ascent with different learning parameters.</h3>
<section id="an-example-of-gradient-descent." class="level4" data-number="7.4.2.1">
<h4 data-number="7.4.2.1" class="anchored" data-anchor-id="an-example-of-gradient-descent."><span class="header-section-number">7.4.2.1</span> An example of gradient descent.</h4>
<p>Consider the objective function <span class="math inline">\(f(x,y) = x^2 + 3y^2\)</span>. The function admits a global minimum at <span class="math inline">\(x = y = 0\)</span>, and its gradient is</p>
<p><span class="math display">\[
\nabla f(x,y) = (2x,6y)'
\]</span></p>
<p>Therefore, the gradient descent iteration, with learning rate <span class="math inline">\(\alpha&gt;0\)</span> is the iteration</p>
<p><span class="math display">\[
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \alpha \begin{pmatrix}
2x_t\\
6y_t
\end{pmatrix}
\]</span>.</p>
<p>That is <span class="math inline">\(x_{t+1} = (1-2\alpha)x_t\)</span> and <span class="math inline">\(y_{t+1} = (1-6\alpha)y_t\)</span>. The algorithms converge to <span class="math inline">\(x=y=0\)</span> if and only if <span class="math inline">\(\alpha&lt;1/3\)</span>. If however, <span class="math inline">\(\frac{1}{3}&lt;\alpha&lt;1\)</span>, we will have convergence for <span class="math inline">\(x\)</span>, but not for <span class="math inline">\(y\)</span>.</p>
<p>The reason for this is that the gradient is steeper in the <span class="math inline">\(y\)</span> direction than the <span class="math inline">\(x\)</span> direction, which leads to comparatively bigger change in <span class="math inline">\(y\)</span> than <span class="math inline">\(x\)</span> in the gradient descent iterations.</p>
<p>To remedy this, we can set a variable learning rate that is different depending on the direction. Set <span class="math inline">\(\alpha_x&gt;0\)</span> and <span class="math inline">\(\alpha_y&gt;0\)</span> as the learning rate in the respective gradient direction <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The gradient ascent iteration is then defined as</p>
<p><span class="math display">\[
\begin{pmatrix}
x_{t+1}\\
y_{t+1}
\end{pmatrix} = \begin{pmatrix}
x_t\\
y_t
\end{pmatrix} - \begin{pmatrix}
\alpha_x &amp; 0\\
0 &amp; \alpha_y
\end{pmatrix}
\begin{pmatrix}
2x_t\\
6y_t
\end{pmatrix}.
\]</span></p>
<p>That is, <span class="math inline">\(x_{t+1} = (1-2\alpha_x)x_t\)</span> and <span class="math inline">\(y_{t+1} = (1-6\alpha_y)y_t\)</span>. With <span class="math inline">\(\alpha_x = \frac{1}{2}\)</span> and <span class="math inline">\(\alpha_y = \frac{1}{6}\)</span>, the gradient descent algorithm can now converge to <span class="math inline">\(0\)</span> in a single iteration!</p>
</section>
<section id="implementation." class="level4" data-number="7.4.2.2">
<h4 data-number="7.4.2.2" class="anchored" data-anchor-id="implementation."><span class="header-section-number">7.4.2.2</span> Implementation.</h4>
<p>We have an expression for the policy of the gradient of the policy in <a href="#eq-policyGradient">Equation&nbsp;<span>7.3</span></a>. In particular, in <a href="#eq-nabla_xi_alpha">Equation&nbsp;<span>7.1</span></a> and <a href="#eq-nabla_xi_delta_t">Equation&nbsp;<span>7.2</span></a>, we remark that the gradient value in the directions of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_3\)</span> have a similar expression. Namely, it depends strongly on <span class="math inline">\(n\)</span>. Similarly, <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_2\)</span> depends on <span class="math inline">\(b\)</span>, and <span class="math inline">\(\theta_4\)</span> and <span class="math inline">\(\theta_5\)</span> depend on neither. However, <span class="math inline">\(b\)</span> can only take values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, which motivates the idea that the gradient may be in practice way steeper in the <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_3\)</span> directions than the other one. This idea would also explain the oscillations that happen in <a href="#fig-experimenttheta">Figure&nbsp;<span>7.1</span></a>. Therefore, we decide to add a variable learning rate depending on the direction. The update step in REINFORCE algorithm with fixed learning rate is</p>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_\theta \pi(a|s,\theta).
\]</span></p>
<p>We change this update operation to</p>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha_{\text{var}} \odot \nabla_\theta \pi(a|s,\theta)
\]</span></p>
<p>where <span class="math inline">\(\alpha_{\text{var}} = (\alpha_{\theta_0}, \alpha_{\theta_1}, \dots , \alpha_{\theta_5})'\)</span> and <span class="math inline">\(\odot\)</span> represent the termwise vector multiplication. One problem is that if we look at the gradient directions, how steep they are depend on the problem parameters. And since <span class="math inline">\(n\)</span> can vary between 5 and 300, this make for a muuuuuch steeper gradient than the direction associated with <span class="math inline">\(b\)</span>, which only varies between 0 and 1. As a result, the direction associated with <span class="math inline">\(n\)</span> tend to converge but not the other. This can be remedied by applying varying learning rate in each direction. It actually works quite well! ’</p>
</section>
</section>
</section>
<section id="exploration-vs-exploitation-tradeoff." class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="exploration-vs-exploitation-tradeoff."><span class="header-section-number">7.5</span> Exploration vs exploitation tradeoff.</h2>
<p>Changing <span class="math inline">\(\sigma^2\)</span> to a higher value makes for a flatter gradient, so not only are we taking more risks, we can actually afford a higher learning rate without exploding!</p>
</section>
<section id="impact-of-initial-condition." class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="impact-of-initial-condition."><span class="header-section-number">7.6</span> Impact of initial condition.</h2>
<p>Gradient based algorithm have a tendency to converge to local minima. (in our case maxima, but same thing really), therefore, it would be interesting to see how initial policy impacts the learned policy now that we have convergence. (Not done yet, but I suspect some nasty surprises!)</p>
</section>
<section id="moving-beyond-the-basics." class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="moving-beyond-the-basics."><span class="header-section-number">7.7</span> Moving beyond the basics.</h2>
<p>(Some discussion on what to do next, etc… ) Example include.</p>
<ul>
<li>Better algorithm, with less sample variance and more sample efficiency.</li>
<li>Moving beyond a linear policy. (approximation using NN).</li>
<li>Meta learning, maybe?</li>
<li>Applying the concept learned here to a more varied set of problem, instead of just confining ourselves to steady state diffusion-convection equation.</li>
</ul>


<div id="refs" class="references csl-bib-body" role="doc-bibliography" style="display: none">
<div id="ref-lillicrap2019continuous" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T. P. Lillicrap <em>et al.</em>, <span>“Continuous control with deep reinforcement learning.”</span> 2019. Available: <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></div>
</div>
<div id="ref-Sutton1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R. S. Sutton and A. G. Barto, <em>Reinforcement learning: An introduction</em>, Second. The MIT Press, 2018. Available: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></div>
</div>
<div id="ref-DBLP:journals/corr/SchulmanWDRK17" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, <span>“Proximal policy optimization algorithms,”</span> <em>CoRR</em>, vol. abs/1707.06347, 2017, Available: <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a></div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./7_policyGradient.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./9_summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>