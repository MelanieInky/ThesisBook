<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bachelor thesis - Reinforcement learning for something. - 7&nbsp; Implementation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./9_summary.html" rel="next">
<link href="./7_policyGradient.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bachelor thesis - Reinforcement learning for something.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Abstract</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_motivation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Motivation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_convecDiff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A test problem - Convection diffusion equation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_solverExploration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Runge-Kutta solver applied to the test problem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_reinforcementBasic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning(RL)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_policyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_implementation.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-linear-approximation-of-the-policy" id="toc-a-linear-approximation-of-the-policy" class="nav-link active" data-scroll-target="#a-linear-approximation-of-the-policy"><span class="toc-section-number">7.1</span>  A Linear Approximation of the Policy</a></li>
  <li><a href="#state-space" id="toc-state-space" class="nav-link" data-scroll-target="#state-space"><span class="toc-section-number">7.2</span>  State Space</a></li>
  <li><a href="#computing-the-reward." id="toc-computing-the-reward." class="nav-link" data-scroll-target="#computing-the-reward."><span class="toc-section-number">7.3</span>  Computing the reward.</a></li>
  <li><a href="#implementation-of-the-reinforce-algorithm." id="toc-implementation-of-the-reinforce-algorithm." class="nav-link" data-scroll-target="#implementation-of-the-reinforce-algorithm."><span class="toc-section-number">7.4</span>  Implementation of the REINFORCE algorithm.</a>
  <ul class="collapse">
  <li><a href="#a-first-experiment" id="toc-a-first-experiment" class="nav-link" data-scroll-target="#a-first-experiment"><span class="toc-section-number">7.4.1</span>  A first experiment</a></li>
  </ul></li>
  <li><a href="#gradient-ascent-with-different-learning-parameters." id="toc-gradient-ascent-with-different-learning-parameters." class="nav-link" data-scroll-target="#gradient-ascent-with-different-learning-parameters."><span class="toc-section-number">7.5</span>  Gradient ascent with different learning parameters.</a></li>
  <li><a href="#exploration-vs-exploitation-tradeoff." id="toc-exploration-vs-exploitation-tradeoff." class="nav-link" data-scroll-target="#exploration-vs-exploitation-tradeoff."><span class="toc-section-number">7.6</span>  Exploration vs exploitation tradeoff.</a></li>
  <li><a href="#impact-of-initial-condition." id="toc-impact-of-initial-condition." class="nav-link" data-scroll-target="#impact-of-initial-condition."><span class="toc-section-number">7.7</span>  Impact of initial condition.</a></li>
  <li><a href="#moving-beyond-the-basics." id="toc-moving-beyond-the-basics." class="nav-link" data-scroll-target="#moving-beyond-the-basics."><span class="toc-section-number">7.8</span>  Moving beyond the basics.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="a-linear-approximation-of-the-policy" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="a-linear-approximation-of-the-policy"><span class="header-section-number">7.1</span> A Linear Approximation of the Policy</h2>
<p>We want to have a policy of the form <span class="math inline">\((\Delta t, \alpha) =A(b,n)' + \text{c}\)</span>, where <span class="math inline">\(A\)</span> is a two by two matrix and <span class="math inline">\(c\)</span> a 2-vector.</p>
<p>We define the following stochastic policy, define first</p>
<p><span class="math display">\[
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} =
\begin{pmatrix}
\theta_0 &amp; \theta_1\\
\theta_2 &amp; \theta_3
\end{pmatrix}
\begin{pmatrix}
b\\
n
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix}
\]</span></p>
<p>Then we chose the random policy <span class="math inline">\(\alpha \sim \mathcal{N}(\mu_\alpha,\sigma^2)\)</span>,and similarly <span class="math inline">\(\Delta t \sim \mathcal{N}(\mu_{\Delta t}, \sigma^2)\)</span>. The term <span class="math inline">\(\sigma^2\)</span>, which is the variance of the policy is chosen fixed, and will help us balance exploration vs exploitation. Since <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\Delta t\)</span> are chosen independently, the joint probability density of both parameters is the product of both marginal pdf, that is</p>
<p><span class="math display">\[
f(\alpha,\Delta t) = f_{1}(\alpha)\cdot f_{2}(\Delta t)
\]</span></p>
<p>where <span class="math display">\[
f_1(\alpha) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}\right)
\]</span></p>
<p>and similarly,</p>
<p><span class="math display">\[
f_2(\Delta t) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)^2}{2\sigma^2}\right)
\]</span>.</p>
<p>Taking the logarithm, we get <span class="math inline">\(\ln(f(\alpha,\Delta t) = \ln(f_1(\alpha)) + \ln(f_2(\Delta t))\)</span>. Thus,</p>
<p><span class="math display">\[
\ln(f_1(\alpha)) = \ln(\frac{1}{\sqrt{2\pi}\sigma}) - \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}
\]</span>.</p>
<p>We now take the gradient w.r.t <span class="math inline">\(\theta\)</span> to get</p>
<p><span class="math display">\[
\nabla_\theta \ln(f_1(\alpha)) = \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T
\]</span></p>
<p>where <span class="math inline">\(\xi_\alpha = \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)}{\sigma^2}\)</span>.</p>
<p>Doing a similar thing with <span class="math inline">\(\Delta t\)</span>, we get the gradient,</p>
<p><span class="math display">\[
\nabla_\theta \ln(f_2(\Delta t)) = \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\]</span></p>
<p>where <span class="math inline">\(\xi_{\Delta t} = \frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)}{\sigma^2}\)</span> We now add both gradients together to we get the gradient of the policy, for a specific action <span class="math inline">\(a = (\alpha, \Delta t)\)</span> and state <span class="math inline">\(s=(b,n)\)</span>,</p>
<p><span id="eq-policyGradient"><span class="math display">\[
\nabla_\theta \ln\pi(a|s,\theta) =  \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T+ \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\tag{7.1}\]</span></span></p>
<p>We used here the standard notation. That is, <span class="math inline">\(\pi(a|s,\theta) = f(\alpha,\Delta t)\)</span>.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>One may remark that the REINFORCE algorithm uses a discrete policy space. This is not an issue. Instead of using the probability mass function of the policy, we will instead use the probability density function as a substitute(<span class="citation" data-cites="lillicrap2019continuous"><a href="10_references.html#ref-lillicrap2019continuous" role="doc-biblioref">[1]</a></span>). The fact that the pdf admits values <span class="math inline">\(&gt;1\)</span> is not an issue as we adjust the learning rate accordingly.</p>
</div>
</section>
<section id="state-space" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="state-space"><span class="header-section-number">7.2</span> State Space</h2>
<p>In the test problem, the <span class="math inline">\(b\)</span> parameter is a physical parameter and can take any value in <span class="math inline">\([0,1]\)</span>, while the value of <span class="math inline">\(n\)</span> depends on the discretization of the differential equation, and can be any integer value. As the value of <span class="math inline">\(n\)</span> get bigger, computation of a single time step in the RK solver can take quite a bit more time, and we will be doing a lot of those computations in the coming section! Therefore, we decide to limit the value of <span class="math inline">\(n\)</span> to an arbitrary maximum of <span class="math inline">\(200\)</span>. We also cap the minimum value of <span class="math inline">\(n\)</span> to an arbitrary minimum of <span class="math inline">\(5\)</span> as those values are simply too low to get a acceptable discretization error, and we do not want to train an agent to solve theses states.</p>
<p>In the last chapter, we wrote that the state transitions were random, more precisely, this now means that a state transition is defined as</p>
<ul>
<li>choosing a new value of <span class="math inline">\(b\sim \mathcal{U}(0,1)\)</span>, and</li>
<li>choosing a new value of <span class="math inline">\(n\)</span> randomly, between <span class="math inline">\(5\)</span> and <span class="math inline">\(200\)</span>.</li>
</ul>
</section>
<section id="computing-the-reward." class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="computing-the-reward."><span class="header-section-number">7.3</span> Computing the reward.</h2>
<p>Once a state and action is chosen, the reward need to be computed. We said before that, for each state and action, we compute the residual ratio after 10 iterations <span class="math inline">\(c_{10}\)</span>. With that ratio, we need to define an appropriate reward metrics. We design the reward such that:</p>
<ul>
<li>The lower the ratio <span class="math inline">\(c_{10}\)</span>, the better the convergence rate and the better the reward should be.</li>
<li>It appears natural to have a positive reward when <span class="math inline">\(c_{10}&lt;1\)</span>, which implies convergence, and a negative reward otherwise.</li>
</ul>
<p>The reward is</p>
<p><span class="math display">\[
r(c_{10}) = \begin{cases}
500\times (1-c_{10}) \;\;\;\;\;\qquad \text{ if } c_{10}&lt;1\\
\max(-3,1 - c_{10}) \qquad \text{ if } c_{10}\geq1
\end{cases}
\]</span></p>
<p>When <span class="math inline">\(c_{10}&lt;1\)</span>, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. Because the ratio tends to be very close to <span class="math inline">\(1\)</span>, we multiply everything by <span class="math inline">\(500\)</span>, adding more contrast to the rewards.</p>
<p>When, on the other hand <span class="math inline">\(c_{10}\geq 1\)</span>, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at <span class="math inline">\(-3\)</span>.</p>
</section>
<section id="implementation-of-the-reinforce-algorithm." class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="implementation-of-the-reinforce-algorithm."><span class="header-section-number">7.4</span> Implementation of the REINFORCE algorithm.</h2>
<p>Now that everything as been defined, the REINFORCE algorithm can be applied to find an optimal policy.</p>
<section id="a-first-experiment" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="a-first-experiment"><span class="header-section-number">7.4.1</span> A first experiment</h3>
<p>We implement the REINFORCE algorithm to the test problem. There are a few hyperparameters to set.</p>
<ul>
<li>The learning rate is set to <span class="math inline">\(\alpha=2\times10^{-8}\)</span>.</li>
<li>The discount rate is set to a low value of <span class="math inline">\(\gamma = 0.1\)</span>, as the state transitions are completely random, there is no reason to prefer long term rewards.</li>
<li>Because the discount rate is so low, the episodes length is set to <span class="math inline">\(20\)</span> as we want to use the updated policy as often as possible.</li>
<li>The standard deviation of the policy parameters is set to <span class="math inline">\(\sigma = 0.1\)</span>.</li>
</ul>
<p>This leaves the choice of the initial value for <span class="math inline">\(\theta\)</span>. While it is possible for the parameters to be random, or all set to 0, we use the experiment done in chapter 4 to use. In <a href="5_solverExploration.html#fig-resratio10">Figure&nbsp;<span>4.1</span></a>, it seems that a policy of <span class="math inline">\(\alpha = 0.3\)</span> and <span class="math inline">\(\Delta t = 2\)</span> is reasonable. Since this was done only for a single set of problem parameters, we have no idea of the relationship between problem parameters and optimal solver parameters. Therefore, we only set the parameter <span class="math inline">\(\theta_4 = 0.2\)</span>, and <span class="math inline">\(\theta_5=2\)</span>, the other parameters are set to 0.</p>
<p>The algorithm is run for 40000 episode, and we observe the evolutions of the theta parameters(<a href="#fig-experimentreward">Figure&nbsp;<span>7.2</span></a>) as well as the average episode reward(<a href="#fig-experimenttheta">Figure&nbsp;<span>7.1</span></a>). Note that in the latter plot, the rolling average of the average reward over the last <span class="math inline">\(k=1000\)</span> episodes is used.</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-experimenttheta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="8_implementation_files/figure-html/fig-experimenttheta-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.1: Evolution of the theta parameters with episode number. The hyperparameters are <span class="math inline">\(\sigma\)</span>=0.1, learning rate 2e-8.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-experimentreward" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="8_implementation_files/figure-html/fig-experimentreward-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.2: Evolution of the rolling average (k=1000) of the average episode reward.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>there are problem with the results</p>
<ul>
<li>The learning rate must be kept low, otherwise things explode nastily.</li>
<li>The convergence is extremely slow, to the point were we may be wondering if it converges at all</li>
</ul>
<p>(The next sections are things I’ve done to mitigate these problem, with varying degrees of successes, the log are there, but also I need to clean up my code and rerun it for reproducibility sake as I was tinkering a lot at the time, the goal is to have a program were each hyperparameter can be chosen at the beginning/as environment variables, and making all the experiments easily reproducible).</p>
</section>
</section>
<section id="gradient-ascent-with-different-learning-parameters." class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="gradient-ascent-with-different-learning-parameters."><span class="header-section-number">7.5</span> Gradient ascent with different learning parameters.</h2>
<p>Here is how we can get better convergence by changing the learning parameters.</p>
<p>One problem is that if we look at the gradient directions, how steep they are depend on the problem parameters. And since <span class="math inline">\(n\)</span> can vary between 5 and 300, this make for a muuuuuch steeper gradient than the direction associated with <span class="math inline">\(b\)</span>, which only varies between 0 and 1. As a result, the direction associated with <span class="math inline">\(n\)</span> tend to converge but not the other. This can be remedied by applying varying learning rate in each direction. It actually works quite well! ’</p>
</section>
<section id="exploration-vs-exploitation-tradeoff." class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="exploration-vs-exploitation-tradeoff."><span class="header-section-number">7.6</span> Exploration vs exploitation tradeoff.</h2>
<p>Changing <span class="math inline">\(\sigma^2\)</span> to a higher value makes for a flatter gradient, so not only are we taking more risks, we can actually afford a higher learning rate without exploding!</p>
</section>
<section id="impact-of-initial-condition." class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="impact-of-initial-condition."><span class="header-section-number">7.7</span> Impact of initial condition.</h2>
<p>Gradient based algorithm have a tendency to converge to local minima. (in our case maxima, but same thing really), therefore, it would be interesting to see how initial policy impacts the learned policy now that we have convergence. (Not done yet, but I suspect some nasty surprises!)</p>
</section>
<section id="moving-beyond-the-basics." class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="moving-beyond-the-basics."><span class="header-section-number">7.8</span> Moving beyond the basics.</h2>
<p>(Some discussion on what to do next, etc… ) Example include.</p>
<ul>
<li>Better algorithm, with less sample variance and more sample efficiency.</li>
<li>Moving beyond a linear policy. (approximation using NN).</li>
<li>Meta learning, maybe?</li>
<li>Applying the concept learned here to a more varied set of problem, instead of just confining ourselves to steady state diffusion-convection equation.</li>
</ul>


<div id="refs" class="references csl-bib-body" role="doc-bibliography" style="display: none">
<div id="ref-lillicrap2019continuous" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T. P. Lillicrap <em>et al.</em>, <span>“Continuous control with deep reinforcement learning.”</span> 2019. Available: <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./7_policyGradient.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./9_summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>