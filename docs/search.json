[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "motivation.html",
    "href": "motivation.html",
    "title": "Motivation",
    "section": "",
    "text": "Let \\(A\\) be non singular square matrix of dimension \\(n\\geq 1\\) and let \\(b\\in\\mathbb{R}^n\\). We consider the linear system \\(Ay = b\\), where \\(y \\in \\mathbb{R}^n\\). The system has for unique solution \\(y^* = A^{-1}b\\). This is a fundamental problem to solve in numerical analysis, and there are numerous numerical methods to solve this, whether they are direct methods or iterative methods. In this thesis, we consider an iterative method. We consider the initial value problem\n\\[\ny'(t) = Ay(t)- b, \\; \\;  y(0) = y_0\n\\]\nwhere \\(y_0\\in \\mathbb{R}^n\\) and \\(t\\in \\mathbb{R}\\). Multiplying the equation by \\(e^{-At}\\), where \\(e^{-At}\\) is the usual matrix exponential, and rearranging the terms yields\n\\[\ne^{-At}y'(t) - Ae^{-At}y(t) = e^{-At}b\n\\]\nWe recognise on the left hand side the derivative of the product \\(e^{-At}y(t)\\), and thus, by the fundamental theorem of calculus,\n\\[\n\\left[ e^{-Au}y(u)\\right]_0^t = \\int_0^t -e^{-Au}b \\; du.\n\\]\nMultiplying by \\(A^{-1}A\\) inside the integral in the LHS, we get\n\\[\ne^{-At}y(t) - y_0 = A^{-1}\\left[e^{-Au}\\right]_0^t b = A^{-1}e^{-At}b - A^{-1}b.\n\\]\nMultiplying each side by \\(e^{At}\\) and rearranging the terms we get an expression for \\(y(t)\\),\n\\[\ny(t) = e^{At}(y_0 - A^{-1}b) + A^{-1}b.\n\\]\nNote that each of those step can be taken backward , which means that the solution we have is unique. We have thus proved\n\nTheorem 1 Let \\(A\\) be a non singular, square matrix of dimension \\(n\\geq 1\\), \\(b\\in\\mathbb{R}^n\\) a vector, and consider the initial value problem\n\\[\ny'(t) = Ay(t) - b, \\; y(0) = y_0\n\\tag{1}\\]\nwhere \\(t \\rightarrow y(t)\\) is a function from \\(\\mathbb{R}\\) to \\(\\mathbb{R}^n\\). Then the problem has a unique solution in the form of\n\\[\ny(t) = e^{At}(y_0 - y^*) + y^*,\n\\]\nwhere \\(y^* = A^{-1}b\\), and \\(e^{At}\\) is defined using the usual matrix exponential.\n\nLet \\(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_n\\) be the (not necessarly distinct) eigenvalues of \\(A\\), write \\(\\lambda_i = a_i + iy_i\\), where \\(a_i,b_i \\in \\mathbb{R}\\) are respectively the real part and the imaginary parts of the \\(i^{\\text{th}}\\) eigenvalue. The following holds\n\nTheorem 2 \\(y(t) \\to y^*\\) as \\(t \\to +\\infty\\) for any initial value \\(y_0\\) if and only if, for all \\(i = 1 , \\dots , n\\), \\(a_i <0\\), that is, all the eigenvalues of \\(A\\) have a strictly negative real part.\n\n\nProof. (In the diagonalisable case)\nWe assume that \\(A\\) is diagonalisable. Write \\(A = P\\Delta P^{-1}\\) where \\(\\Delta\\) is diagonal.\n\\[\n\\Delta = \\begin{pmatrix}   \n\\lambda_1 & & &\\\\\n&\\lambda_2 & &  \\\\\n&& \\ddots & \\\\\n&&& \\lambda_n\n\\end{pmatrix}\n\\]\nThen \\(e^{At} = Pe^{\\Delta t} P^{-1}\\), where\n\\[\ne^{\\Delta t} = \\begin{pmatrix}   \ne^{\\lambda_1 t} & & &\\\\\n&e^{\\lambda_2 t} & &  \\\\\n&& \\ddots & \\\\\n&&& e^{\\lambda_n t}\n\\end{pmatrix}\n\\]\nLet \\(z(t) = P^{-1}(y(t)-y^*)\\), where \\(y(t)\\) is the unique solution to Equation 1 for some arbitrary initial value \\(y_0\\).\nSince \\(P\\) is non singular, \\(y(t) \\to y^*\\) if and only if \\(z(t) \\to 0\\). We have\n\\[\nz(t) = P^{-1} e^{At}(y_0-y^*)\n\\]\nWe note that \\(P^{-1} e^{At} = e^{\\Delta t} P^{-1}\\), thus\n\\[\nz(t) = e^{\\Delta t} P^{-1}(y_0-y^*).\n\\]\nLooking at the \\(i^{\\text{th}}\\) element \\(z(t)_i\\), we have\n\\[\n|z(t)_i| = e^{a_it}\\left( P^{-1}(y_0-y^*)\\right)_i\n\\]\nwhere \\(a_i = \\Re[\\lambda_i]\\). Clearly, if \\(a_i < 0\\), \\(z(t)_i \\to 0\\) as \\(t \\to +\\infty\\). If this holds for any \\(i = 1, \\dots , n\\), then \\(z(t) \\to 0\\) as \\(t\\to +\\infty\\). This proves (\\(\\Leftarrow\\)).\nThis is also a necessary condition. Indeed, since \\(y_0\\) is arbitrary, we can chose it so that \\(P^{-1}(y_0-y^*) = (1, \\dots , 1)^T\\). Then \\(z(t) = (e^{\\lambda_1 t}, e^{\\lambda_2 t}, \\dots , e^{\\lambda_n t})^T\\) which converges to \\(0\\) only if all the eigenvalues have a strictly negative real part.\n\n\nRemark. A general proof is available on (Bellman 1953, chap. 1)\n\nWe now go back to the original problem of solving the linear system \\(Ay = b\\). If all the eigenvalues of \\(A\\) have a strictly negative real part, then, any numerical solver for the initial value problem \\(y'(t) = Ay(t) - b\\) with \\(y(0) = y_0\\) where \\(t\\) is some pseudo-time variable also becomes an iterative solver for the linear system \\(Ay = b\\), as \\(y(t) \\to y^*\\).\n\nRemark. If all the eigenvalues of \\(A\\) have a strictly positive real part, then we can simply solve \\(y' = (-A)y - (-b) = -Ay+b\\) instead.\n\n\n\n\n\nBellman, Richard Ernest. 1953. Stability Theory of Differential Equations /. New York : McGraw-Hill,."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "Part1.html",
    "href": "Part1.html",
    "title": "Testing ground for bachelor thesis",
    "section": "",
    "text": "Explicit RK2 and stability function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass testProblem:\n## Define it as \n    def __init__(self,b,n) -> None:\n        self.n = n\n        self.b = b\n        self.deltaX = 1 / (n+1)\n        self.M = self.buildM(b,n,self.deltaX)\n        self.e = self.buildE(n, self.deltaX)\n\n\n    def buildM(self,b,n,deltaX):\n        \"\"\"\n        we go from u0 to u(n+1).\n        \"\"\"\n        deltaX = 1 / (n+1)\n        A = deltaX *(np.eye(n) -1 * np.eye(n,k = -1))\n        B = b* (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))\n        return A-B\n    \n    def buildE(self,n,deltaX):\n        return deltaX**2 *np.ones(n)\n    \n    def f(self,y):\n        return self.e - self.M@y\n    \n    def oneStepSmoother(self,y,t,deltaT,alpha):\n        \"\"\"\n        Perform one pseudo time step deltaT of the solver for the diff eq\n        y' = e - My = f(y). .\n        \"\"\"\n        k1 = self.f(y)\n        k2 = self.f(y + alpha*deltaT*k1)\n        yNext = y + deltaT*k2\n        return yNext\n    \n    def findOptimalParameters(self):\n        #This is where the reinforcement learning algorithm \n        #take place in\n        return 0 , 0\n    \n\n    def mainSolver(self,n_iter = 10):\n        \"\"\" Main solver for the problem, calculate the approximated solution\n        after n_iter pseudo time steps. \"\"\"\n        resNormList = np.zeros(n_iter+1)\n        t = 0\n        #Initial guess y = e\n        y = np.ones(e)\n        resNormList[0] = np.linalg.norm(self.M@y-self.e)\n        ##Finding the optimal params\n        alpha, deltaT = self.findOptimalParameters()\n        ##Will need to be removed, just for debugging\n        alpha = 0.5\n        deltaT = 0.00006\n        #For now, we use our best guess\n        for i in range(n_iter):\n            y = self.oneStepSmoother(y,t,deltaT,alpha)\n            t += deltaT\n            resNorm = np.linalg.norm(self.M@y - self.e)\n            resNormList[i+1] = resNorm\n        return y , resNormList\n\n    def mainSolver2(self,alpha, deltaT, n_iter = 10):\n        \"\"\" Like the main solver, except we give \n        the parameters explicitely \"\"\"\n        y = np.array([0.00719735, 0.01434065, 0.02142834, 0.02845879, 0.03543034,\n       0.04234128, 0.04918987, 0.05597432, 0.06269281, 0.06934346,\n       0.07592437, 0.08243356, 0.08886904, 0.09522877, 0.10151064,\n       0.10771254, 0.11383227, 0.11986762, 0.1258163 , 0.13167601,\n       0.13744437, 0.14311899, 0.1486974 , 0.15417709, 0.15955552,\n       0.16483009, 0.16999815, 0.175057  , 0.1800039 , 0.18483606,\n       0.18955064, 0.19414475, 0.19861545, 0.20295975, 0.20717461,\n       0.21125694, 0.21520361, 0.21901143, 0.22267715, 0.22619749,\n       0.22956911, 0.23278861, 0.23585255, 0.23875743, 0.24149971,\n       0.24407579, 0.246482  , 0.24871466, 0.25076999, 0.25264419,\n       0.25433339, 0.25583367, 0.25714106, 0.25825152, 0.25916098,\n       0.25986529, 0.26036025, 0.26064162, 0.26070509, 0.26054628,\n       0.26016077, 0.25954408, 0.25869166, 0.25759892, 0.25626119,\n       0.25467375, 0.25283181, 0.25073051, 0.24836496, 0.24573017,\n       0.2428211 , 0.23963265, 0.23615963, 0.23239681, 0.22833888,\n       0.22398045, 0.21931606, 0.2143402 , 0.20904726, 0.20343156,\n       0.19748735, 0.1912088 , 0.18458999, 0.17762494, 0.17030756,\n       0.16263169, 0.15459109, 0.14617941, 0.13739022, 0.12821701,\n       0.11865315, 0.10869192, 0.09832652, 0.08755003, 0.07635541,\n       0.06473555, 0.05268319, 0.04019099, 0.02725147, 0.01385704])\n        resNormList = np.zeros(n_iter+1)\n        t = 0\n        #Initial guess y = e\n        #y = np.ones(n)\n        resNormList[0] = np.linalg.norm(self.M@y-self.e)\n        #For now, we use our best guess\n        for i in range(n_iter):\n            y = self.oneStepSmoother(y,t,deltaT,alpha)\n            t += deltaT\n            resNorm = np.linalg.norm(self.M@y - self.e)\n            resNormList[i+1] = resNorm\n        return y , resNormList\n\n\nWe now have everything we need to get going, let’s plot the residual norm over iteration as a first test\n\n\nCode\n#Create the object\nb = 0.5\nn = 100\n\nalpha = 0.13813813813813813\ndeltaT = 3.5143143143143143\nconvDiffProb = testProblem(b,n)\ny, resNormList = convDiffProb.mainSolver2(0.093093,5.6003,20)\n\nx = np.linspace(0,1,n+2) #Create space\nyTh = np.zeros(n+2)\nyTh[1:n+1] = np.linalg.solve(convDiffProb.M,convDiffProb.e)\n\nyApprox = np.zeros(n+2)\nyApprox[1:n+1] = y\nfig, (ax1,ax2) = plt.subplots(1,2)\n\nax1.plot(resNormList)\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Residual norm\")\nax1.set_yscale('log')\n\nax2.plot(x,yTh,label = 'Discretised solution')\nax2.plot(x,yApprox,label = \"iterative solution\")\nax2.legend()\n\nfig.show()\n\n\n/tmp/ipykernel_6054/2529022355.py:27: UserWarning:\n\nMatplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n\n\n\n\n\n\nEvolution of the residual norm over a number of iteration.\n\n\n\n\n\n\nCode\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\ndef resRatio(resNormList):\n    return resNormList[-1] / resNormList[-2]\n\n\n\nl = 100\ndeltaTgrid = np.linspace(0.9,10,l)\nalphaGrid = np.linspace(0,1,l)\n\ndeltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)\n\nresRatioGrid2 = np.zeros((l,l))\n\nfor i in range(l):\n    print(i)\n    for j in range(l):\n        #print('alpha', alphaGrid[j,0])\n        #print('deltaT', deltaTgrid[0,i])\n        y , resNormList = convDiffProb.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)\n        ratio = resRatio(resNormList)\n        #print('ratio', ratio)\n        resRatioGrid2[j,i] = resRatio(resNormList)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n\nclippedRatio = np.clip(resRatioGrid2,0.8,0.9)\nsurf = ax.contour(deltaTgrid,alphaGrid,clippedRatio,levels = [0.8,0.85,0.9])\n\ntransformedContour = np.log(1/(1+np.exp(-clippedRatio+1)))\n\n\n\nprint(np.nanmin(resRatioGrid2))\nprint(np.argmin(resRatioGrid2))\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n\n\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n\n\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n\n\n90\n91\n92\n93\n94\n95\n96\n97\n98\n\n\n99\n0.9971231634706321\n952\n\n\n/tmp/ipykernel_6054/930702461.py:31: UserWarning:\n\nNo contour levels were found within the data range.\n\n\n\n\n\n\nContour plot\n\n\nCode\nfig, ax = plt.subplots()\ncp = ax.contour(deltaTgrid,alphaGrid,resRatioGrid2,levels = [0.83,0.86,0.88,0.9,1], cmap=cm.coolwarm, linewidth=0)\nax.clabel(cp)\n#ax.view_init(elev = 90,azim = 150)\nplt.show()\n\n\n/tmp/ipykernel_6054/383841379.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'linewidth'\n\n\n\n\n\n\nSurface plot\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(deltaTgrid,alphaGrid,np.clip(resRatioGrid2,0.5,1), cmap=cm.coolwarm, linewidth=0)\nax.view_init(elev = 90,azim = 180)\nplt.show()\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n# Make data.\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y)\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R)\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\n# Customize the z axis.\nax.set_zlim(-1.01, 1.01)\nax.zaxis.set_major_locator(LinearLocator(10))\n# A StrMethodFormatter is used automatically\nax.zaxis.set_major_formatter('{x:.02f}')\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.show()\n\n\n\n\n\n\n\nCode\nimport sys\nprint(sys.executable)\n\n\n/home/melanie/anaconda3/bin/python\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nNecessary functions go here.\n\n\nCode\ndef RK2(f,y,t,deltaT,alpha,**args):\n    \"\"\"Second order family of Rk2\n    c = [0,alpha], bT = [1-1/(2alpha), 1/(2alpha)] , a2,1 = alpha \"\"\"\n    k1 = f(t,y,**args)\n    k2 = f(t + alpha*deltaT, y + alpha*deltaT*k1,**args)\n    yNext = y + deltaT*(k1*(1-1/(2*alpha)) + k2 * 1/(2*alpha))\n    return yNext\n    \ndef buildM(b,n):\n    \"\"\"\n    we go from u0 to u(n+1).\n    \"\"\"\n    deltaX = 1 / (n+1)\n    A = 1/deltaX *(np.eye(n) -1 * np.eye(n,k = -1))\n    B = b/deltaX**2 * (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))\n    return A-B\n\ndef buildE(n):\n    return np.ones(n)\n\ndef f(t,y,M,e):\n    return e - M@y\n\n\ndef mainSolver(deltaT, alpha,b,f = f,n_iter = 10,n_points=100):\n    t = 0\n    e = buildE(n_points)\n    M = buildM(b,n_points)\n    #First guess\n    y = np.copy(e)\n    resNorm = np.linalg.norm(M@y -e)\n\n    for i in range(n_iter):\n        y = RK2(f,y,t,deltaT,alpha,M = M,e = e)\n        t += deltaT\n        lastResNorm , resNorm = resNorm ,  np.linalg.norm(M@y - e)\n    return resNorm / lastResNorm\n\n\n\n\nCode\nmainSolver(0.0001,0.5,0.5)\n\n\n0.9678775609609744\n\n\nTo facilitate everything, we discretise the space with 100 interior points only, and with parameter \\(b = 0.5\\).\nThis is how the solution looks like with the discretisation\n\n\nCode\nb = 0.5\nn = 100\n\nM = buildM(b,n)\ne = buildE(n)\n\nx = np.linspace(0,1,n+2)\nx2 = np.linspace(0,1,n)\n\nanalyticSol = x - (np.exp(-(1-x)/b)-np.exp(-1/b))/(1-np.exp(-1/b))\nu = np.linalg.solve(M,e)\n\nplt.plot(x,analyticSol,label = 'Analytical solution')\nplt.plot(x2,u,label = 'Discretised solution')\nplt.legend()\n\n\n<matplotlib.legend.Legend at 0x7f1f1976ea90>\nDiscretised solution vs analytical solution\n\n\n\n\n\nHow would changing the parameters affect the residuals ratio after 10 iterations?\n\n\nCode\ndeltaTGrid = np.linspace(0.00001,0.0001,100)\n\nratio = np.zeros(100)\ni = 0\nfor deltaT in deltaTGrid:\n    ratio[i] = mainSolver(deltaT,0.6,0.5)\n    i+=1\n\nplt.plot(deltaTGrid,ratio)\nplt.xlabel('Delta T')\nplt.ylabel('Ratio')\n\n\nText(0, 0.5, 'Ratio')\nImpact of the choice of time step with the residual ratios.\n\n\n\n\n\nHow would changing the RK parameter change the residual ratio after 10 iterations? Here we take the optimal delta T we found earlier.\n\n\nCode\n#fig-cap: Changing alpha does not do much...\nalphaGrid = np.linspace(0.01,0.99,100)\n\nratio = np.zeros(100)\ni = 0\nfor alpha in alphaGrid:\n    ratio[i] = mainSolver(0.00007,alpha,0.5)\n    i+=1\n\nplt.plot(alphaGrid,ratio)\nplt.xlabel('alpha')\nplt.ylabel('Ratio')\n\n\nText(0, 0.5, 'Ratio')\n\n\n\n\n\nPendulum test\n\n\nCode\ndef f(t,y):\n    g = 9.81\n    l = 1\n    f1 = y[1]\n    f2 = -g/l* np.sin(y[0])\n    return np.array([f1,f2])\n\n\n\n#Pendulum\ndeltaT = 0.01\nt_min = 0\nn = 1000\nt = t_min\ntArray = np.zeros(n+1)\ntArray[0] = t\ny = np.array([np.pi/2,0])\nyArray = np.zeros((n+1,2))\nyArray[0] = y\nfor i in range(n):\n    y = RK2(f,y,t,deltaT,0.9)\n    t+=deltaT\n    tArray[i+1] = t\n    yArray[i+1] = y\n\n\nplt.plot(tArray,yArray[:,0])"
  }
]