[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ThesisBook",
    "section": "",
    "text": "Preface\nThis is a Quarto book. tests\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "motivation.html",
    "href": "motivation.html",
    "title": "2  Motivation",
    "section": "",
    "text": "Let \\(A\\) be non singular square matrix of dimension \\(n\\geq 1\\) and let \\(b\\in\\mathbb{R}^n\\). We consider the linear system \\(Ay = b\\), where \\(y \\in \\mathbb{R}^n\\). The system has for unique solution \\(y^* = A^{-1}b\\). This is a fundamental problem to solve in numerical analysis, and there are numerous numerical methods to solve this, whether they are direct methods or iterative methods. In this thesis, we consider an iterative method. We consider the initial value problem\n\\[\ny'(t) = Ay(t)- b, \\; \\;  y(0) = y_0\n\\]\nwhere \\(y_0\\in \\mathbb{R}^n\\) and \\(t\\in \\mathbb{R}\\). Multiplying the equation by \\(e^{-At}\\), where \\(e^{-At}\\) is the usual matrix exponential, and rearranging the terms yields\n\\[\ne^{-At}y'(t) - Ae^{-At}y(t) = e^{-At}b\n\\]\nWe recognise on the left hand side the derivative of the product \\(e^{-At}y(t)\\), and thus, by the fundamental theorem of calculus,\n\\[\n\\left[ e^{-Au}y(u)\\right]_0^t = \\int_0^t -e^{-Au}b \\; du.\n\\]\nMultiplying by \\(A^{-1}A\\) inside the integral in the LHS, we get\n\\[\ne^{-At}y(t) - y_0 = A^{-1}\\left[e^{-Au}\\right]_0^t b = A^{-1}e^{-At}b - A^{-1}b.\n\\]\nMultiplying each side by \\(e^{At}\\) and rearranging the terms we get an expression for \\(y(t)\\),\n\\[\ny(t) = e^{At}(y_0 - A^{-1}b) + A^{-1}b.\n\\]\nNote that each of those step can be taken backward , which means that the solution we have is unique. We have thus proved\n\nTheorem 2.1 Let \\(A\\) be a non singular, square matrix of dimension \\(n\\geq 1\\), \\(b\\in\\mathbb{R}^n\\) a vector, and consider the initial value problem\n\\[\ny'(t) = Ay(t) - b, \\; y(0) = y_0\n\\tag{2.1}\\]\nwhere \\(t \\rightarrow y(t)\\) is a function from \\(\\mathbb{R}\\) to \\(\\mathbb{R}^n\\). Then the problem has a unique solution in the form of\n\\[\ny(t) = e^{At}(y_0 - y^*) + y^*,\n\\]\nwhere \\(y^* = A^{-1}b\\), and \\(e^{At}\\) is defined using the usual matrix exponential.\n\nLet \\(\\lambda_1 , \\lambda_2 , \\dots , \\lambda_n\\) be the (not necessarly distinct) eigenvalues of \\(A\\), write \\(\\lambda_i = a_i + iy_i\\), where \\(a_i,b_i \\in \\mathbb{R}\\) are respectively the real part and the imaginary parts of the \\(i^{\\text{th}}\\) eigenvalue. The following holds\n\nTheorem 2.2 \\(y(t) \\to y^*\\) as \\(t \\to +\\infty\\) for any initial value \\(y_0\\) if and only if, for all \\(i = 1 , \\dots , n\\), \\(a_i <0\\), that is, all the eigenvalues of \\(A\\) have a strictly negative real part.\n\n\nProof. (In the diagonalisable case)\nWe assume that \\(A\\) is diagonalisable. Write \\(A = P\\Delta P^{-1}\\) where \\(\\Delta\\) is diagonal.\n\\[\n\\Delta = \\begin{pmatrix}   \n\\lambda_1 & & &\\\\\n&\\lambda_2 & &  \\\\\n&& \\ddots & \\\\\n&&& \\lambda_n\n\\end{pmatrix}\n\\]\nThen \\(e^{At} = Pe^{\\Delta t} P^{-1}\\), where\n\\[\ne^{\\Delta t} = \\begin{pmatrix}   \ne^{\\lambda_1 t} & & &\\\\\n&e^{\\lambda_2 t} & &  \\\\\n&& \\ddots & \\\\\n&&& e^{\\lambda_n t}\n\\end{pmatrix}\n\\]\nLet \\(z(t) = P^{-1}(y(t)-y^*)\\), where \\(y(t)\\) is the unique solution to Equation 2.1 for some arbitrary initial value \\(y_0\\).\nSince \\(P\\) is non singular, \\(y(t) \\to y^*\\) if and only if \\(z(t) \\to 0\\). We have\n\\[\nz(t) = P^{-1} e^{At}(y_0-y^*)\n\\]\nWe note that \\(P^{-1} e^{At} = e^{\\Delta t} P^{-1}\\), thus\n\\[\nz(t) = e^{\\Delta t} P^{-1}(y_0-y^*).\n\\]\nLooking at the \\(i^{\\text{th}}\\) element \\(z(t)_i\\), we have\n\\[\n|z(t)_i| = e^{a_it}\\left( P^{-1}(y_0-y^*)\\right)_i\n\\]\nwhere \\(a_i = \\Re[\\lambda_i]\\). Clearly, if \\(a_i < 0\\), \\(z(t)_i \\to 0\\) as \\(t \\to +\\infty\\). If this holds for any \\(i = 1, \\dots , n\\), then \\(z(t) \\to 0\\) as \\(t\\to +\\infty\\). This proves (\\(\\Leftarrow\\)).\nThis is also a necessary condition. Indeed, since \\(y_0\\) is arbitrary, we can chose it so that \\(P^{-1}(y_0-y^*) = (1, \\dots , 1)^T\\). Then \\(z(t) = (e^{\\lambda_1 t}, e^{\\lambda_2 t}, \\dots , e^{\\lambda_n t})^T\\) which converges to \\(0\\) only if all the eigenvalues have a strictly negative real part.\n\n\nRemark. A general proof is available on (Bellman 1953, chap. 1)\n\nWe now go back to the original problem of solving the linear system \\(Ay = b\\). If all the eigenvalues of \\(A\\) have a strictly negative real part, then, any numerical solver for the initial value problem \\(y'(t) = Ay(t) - b\\) with \\(y(0) = y_0\\) where \\(t\\) is some pseudo-time variable also becomes an iterative solver for the linear system \\(Ay = b\\), as \\(y(t) \\to y^*\\).\n\nRemark. If all the eigenvalues of \\(A\\) have a strictly positive real part, then we can simply solve \\(y' = (-A)y - (-b) = -Ay+b\\) instead.\n\n\n\n\n\nBellman, Richard Ernest. 1953. Stability Theory of Differential Equations /. New York : McGraw-Hill,."
  },
  {
    "objectID": "testEqIntro.html",
    "href": "testEqIntro.html",
    "title": "3  A test problem - Convection diffusion equation",
    "section": "",
    "text": "As a test case for the solver, we consider the steady state convection-diffusion equation.\n\\[\n    u_x = bu_{xx} + 1\n\\]\nwhere \\(b\\) is some physical parameter and \\(u(x)\\) is defined on the interval \\([0,1]\\). The boundary condition are given by \\(u(0) = u(1) = 0\\). This equation has an analytical solution that is given by\n\\[\nu(x) = x - \\frac{e^{-(1-x)/b} - e^{-1/b}}{1-e^{-1/b}}.\n\\]\nWe are however interested in solving this numerically, with a finite difference approach. We partition the interval \\([0,1]\\) into equidistant points \\(x_i, i = 0, \\dots n\\). We note the distance between each points as \\(\\Delta x\\), and we have \\(u(x_0) = u(0) = 0\\) and \\(u(x_n) = u(1) = 0\\). We use the notation \\(u^i = u(x_i)\\). We approximate, for \\(i \\geq 1\\) the derivative\n\\[\n    u_x^i = \\frac{u^i - u^{i-1}}{\\Delta x}\n\\] and the second order derivative is approximated by \\[\n    u^i_{xx} = \\frac{u^{i+1} - 2u^i + u^{i-1}}{\\Delta x ^2}\n\\]\nNote that the first derivative is approximated backward in time. For \\(i = 1 , \\dots , n-1\\), we thus have the approximation\n\\[\nu_x^i =  \\frac{u^i - u^{i-1}}{\\Delta x}  = b \\frac{u^{i+1} - 2u^i + u^{i-1}}{\\Delta x ^2} + 1\n\\]\nThis can be given in matrix format by letting \\(u = (u^1,\\dots, u^{n-1})^T\\)\n\\[\n    Au = Bu + e\n\\]\nwhere \\(e = (1,1,\\dots , 1)^T\\),\n\\[\nA = \\frac{1}{\\Delta x}\\begin{bmatrix}\n    1 &  &&&\\\\\n    -1 & 1 &&\\\\\n    & -1 & 1 &\\\\\n    &&\\ddots & \\ddots &\\\\\n    &&&-1& 1\n\\end{bmatrix}    \n\\]\nand\n\\[\nB =  \\frac{b}{\\Delta x ^2}\\begin{bmatrix}\n    -2 & 1 &&&\\\\\n    1 & \\ddots & \\ddots &&\\\\\n    & \\ddots & \\ddots & \\ddots&\\\\\n    && \\ddots & \\ddots & 1 &\\\\\n    &&&1 & -2  \n\\end{bmatrix} .\n\\]\nWith \\(M = A-B\\), we have to solve the linear system\n\\[\nMu = e\n\\tag{3.1}\\]\nwhere \\(M\\) is a square matrix of dimension \\((n-1) \\times (n-1)\\) and \\(e\\) is the one vector of dimension \\(n-1\\).\n\nRemark. (To make better but I think it works) It is apparent that \\(M\\) is diagonally dominant. Since all elements of the diagonal are positive, then so are the eigenvalues real part. Assuming \\(M\\) is non singular, we have that \\(-M\\) is stable.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom testProblemClass import testProblem\n\nn = 50\nb = 0.09\nproblem = testProblem(b,n)\n\nM = problem.M\ne = problem.e\n\ny = np.linalg.solve(M,e)\n\nx = np.linspace(0,1,n)\ny_th = x - (np.exp(-(1-x)/b) - np.exp(-1/b))/(1-np.exp(-1/b))\n\nfig , ax = plt.subplots()\nax.plot(x,y_th,label='theoretical solution')\nax.plot(x,y,label='discretized solution')\nax.legend()\nax.set_xlabel('x')\nax.set_ylabel('u(x)')\n\nplt.show()\n\n\n\n\n\nTheoretical and discretized solution of the problem with \\(b = 0.09\\), \\(n = 50\\).\n\n\n\n\nTo solve this linear system, we use the method highlighted before. To make it easier for later, we chose to scale \\(M\\) so that its diagonal elements are \\(1\\). This allows us to have all eigenvalues in the circle centered around \\(1\\) with radius \\(1\\) independently of the parametrisation. Setting \\(\\gamma = \\frac{1}{\\Delta x} + \\frac{2b}{\\Delta x^2}\\), solving Equation 3.1 is equivalent to solving the system\n\\[\nNu = b\n\\]\nwhere with \\(N = M/\\gamma\\), \\(b = e/\\gamma\\). The eigenvalues of \\(M\\) are also scaled by \\(1/\\gamma\\) so \\(-N\\) is stable, assuming it is non singular. We are now ready to solve the system iteratively using ODE solver.\nWe thus introduce a pseudo time variable \\(t\\) and we consider the ODE.\n\\[\nu'(t) = b - Nu(t)\n\\]\nAssuming \\(N\\) is non singular, we can use Theorem 2.2 to guarantee that the ODE will converge to a steady state independently of the initial value we chose. In the next chapter, we will apply the Runge-Kutta scheme we saw earlier to the problem and will see how parameters changes results."
  },
  {
    "objectID": "solverExploration.html",
    "href": "solverExploration.html",
    "title": "4  Runge Kutta solver applied to the test problem",
    "section": "",
    "text": "We now have to solve the ODE \\(u' = b - Nu\\) where \\(M\\) depends on the problem parameters \\(b\\) and \\(\\Delta x = 1 / (n+1)\\), where \\(n\\) is the chosen number of subdivisions of \\([0,1]\\). Since we are only interested on the asymptotic behavior of \\(u\\), we only need to care about the stability of the numerical solver we wish to use. We consider the following RK scheme with two stages.\nblablbal\nThis solver has two parameters \\(\\Delta t\\) and \\(\\alpha\\). The objective is for the solver to converge to a steady state solution as fast as possible. Set \\(u_0 = u(0) = e\\) as an initial value. We define the relative residual after \\(k\\) steps as\n\\[\nr_k = ||Nu_k - b||/ ||b||.\n\\]\nwhere \\(||.||\\) is the 2-norm.\nIf the solver we chose is stable, then \\(||r_k|| \\to 0\\) as \\(k \\to \\infty\\). We define now the convergence at step \\(n\\) to be the ratio of residual at step \\(k\\) and \\(k-1\\). That is\n\\[\nc_k = \\frac{||r_k||}{||r_{k-1}||} = \\frac{||Mu_k - e||}{||Mu_{k-1}-e||}\n\\]\nwhere \\(||.||\\) is the 2-norm.\n\n\n\n\n\nEvolution of the residual norm over iteration, with problem parameters \\(n = 100\\) and \\(b = 0.05\\), and RK parameters \\(\\Delta t = 1.6\\) and \\(\\alpha = 0.3\\).\n\n\n\n\n# A small experiment.\nWe are interested in finding the best parameters \\((\\Delta t, \\alpha)\\) to use for some specific problem parameters \\((b, n)\\). Since the residual ratio vary quite a bit depending on the number of iteration, we decide to investigate the residual ratio after 10 iterations and 100 iterations. So, for the problem parameters \\(b = 0.05\\), and \\(n = 100\\), we plot \\(c_{10 }= f(\\Delta t, \\alpha)\\) and \\(c_{100} = g(\\Delta t, \\alpha)\\). We wish to answer the following questions\n\nWhere are the optimal parameters for this specific problem, that is, the ones that minimize \\(c_{10}\\) and \\(c_{100}\\), and do they also depend on the iteration number or not.\nWhat do these functions look like. In particular, we may be interested in the function convexity.\n\nIn both cases, we use a contour plot\n\n\nCode\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\ndef resRatio(resNormList):\n    return resNormList[-1] / resNormList[-2]\n\n\n\nl = 100\ndeltaTgrid = np.linspace(0.1,4.3,l)\nalphaGrid = np.linspace(0,1,l)\n\ndeltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)\n\nresRatioGrid2 = np.zeros((l,l))\n\n#Version 10\nfor i in range(l):\n    for j in range(l):\n        #print('alpha', alphaGrid[j,0])\n        #print('deltaT', deltaTgrid[0,i])\n        y , resNormList = problem.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)\n        ratio = resRatio(resNormList)\n        #print('ratio', ratio)\n        resRatioGrid2[j,i] = resRatio(resNormList)\n\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(deltaTgrid,alphaGrid,np.clip(resRatioGrid2,0.5,1), cmap=cm.coolwarm, linewidth=0)\nax.view_init(elev = 90,azim = 180)\nplt.show()\n\n\n\n\n\n\n\nCode\nimport plotly.graph_objects as go\n\nfig = go.Figure(data = \n    go.Contour(\n        z = np.clip(resRatioGrid2,0.5,1),\n        y = alphaGrid[:,0],\n        x = deltaTgrid[0,:],\n        colorbar=dict(\n            title='Residual ratio, after 10 iter.', # title here\n            titleside='right',\n            titlefont=dict(\n            size=14,\n            family='Arial, sans-serif'))\n    ))\n\nfig.update_layout(\n    xaxis_title = 'Delta t',\n    yaxis_title = 'alpha'\n)"
  },
  {
    "objectID": "Part1.html",
    "href": "Part1.html",
    "title": "5  Testing ground for bachelor thesis",
    "section": "",
    "text": "6 Explicit RK2 and stability function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom testProblemClass import testProblem\n\n\nWe now have everything we need to get going, let’s plot the residual norm over iteration as a first test\n\n\nCode\n#Create the object\nb = 0.5\nn = 100\n\nalpha = 0.13813813813813813\ndeltaT = 3.5143143143143143\nconvDiffProb = testProblem(b,n)\ny, resNormList = convDiffProb.mainSolver2(0.093093,5.6003,20)\n\nx = np.linspace(0,1,n+2) #Create space\nyTh = np.zeros(n+2)\nyTh[1:n+1] = np.linalg.solve(convDiffProb.M,convDiffProb.e)\n\nyApprox = np.zeros(n+2)\nyApprox[1:n+1] = y\nfig, (ax1,ax2) = plt.subplots(1,2)\n\nax1.plot(resNormList)\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Residual norm\")\nax1.set_yscale('log')\n\nax2.plot(x,yTh,label = 'Discretised solution')\nax2.plot(x,yApprox,label = \"iterative solution\")\nax2.legend()\n\nfig.show()\n\n\n/tmp/ipykernel_2105/2529022355.py:27: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\nEvolution of the residual norm over a number of iteration.\n\n\n\n\n\n\nCode\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\ndef resRatio(resNormList):\n    return resNormList[-1] / resNormList[-2]\n\n\n\nl = 100\ndeltaTgrid = np.linspace(0.000001,0.00001,l)\nalphaGrid = np.linspace(0,1,l)\n\ndeltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)\n\nresRatioGrid2 = np.zeros((l,l))\n\nfor i in range(l):\n    for j in range(l):\n        #print('alpha', alphaGrid[j,0])\n        #print('deltaT', deltaTgrid[0,i])\n        y , resNormList = convDiffProb.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)\n        ratio = resRatio(resNormList)\n        #print('ratio', ratio)\n        resRatioGrid2[j,i] = resRatio(resNormList)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n\nclippedRatio = np.clip(resRatioGrid2,0.8,0.9)\nsurf = ax.contour(deltaTgrid,alphaGrid,clippedRatio,levels = [0.8,0.85,0.9])\n\ntransformedContour = np.log(1/(1+np.exp(-clippedRatio+1)))\n\n\n\nprint(np.nanmin(resRatioGrid2))\nprint(np.argmin(resRatioGrid2))\n\n\n0.9999999492259216\n99\n\n\n\n\n\nContour plot\n\n\nCode\nfig, ax = plt.subplots()\ncp = ax.contour(deltaTgrid,alphaGrid,resRatioGrid2,levels = [0.83,0.86,0.88,0.9,1], cmap=cm.coolwarm, linewidth=0)\nax.clabel(cp)\n#ax.view_init(elev = 90,azim = 150)\nplt.show()\n\n\n/tmp/ipykernel_2105/383841379.py:2: UserWarning: The following kwargs were not used by contour: 'linewidth'\n  cp = ax.contour(deltaTgrid,alphaGrid,resRatioGrid2,levels = [0.83,0.86,0.88,0.9,1], cmap=cm.coolwarm, linewidth=0)\n\n\n\n\n\nSurface plot\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(deltaTgrid,alphaGrid,np.clip(resRatioGrid2,0.5,1), cmap=cm.coolwarm, linewidth=0)\nax.view_init(elev = 90,azim = 180)\nplt.show()\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n# Make data.\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y)\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R)\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\n# Customize the z axis.\nax.set_zlim(-1.01, 1.01)\nax.zaxis.set_major_locator(LinearLocator(10))\n# A StrMethodFormatter is used automatically\nax.zaxis.set_major_formatter('{x:.02f}')\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.show()\n\n\n\n\n\n\n\nCode\nimport sys\nprint(sys.executable)\n\n\n/home/melanie/.pyenv/versions/3.11.0/bin/python\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nNecessary functions go here.\n\n\nCode\ndef RK2(f,y,t,deltaT,alpha,**args):\n    \"\"\"Second order family of Rk2\n    c = [0,alpha], bT = [1-1/(2alpha), 1/(2alpha)] , a2,1 = alpha \"\"\"\n    k1 = f(t,y,**args)\n    k2 = f(t + alpha*deltaT, y + alpha*deltaT*k1,**args)\n    yNext = y + deltaT*(k1*(1-1/(2*alpha)) + k2 * 1/(2*alpha))\n    return yNext\n    \ndef buildM(b,n):\n    \"\"\"\n    we go from u0 to u(n+1).\n    \"\"\"\n    deltaX = 1 / (n+1)\n    A = 1/deltaX *(np.eye(n) -1 * np.eye(n,k = -1))\n    B = b/deltaX**2 * (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))\n    return A-B\n\ndef buildE(n):\n    return np.ones(n)\n\ndef f(t,y,M,e):\n    return e - M@y\n\n\ndef mainSolver(deltaT, alpha,b,f = f,n_iter = 10,n_points=100):\n    t = 0\n    e = buildE(n_points)\n    M = buildM(b,n_points)\n    #First guess\n    y = np.copy(e)\n    resNorm = np.linalg.norm(M@y -e)\n\n    for i in range(n_iter):\n        y = RK2(f,y,t,deltaT,alpha,M = M,e = e)\n        t += deltaT\n        lastResNorm , resNorm = resNorm ,  np.linalg.norm(M@y - e)\n    return resNorm / lastResNorm\n\n\n\n\nCode\nmainSolver(0.0001,0.5,0.5)\n\n\n0.9678775609609779\n\n\nTo facilitate everything, we discretise the space with 100 interior points only, and with parameter \\(b = 0.5\\).\nThis is how the solution looks like with the discretisation\n\n\nCode\nb = 0.5\nn = 100\n\nM = buildM(b,n)\ne = buildE(n)\n\nx = np.linspace(0,1,n+2)\nx2 = np.linspace(0,1,n)\n\nanalyticSol = x - (np.exp(-(1-x)/b)-np.exp(-1/b))/(1-np.exp(-1/b))\nu = np.linalg.solve(M,e)\n\nplt.plot(x,analyticSol,label = 'Analytical solution')\nplt.plot(x2,u,label = 'Discretised solution')\nplt.legend()\n\n\n<matplotlib.legend.Legend at 0x7f8a957f2c90>\nDiscretised solution vs analytical solution\n\n\n\n\n\nHow would changing the parameters affect the residuals ratio after 10 iterations?\n\n\nCode\ndeltaTGrid = np.linspace(0.00001,0.0001,100)\n\nratio = np.zeros(100)\ni = 0\nfor deltaT in deltaTGrid:\n    ratio[i] = mainSolver(deltaT,0.6,0.5)\n    i+=1\n\nplt.plot(deltaTGrid,ratio)\nplt.xlabel('Delta T')\nplt.ylabel('Ratio')\n\n\nText(0, 0.5, 'Ratio')\nImpact of the choice of time step with the residual ratios.\n\n\n\n\n\nHow would changing the RK parameter change the residual ratio after 10 iterations? Here we take the optimal delta T we found earlier.\n\n\nCode\n#fig-cap: Changing alpha does not do much...\nalphaGrid = np.linspace(0.01,0.99,100)\n\nratio = np.zeros(100)\ni = 0\nfor alpha in alphaGrid:\n    ratio[i] = mainSolver(0.00007,alpha,0.5)\n    i+=1\n\nplt.plot(alphaGrid,ratio)\nplt.xlabel('alpha')\nplt.ylabel('Ratio')\n\n\nText(0, 0.5, 'Ratio')\n\n\n\n\n\nPendulum test\n\n\nCode\ndef f(t,y):\n    g = 9.81\n    l = 1\n    f1 = y[1]\n    f2 = -g/l* np.sin(y[0])\n    return np.array([f1,f2])\n\n\n\n#Pendulum\ndeltaT = 0.01\nt_min = 0\nn = 1000\nt = t_min\ntArray = np.zeros(n+1)\ntArray[0] = t\ny = np.array([np.pi/2,0])\nyArray = np.zeros((n+1,2))\nyArray[0] = y\nfor i in range(n):\n    y = RK2(f,y,t,deltaT,0.9)\n    t+=deltaT\n    tArray[i+1] = t\n    yArray[i+1] = y\n\n\nplt.plot(tArray,yArray[:,0])"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bellman, Richard Ernest. 1953. Stability Theory of Differential\nEquations /. New York : McGraw-Hill,."
  }
]