[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning for the Optimization of Explicit Runge Kutta Method Parameters",
    "section": "",
    "text": "Introduction\nMachine learning is everywhere. It has applications in computer vision [1], robotic [2], finance [3], recommender systems [4], playing games at a high level [5] or even discovering new matrix multiplication algorithms [6]. The use of machine learning in scientific problems which has been aptly called scientific machine learning is also growing, with the most important example being combining neural network and physic laws to either discover or solve partial differential equations [7].\nIn this thesis, we focus on studying reinforcement learning, which is one of the three main machine learning paradigm. The three main paradigm are as follow[8, Ch. 1.1]:\n\nSupervised learning, where we learn using data containing an input, and a desired output. Regression models are an example of a supervised learning.\nUnsupervised learning, where the data only has an input but no desired output. Examples include clustering algorithms.\nReinforcement learning, in which we have an intelligent agent who learns to do something by interacting with its environment, receiving feedback in the form of rewards which the agent wants to maximize.\n\nWhat sets apart reinforcement learning from its cousins unsupervised and supervised learning is the introduction of the concept of reward. The agent learns by trial and error, and wants to maximize the rewards it gets over time. This is, in essence, quite similar to how we animals learn to do things, and it is no surprise that reinforcement learning traces its roots from the field of animal learning [9]. Another important root of reinforcement learning comes from the field of optimal control, where the agent and environment of reinforcement learning are respectively the controller and controlled system in control theory [8, Ch. 1.7].\nTo study reinforcement learning, we need a playground. That playground could be an already established playground (such as the Gymnasium API), but in this thesis we use our own playground, which we find in the realm of numerical differential equation solvers.\nNumerical methods for differential equations are amongst the most important methods in numerical analysis. All of these methods have specific strengths and weaknesses. They all have, however, some parameters that need to be chosen, if only for the step size. These parameters have to be chosen to maximize performance, and depend on the problem. In some cases they are taken using some heuristics, but they can also be searched for computationally, which is an issue, as any computation means more time to get to the solution. It would therefore be a great time saver if a computer could learn these heuristics, for example using reinforcement learning! This is the playground we use in this thesis, albeit with a reduced scope.\nWe start by motivating the use of numerical ODE solvers to solve linear systems. As a case study, we have a specific type of linear systems, which appears when discretizing the steady state, one dimensional convection diffusion equation \\(u_{x} = bu_{xx} +1\\). Doing so, we end up with two problem parameters; \\(b\\), which is a physical constant, and \\(n\\), stemming from the discretization. The studied numerical solver is an explicit Runge-Kutta method, and has two parameters, a (pseudo-) time step \\(\\Delta t\\) and another parameter \\(\\alpha\\), which need to be chosen. How to choose these solver parameters, as a function of the problem parameters is then left to the realm of reinforcement learning.\nWe then introduce through intuitive examples (and a very cute bunny) the main concepts of reinforcement learning, such as states, actions, state transitions and rewards which are then formalized as a Markov decision process. We then introduce policy gradient methods, and in particular we introduce the classical REINFORCE [10] algorithm, which we use to optimize the solver parameters for the studied linear systems.\nThe results, while positive, are hampered somewhat by the fact that the method used in this thesis is not a natural fit to what makes reinforcement learning so powerful. A discussion on how to redefine the problem to make better use of the strengths of reinforcement learning will follow.\n\n\n\n\n[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models.” 2022. Available: https://arxiv.org/abs/2112.10752\n\n\n[2] J. Kober, J. Bagnell, and J. Peters, “Reinforcement learning in robotics: A survey,” The International Journal of Robotics Research, vol. 32, pp. 1238–1274, Sep. 2013, doi: 10.1177/0278364913495721.\n\n\n[3] B. Hambly, R. Xu, and H. Yang, “Recent advances in reinforcement learning in finance,” Mathematical Finance, vol. n/a, no. n/a, doi: https://doi.org/10.1111/mafi.12382.\n\n\n[4] X. Chen, L. Yao, J. McAuley, G. Zhou, and X. Wang, “A survey of deep reinforcement learning in recommender systems: A systematic review and future directions.” 2021. Available: https://arxiv.org/abs/2109.03540\n\n\n[5] D. Silver et al., “Mastering the game of go with deep neural networks and tree search,” Nature, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: 10.1038/nature16961.\n\n\n[6] A. Fawzi et al., “Discovering faster matrix multiplication algorithms with reinforcement learning,” Nature, vol. 610, no. 7930, pp. 47–53, Oct. 2022, doi: 10.1038/s41586-022-05172-4.\n\n\n[7] S. Cuomo, V. S. di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli, “Scientific machine learning through physics-informed neural networks: Where we are and what’s next.” 2022. Available: https://arxiv.org/abs/2201.05624\n\n\n[8] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, Second. The MIT Press, 2018. Available: http://incompleteideas.net/book/the-book-2nd.html\n\n\n[9] C. Mahoney, “Reinforcement learning: A review of the historic, modern, and future applications of this special form of machine learning.” https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6, 2021.\n\n\n[10] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine Learning, vol. 8, no. 3, pp. 229–256, May 1992, doi: 10.1007/BF00992696.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "8_implementation.html",
    "href": "8_implementation.html",
    "title": "6  Implementation",
    "section": "",
    "text": "6.1 A linear approximation of the policy\nWe have now defined policy gradient methods as a way to address two issues that happened when we translated the problem as defined in the beginning of the last chapter. These issues being the need for a model free method, and dealing with large, and even infinite state and action set.\nWe need to define a policy of the form \\(\\pi(a|s,\\mathbfit{\\theta})\\), where \\(a=(\\Delta t, \\alpha)\\) is the action of choosing the solver parameters, given a pair of problem parameters, that is some state, \\(s=(b,n)\\). We choose to have a policy of the form \\((\\Delta t, \\alpha) \\approx A(b,n)^\\intercal + c\\), where \\(A\\) is a two by two matrix and \\(c\\) a 2-vector. Furthermore, this policy need to be stochastic for the REINFORCE algorithm to work.\nWe remark that the action space is, in our case, continuous, so the policy has to be over a continuous action space. In the discrete case, the policy is \\(\\pi(a|s,\\mathbfit{\\theta})\\) is a probability mass function, so that \\(\\sum_a\\pi(a|s,\\mathbfit{\\theta}) = 1\\). We extend it to continuous action space by considering \\(\\pi\\) as a probability density function instead, and replacing the sum by an integral, that is \\(\\int_{a\\in \\mathcal{A}}\\pi(a|s,\\mathbfit{\\theta}) = 1\\)[1].\nLet \\(s\\) be a given state, \\(s=(b,n)\\). We first define the values \\(\\mu_\\alpha\\) and \\(\\mu_{\\Delta t}\\),\n\\[\n\\begin{pmatrix}\n\\mu_\\alpha \\\\\n\\mu_{\\Delta t}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta_0 & \\theta_1\\\\\n\\theta_2 & \\theta_3\n\\end{pmatrix}\n\\begin{pmatrix}\nb\\\\\nn\n\\end{pmatrix} +\n\\begin{pmatrix}\n\\theta_4\\\\\n\\theta_5\n\\end{pmatrix}.\n\\tag{6.1}\\]\nwhere \\(\\mathbfit{\\theta} = (\\theta_0,\\theta_1,\\dots, \\theta_5)^\\intercal \\in \\mathbb{R}^6\\). \\(\\mu_\\alpha\\) and \\(\\mu_{\\Delta_t}\\) can be regarded as “the deterministic policy”. Around this deterministic policy, we add some noise, specifically Gaussian noise to get the stochastic policy\n\\[\n\\alpha \\sim  \\mathcal{N}(\\mu_\\alpha,\\sigma^2),\n\\]\nand independently, \\[\n\\Delta t \\sim  \\mathcal{N}(\\mu_{\\Delta t}, \\sigma^2).\n\\]\nHere \\(\\mathcal{N}(\\mu,\\sigma^2)\\) is the normal distribution, with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and we choose \\(\\sigma\\) fixed in this thesis. We thus have a policy of the Since \\(\\alpha\\) and \\(\\Delta t\\) are chosen independently, the joint probability density of both parameters is the product of both marginal probability density function, that is\n\\[\n\\pi(a = (\\Delta t, \\alpha)|s,\\mathbfit{\\theta})  = f_{1}(\\alpha)\\cdot f_{2}(\\Delta t),\n\\]\nwhere \\[\nf_1(\\alpha) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(\\alpha - \\theta_0b-\\theta_1n-\\theta_4)^2}{2\\sigma^2}\\right),\n\\]\nand similarly,\n\\[\nf_2(\\Delta t) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(\\Delta t - \\theta_2b-\\theta_3n-\\theta_5)^2}{2\\sigma^2}\\right).\n\\]\nTaking the logarithm, we get \\(\\ln(f(\\alpha,\\Delta t)) = \\ln(f_1(\\alpha)) + \\ln(f_2(\\Delta t))\\). Thus,\n\\[\n\\ln(f_1(\\alpha)) = \\ln(\\frac{1}{\\sqrt{2\\pi}\\sigma}) - \\frac{(\\alpha - \\theta_0b-\\theta_1n-\\theta_4)^2}{2\\sigma^2}.\n\\]\nWe now take the gradient w.r.t \\(\\mathbfit{\\theta}\\) to get\n\\[\n\\nabla_{\\mathbfit{\\theta}} \\ln(f_1(\\alpha)) = \\xi_\\alpha (b\\theta_0,n\\theta_1,0,0,\\theta_4,0)^\\intercal,\n\\tag{6.2}\\]\nwhere \\(\\xi_\\alpha = \\frac{(\\alpha - \\theta_0b-\\theta_1n-\\theta_4)}{\\sigma^2}\\).\nDoing a similar thing with \\(\\Delta t\\), we get the gradient,\n\\[\n\\nabla_{\\mathbfit{\\theta}} \\ln(f_2(\\Delta t)) = \\xi_{\\Delta t}(0,0,b\\theta_2,n\\theta_3,0,\\theta_5)^\\intercal,\n\\tag{6.3}\\]\nwhere \\(\\xi_{\\Delta t} = \\frac{(\\Delta t - \\theta_2b-\\theta_3n-\\theta_5)}{\\sigma^2}\\). We now add both gradients together to get the gradient of the policy, for a specific action \\(a = (\\alpha, \\Delta t)\\) and state \\(s=(b,n)\\):\n\\[\n\\nabla_{\\mathbfit{\\theta}} \\ln\\pi(a|s,{\\mathbfit{\\theta}}) =  \\xi_\\alpha (b\\theta_0,n\\theta_1,0,0,\\theta_4,0)^T+ \\xi_{\\Delta t}(0,0,b\\theta_2,n\\theta_3,0,\\theta_5)^\\intercal.\n\\tag{6.4}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "8_implementation.html#implementation-of-the-reinforce-algorithm",
    "href": "8_implementation.html#implementation-of-the-reinforce-algorithm",
    "title": "6  Implementation",
    "section": "6.2 Implementation of the REINFORCE algorithm",
    "text": "6.2 Implementation of the REINFORCE algorithm\nNow that everything has been defined, the REINFORCE algorithm can be applied to find an optimal policy.\n\n6.2.1 Algorithm code\nWe present in this section the full training in a pseudo code format. The full code is written in a more modular way, and is available on the appendix, as well as on GitHub.\n\n##Doesn't run, as it needs other functions\n##but is a good bridge between pseudocode and the\n##full code\nlearning_rate = 1e-8\ngamma = 0 #Discount factor\ninitial_theta = [0,0,0,0,0.3,2] #Good enough theta\nsigma = 0.1 #Standard dev for the policy\nnumber_of_episodes = 1000 \nepisode_length = 20\n\n\nfor i in range(number_of_episodes):\n    #Generate an episode\n    #Choose an initial starting state, at random(uniformly)\n    b , n = state_transition()\n    #Create an episode object, which will have \n    #The history of the trajectory\n    episode = Episode(length = episode_length)\n    for j in range(episode_length):\n        #Get the action, according to the policy we defined before\n        delta_t, alpha = get_action(b,n,sigma,theta) #pi(a|s,theta)\n        #Then compute the residual ratio, after n_iter of the RK2 solver.\n        res_ratio = compute_res_ratio(b,n,delta_t,alpha,n_iter = 10)\n        reward = 1 - res_ratio #The lower the res ratio, the better the reward\n        #Save the state action and rewards inside the episode object\n        #so that we can access it later\n        episode.save(b,n,alpha,delta_t,reward, position = j)\n        #Then get to a new state, at random\n        b , n = state_transition()\n    #Now that we have an episode, we can apply REINFORCE\n    #and update our policy accordingly\n    for k in range(episode_length):\n        #Get access to s_k, a_k, r_{k+1}\n        b , n , delta_t, alpha , reward = episode.get(k)\n        #Get the log likelihood of the action a_k, as in Eq 6.4\n        log_lik_gradient = get_log_lik_gradient(b,n,alpha,delta_t,sigma)\n        #Estimage the return Eq 5.9\n        estimated_return = 0\n        for l in range(k, episode_length):\n            #episode.reward_hist[l] is R_{l+1}\n            estimated_return += episode.reward_hist[l] * (gamma**(l-t))\n        ##Update the policy\n        theta = theta + learning_rate * log_lik_gradient * estimated_return\n#We end up with an updated theta, that is a better policy.\n\n\n\n6.2.2 A first experiment\nWe implement the REINFORCE algorithm to the test problem. There are a few hyperparameters to set.\n\nThe learning rate is set to \\(\\alpha=1\\times10^{-8}\\).\nThe discount rate is set to \\(\\gamma = 0\\), as the state transitions have no relationship with the actions taken, there is no reason to prefer long term rewards.\nBecause the discount rate is so low, there is no bias added by estimating the returns at the end of the episodes. The episodes length is set to \\(20\\) as we want to use the updated policy as often as possible.\nThe standard deviation of the policy parameters is set to \\(\\sigma = 0.1\\).\n\nThis leaves the choice of the initial value for \\(\\mathbfit{\\theta}\\). While it is possible for the parameters to be random, or all set to 0, we use the experiment done in chapter 4 to use. In Figure 3.2 (a), it seems that a policy of \\(\\alpha = 0.3\\) and \\(\\Delta t = 2\\) is a reasonable choice. Since this was done only for a single set of problem parameters, we have no idea of the relationship between problem parameters and optimal solver parameters. Therefore, we only set the parameter \\(\\theta_4 = 0.3\\), and \\(\\theta_5=2\\), the other parameters are set to 0.\nThe algorithm is run for 50000 episodes, and we observe the evolution of the parameters theta(Figure 6.1).\nSince the discount rate is set to \\(0\\), in any state, the return is the instant reward received by the agent over a single episode. So, for an episode of length \\(l\\), we have the rewards \\(r_1, r_2, \\dots, r_l\\). Then, we can plot the average reward \\(r_{av} = \\frac{r_1 + r_2 +\\dots + r_l}{l}\\) over each episode. Because the variance of \\(r_{av}\\) is still high, we use the rolling average of \\(r_{av}\\) over the last \\(k = 50\\) episodes as a smoother.\nThe average reward is the no scaling reward in Figure 6.2 and is trending upward with successive episodes, which is the intended behavior of the algorithm. However, there are certain problems that have been made apparent by the two plots:\n\nDespite running the algorithm for a long time, some of the elements of \\(\\mathbfit{\\theta}\\) have barely changed, and it is clear that we are far from any convergence of the reward function.\nEven with smoothing, it is apparent that the method has a high variance.\nIt seems that \\(\\theta_1\\) and \\(\\theta_3\\) vary quite a bit over time whereas the other parameters have a steady rate of change.\n\n\n\n\n\n\n\n\n\nFigure 6.1: Evolution of the \\(\\theta\\) parameters in a first experiment.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Evolution of the rolling average (k=50) of the average episode reward, with or without scaling.\n\n\n\n\n\nThe slow apparent convergence rate can not be mitigated by a higher learning rate, as this empirically leads to divergence issues.\nThe high variance is typical of reinforcement learning tasks, and in particular Monte Carlo based methods, which REINFORCE is a part of. That being said, there exists much better methods that can reduce this variance, at the expense of introducing some bias, such as for example actor-critics methods [2, Ch. 13.5], or proximal policy optimization (PPO) [3]. Both of these methods are not explored in this thesis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "8_implementation.html#scaling-the-parameters",
    "href": "8_implementation.html#scaling-the-parameters",
    "title": "6  Implementation",
    "section": "6.3 Scaling the parameters",
    "text": "6.3 Scaling the parameters\nTo address the slow convergence problem, we start with a motivating example.\n\n6.3.1 A motivating example of gradient descent\nConsider the loss function \\(f(x,y) = x^2 + 9y^2\\). The function admits a global minimum at \\(x = y = 0\\), and its gradient given by\n\\[\n\\nabla f(x,y) = (2x,18y)^\\intercal.\n\\]\nTherefore, the gradient descent iteration, with learning rate \\(\\alpha&gt;0\\), is the iteration\n\\[\n\\begin{pmatrix}\nx_{t+1}\\\\\ny_{t+1}\n\\end{pmatrix} = \\begin{pmatrix}\nx_t\\\\\ny_t\n\\end{pmatrix} - \\alpha \\begin{pmatrix}\n2x_t\\\\\n18y_t\n\\end{pmatrix}.\n\\]\nThat is \\(x_{t+1} = (1-2\\alpha)x_t\\) and \\(y_{t+1} = (1-18\\alpha)y_t\\). The iterates converge to \\(x=y=0\\) if and only if \\(\\alpha&lt;1/9\\). If however, \\(\\frac{1}{9}&lt;\\alpha&lt;1\\), we will have convergence for \\(x\\), but not for \\(y\\).\nThe reason for this is that the gradient is steeper in the \\(y\\) direction than the \\(x\\) direction, which leads to comparatively bigger change in \\(y\\) than \\(x\\) in the gradient descent iterations.\nTo remedy this, we can use a change of variable \\(z = 3y\\). Then \\(f(x,z) = x^2 + z^2\\). The gradient descent iteration is then given by\n\\[\n\\begin{pmatrix}\nx_{t+1}\\\\\ny_{t+1}\n\\end{pmatrix} = \\begin{pmatrix}\nx_t\\\\\ny_t\n\\end{pmatrix} - \\alpha\n\\begin{pmatrix}\n2x_t\\\\\n2y_t\n\\end{pmatrix}.\n\\]\nThat is, \\(x_{t+1} = (1-2\\alpha_x)x_t\\) and \\(z_{t+1} = (1-2\\alpha_y)y_t\\). This converges to \\(0\\) if and only if \\(0&lt;\\alpha&lt;\\frac{1}{2}\\), which means we can afford a much bigger learning rate. With \\(\\alpha = \\frac{1}{2}\\), the gradient descent algorithm can now converge to \\(0\\) in a single iteration!\n\n\n6.3.2 Changing the variable\nThis section is born from an intuitive idea and is for this reason less formal than the rest. Looking at the equation for the gradient of the log policy (Equation 6.4), we notice that the gradient has a similar expression in each direction. More particularly, the gradient in the direction \\(i\\) is given by the partial derivative\n\\[\n\\frac{\\partial \\ln \\pi(a|s,\\mathbfit{\\theta})}{\\partial \\theta_i} = \\xi_{\\alpha,\\Delta t} (\\_) \\theta_i\n\\]\nwhere \\(\\xi_{\\alpha,\\Delta t}\\) is either \\(\\xi_\\alpha\\) (Equation 6.2) or \\(\\xi_{\\Delta t}\\) (Equation 6.3), and \\((\\_)\\) is either:\n\n\\(b\\) in the directions \\(\\theta_0\\) and \\(\\theta_2\\).\n\\(n\\) in the directions \\(\\theta_1\\) and \\(\\theta_3\\).\n\\(1\\) in the directions \\(\\theta_4\\) and \\(\\theta_5\\).\n\nUsing the motivating example above, we’ve seen that it can be a good idea to rescale some variables so that the gradient is as “steep” in all directions. However, in this case, \\(n\\) can vary between \\(5\\) and \\(200\\), while \\(b\\) only varies between \\(0\\) and \\(1\\). This motivate the idea that, in order to make a gradient “as steep” in all directions.\nInstead of using \\(n\\) directly, we now use the scaled variable\n\\[\nn' = \\frac{n-5}{200}.\n\\]\nSince \\(n\\) can vary between \\(5\\) and \\(200\\), \\(n'\\) can have values between \\(0\\) and \\(1\\), just like the values of \\(b\\). Everything then follows by simply replacing \\(n\\) by \\(n'\\) in Section 6.1. The new deterministic policy is\n\\[\n\\begin{pmatrix}\n\\mu_\\alpha \\\\\n\\mu_{\\Delta t}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta_0 & \\theta_1\\\\\n\\theta_2 & \\theta_3\n\\end{pmatrix}\n\\begin{pmatrix}\nb\\\\\nn'\n\\end{pmatrix} +\n\\begin{pmatrix}\n\\theta_4\\\\\n\\theta_5\n\\end{pmatrix},\n\\tag{6.5}\\]\nand the equation of the gradient is unchanged, with the exception of replacing \\(n\\) by \\(n'\\) everywhere.\nWith this change implemented, we rerun the first experiment. All the parameters are the same, except that the learning rate can is now set to \\(\\alpha = 2\\times 10^{-4}\\) without divergence. Compared to the first experiment, the average episode reward is much better, as seen in Figure 6.2.\n\n\n\n\n\n\n\n\nFigure 6.3: Evolution of the theta parameters with different initial parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Evolution of the rolling average(\\(k=500\\)) of the average episode reward for different initial parameters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "8_implementation.html#impact-of-initial-conditions",
    "href": "8_implementation.html#impact-of-initial-conditions",
    "title": "6  Implementation",
    "section": "6.4 Impact of initial conditions",
    "text": "6.4 Impact of initial conditions\nGradient based iterations use the local information about an objective (or loss) function \\(J(\\theta)\\) to compute the update \\(\\mathbfit{\\theta} \\rightarrow \\mathbfit{\\theta} \\pm \\alpha \\nabla J(\\mathbfit{\\theta})\\). This local behavior also means that any convergence of gradient descent is to a local minimum, and we can’t be certain that this mimimum is a global minimum.\nLet us test whether the algorithm converges to the same values regardless of initial conditions. The third experiment is then to run the algorithm with the same parameters, but with varied initial conditions, and to visualize the results, both in the average rewards and the evolution of \\(\\mathbfit{\\theta}\\) over 200000 episodes.\nThe evolution of \\(\\mathbfit{\\theta}\\) is in Figure 6.3, and the rolling average of the average episode reward is plotted in Figure 6.4 for different initial value for \\(\\mathbfit{\\theta}\\). It turns out that while convergence in reward is to the same values, the parameter \\(\\theta_3\\) does not seem to converge to the same value. Furthermore, even with such a large amount of episodes, it is not clear if the other parameters converged.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "8_implementation.html#further-results",
    "href": "8_implementation.html#further-results",
    "title": "6  Implementation",
    "section": "6.5 Further results",
    "text": "6.5 Further results\nThe average reward of the episode is a nice way to report on the performance of the method. However, it is difficult to interpret how the model performs once we have found some optimal parameters \\(\\mathbfit{\\theta}^*\\). In particular, by using the REINFORCE algorithm, the policy function has to be stochastic during training. The actual policy we choose can, however, be deterministic. So, at the risk of adding some bias, we remove the noise \\(\\sigma\\) in the policy and choose to use the “deterministic policy” \\(\\alpha = \\mu_\\alpha\\), \\(\\Delta t = \\mu_{\\Delta t}\\), as in Equation 6.1, and we denote this policy by \\(\\pi_d(a|s,\\mathbfit{\\theta}^*)\\). For the value of \\(\\mathbfit{\\theta}^*\\), we use its last value in the second experiment, which is (with some rounding off)\n\\[\n\\mathbfit{\\theta}^* = (-3.606 \\times 10^{-3},4.476\\times 10^{-3},-3.598\\times 10^{-4},-0.3542 ,0.2435,1.1305)^\\intercal.\n\\]\nThen, we compute the value of \\(\\rho_{10,b,n}\\) using this policy and for different values of \\(n\\) and \\(b\\), the results are as below in Figure 6.5. While we have convergence at any point, the convergence is slow, and the maximum value for \\(\\rho_{10}\\) is \\(0.99917\\). Referring back to the grid search experiment(see Figure 3.2), this slow convergence is also partly an issue with the solver itself.\nSince we trained the policy on \\(\\rho_{10}\\), it may be a good idea to check if the solver still converges when we compute more iterations. The result are in Figure 6.5. There are some points where the solver diverges, which is a problem in particular because the point where it diverges are for small values of \\(b\\), which is often the case physically.\nThis divergence indicates that it may be a good idea to further train the learned policy by computing \\(\\rho_{100}\\), and having a reward of \\(1-\\rho_{100}\\) instead of \\(1-\\rho_{10}\\). This of course means that the training time will have to be longer. In that case, we can set \\(\\mathbfit{\\theta}^*\\) as a starting parameter for the policy.\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(\\rho_{10}\\). Maximum residual ratio: 0.99922.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\rho_{100}\\). Note the divergence in black, for low values of \\(b\\).\n\n\n\n\n\n\n\nFigure 6.5: Evolution of the residual ratio \\(\\rho_{10}\\) and \\(\\rho_{100}\\), with the learned policy, depending on the problem parameters \\(n\\) and \\(b\\).\n\n\n\n\n\n\n\n[1] T. P. Lillicrap et al., “Continuous control with deep reinforcement learning.” 2019. Available: https://arxiv.org/abs/1509.02971\n\n\n[2] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, Second. The MIT Press, 2018. Available: http://incompleteideas.net/book/the-book-2nd.html\n\n\n[3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” CoRR, vol. abs/1707.06347, 2017, Available: http://arxiv.org/abs/1707.06347",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "5_solverExploration.html",
    "href": "5_solverExploration.html",
    "title": "3  Explicit Runge-Kutta Method",
    "section": "",
    "text": "3.1 A small introduction to explicit Runge-Kutta methods\nThis section aims to introduce explicit Runge-Kutta methods, [1, Ch. 3], which we use in this paper. We consider solving a generic initial value problem of the form\n\\[\ny'(t) = f(t,y(t)), \\quad y(0) = y_0.\n\\]\nIf we know, for an instant \\(t_n\\), the value for \\(y(t_n)\\), we can compute the value of \\(y\\) at instant \\(t_{n+1} = t_n +\\Delta t\\) by integrating\n\\[\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}}f(u,y(u))\\; du,\n\\]\nand with the change of variable \\(u = t_n + \\Delta t\\tau\\), we have\n\\[\ny(t_{n+1}) = y(t_n) + \\Delta t\\int_0^1f(t_n+\\Delta t\\tau,y(t_n+\\Delta t\\tau)) \\; d\\tau.\n\\] The problem is finding a suitable way to compute the integral above. An elementary approach is to use the current value of \\(f(t_n,y(t_n))\\) and to treat \\(f\\) as constant, thus defining the sequence\n\\[\ny_{n+1} = y_n + \\Delta tf(t_n,y_n),\n\\]\nwhere \\(y_{n} \\approx y(t_{n})\\), \\(y_0 = y(0)\\). This is the explicit Euler’s method. We now want to exploit quadrature formulas for numerical integration. Let \\(c_j \\in [0,1], j=1,2,\\dots, \\nu\\), where \\(\\nu\\) is an integer, be the nodes in the quadrature formula, with their associated weight \\(b_j, j=1,2,\\dots, \\nu\\). A quadrature formula for the integral is then of the form\n\\[\n\\int_0^1 f(t_n+\\Delta t \\tau,y(t_n+\\Delta t\\tau))\\; d\\tau \\approx \\sum_{j=1}^\\nu b_j f(t_n + \\Delta t c_j,y(t_n+\\Delta t c_j)).\n\\]\nThis is all well and good, except that we have to know the values \\(y(t_n+\\Delta c_j)\\), which we do not possess. We can however, play pretend and compute an approximation of these values \\(\\xi_j \\approx y(t_n+\\Delta t c_j), j=1,\\dots, \\nu\\). The \\(\\xi_j\\) are called stage values. [2]. The main idea to use the \\(\\xi_i\\)’s to compute \\(\\xi_j\\), using a linear combination of the terms \\(f(t_n + \\Delta t c_j, \\xi_i)\\). That is\n\\[\n\\xi_i = y_n + \\Delta t \\sum_{j=1}^\\nu a_{ij}f(t_n+\\Delta t c_j, \\xi_j),\n\\]\nfor \\(i = 1,\\dots, \\nu\\), where the \\(a_{ij}\\) are some well chosen values, which is not in scope of this thesis. To simplify notation, we note \\(A\\) as the square array containing the \\(a_{ij}\\) parameters, that is \\(A_{ij} = a_{ij}\\), \\(c = (c_1,\\dots,c_\\nu)^\\intercal\\) the vector of nodes, and \\(b = (b_1,\\dots, b_\\nu)^\\intercal\\) the vector of weights. An RK method is then written in the form of the following array, also called a Butcher tableau:\n\\[\n\\renewcommand\\arraystretch{1.2}\n\\begin{array}\n{c|c}\nc & A\\\\\n\\hline & b^\\intercal\n\\end{array}.\n\\]\nWe remark that if, for any \\(j\\geq i\\), \\(a_{ij} \\neq 0\\), then we will need to know \\(\\xi_j\\) to compute \\(\\xi_i\\), which involves solving an equation, making the method implicit. We consider here explicit methods, where we can compute \\(\\xi_{i+1}\\) if we know \\(\\xi_j, j = 1 , \\dots , i-1\\). Since we know \\(f(t_n,y_n)\\), we choose \\(a_{11} = 0\\) and \\(c_1 = 0\\). An explicit RK method is then of the form\n\\[\ny_{n+1} = y_n + h\\sum_{j=1}^\\nu b_j f(t_n+\\Delta t c_j,\\xi_j),\n\\]\nwhere the stage values \\(\\xi_j\\) are computed sequentially as follow\n\\[\\begin{align*}\n\\xi_1 &= y_n,\\\\\n\\xi_2 &= y_n + \\Delta t a_{2,1}f(t_n,\\xi_1),\\\\\n\\xi_3 &= y_n + \\Delta t a_{3,1}f(t_n,\\xi_1) +  \\Delta t a_{3,2}f(t_n+\\Delta t c_2,\\xi_2),\\\\\n\\vdots\\\\\n\\xi_\\nu &= y_n + \\Delta t \\sum_{j=1}^{\\nu-1} a_{\\nu,j}f(t_n + \\Delta t c_j,\\xi_j).\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explicit Runge-Kutta Method</span>"
    ]
  },
  {
    "objectID": "5_solverExploration.html#sec-RK_solver_test_problem",
    "href": "5_solverExploration.html#sec-RK_solver_test_problem",
    "title": "3  Explicit Runge-Kutta Method",
    "section": "3.2 Application to the test problem",
    "text": "3.2 Application to the test problem\nWe now have to solve the ODE \\(u'(t) = e - Mu(t)\\) where \\(M\\) depends on the problem parameters \\(b\\) and \\(\\Delta x = 1 / (n+1)\\), and \\(n\\) is the chosen number of subdivisions of \\([0,1]\\). We consider in this thesis the following RK method with two stages [2];\n\\[\n\\begin{array}\n{c|cc}\n0 & &\\\\\n\\alpha & \\alpha & \\\\\n\\hline & 0 & 1\n\\end{array}.\n\\]\n\nRemark. This RK method can be extended to more stages. We only need the last stage value to compute the time step update, and we only need to compute the stage values sequentially using only the last stage value calculated. This makes it possible, when programming the method, to simply to do the update of the variable \\(\\xi\\) in place inside the computer memory. Such methods are thus memory efficient.\n\nThis solver has two parameters, namely the (pseudo) time step \\(\\Delta t\\) and \\(\\alpha\\), where \\(\\alpha \\in [0,1]\\).\nThe goal is for the solver to converge to a steady state solution in as few iterations as possible.\n\n3.2.1 A note on stability\nUsing the same notation as before for the stage values and the studied RK method, for the equation \\(u'(t) = f(t,u(t))\\), we have \\(\\xi_1 = u_n\\),\n\\[\n\\xi_2 = u_n + \\Delta t \\alpha f(t_n,\\xi_1) = u_n + \\Delta t \\alpha f(t_n,u_n).\n\\]\nThe update is thus;\n\\[\nu_{n+1} = u_n + \\Delta t f(t_n+\\alpha \\Delta t, \\xi_2).\n\\]\nIn the test problem case, \\(f(t_n,\\mathbfit{u}_n) = \\mathbfit{e} - \\mathbfit{Mu}_n\\), and we get the update\n\\[\n\\mathbfit{u}_{n+1} = \\mathbfit{u}_n + \\Delta t\\left[\\mathbfit{e}-\\mathbfit{M}(\\mathbfit{u}_n+\\alpha\\Delta t(\\mathbfit{e}-\\mathbfit{Mu_n}))\\right].\n\\]\nAfter a few lines of computation, we get the following iteration,\n\\[\n\\mathbfit{u}_{n+1} = \\left[\\mathbfit{I}- \\Delta t(\\mathbfit{I}- \\alpha \\Delta t \\mathbfit{M})\\mathbfit{M}\\right]\\mathbfit{u}_n + \\left[\\Delta t(\\mathbfit{I} - \\alpha \\Delta t \\mathbfit{M})\\right]\\mathbfit{e}.\n\\tag{3.1}\\]\nThis iteration is of the form \\(\\mathbfit{u}_{n+1} =\\mathbfit{Ku}_n  + \\mathbfit{Le}\\), where\n\\[\n\\mathbfit{L} = \\Delta t\\left[ \\mathbfit{I} - \\alpha \\Delta t \\mathbfit{M}\\right]\n\\tag{3.2}\\] and \\[\n\\mathbfit{K} = \\mathbfit{I}- \\mathbfit{LM} = \\left[\\mathbfit{I}- \\Delta t(\\mathbfit{I}- \\alpha \\Delta t \\mathbfit{M})\\mathbfit{M}\\right].\n\\tag{3.3}\\]\nWe recognize this iteration as a linear stationary iteration, which converges to a unique fixed point for any starting value \\(\\mathbfit{u}_0\\) if and only if \\(\\rho(K)&lt;1\\), [3, Ch. 2.2], where \\(\\rho(\\mathbfit{K})\\) is the spectral radius of \\(\\mathbfit{K}\\). Furthermore, this iteration satisfies the consistency requirement \\(\\mathbfit{K} = \\mathbfit{I}-\\mathbfit{LM}\\), so the fixed point, when it exists, is \\(\\mathbfit{u}^* = \\mathbfit{M}^{-1}\\mathbfit{e}\\).\n\nRemark. One may remark that \\(\\mathbfit{K} = p(\\Delta t \\mathbfit{M})\\) with \\(p(z) = 1- z + \\alpha z^2\\) a polynomial. This polynomial is also the stability polynomial of the RK method [2].\n\n\n\n3.2.2 Residual ratios\nWe have shown in the last section the sufficient and necessary condition for the solver to converge to the desired solution \\(\\mathbfit{u}^* = \\mathbfit{M}^{-1}\\mathbfit{e}\\). This condition is that \\(\\rho(\\mathbfit{K})&lt;1\\). The spectral radius can be computed with power iterations [4, Pt. V], but this is an expensive task that we may not be able to do in practice. Furthermore, the derivation of \\(\\mathbfit{K}\\) is specific to this method, and may not be as accessible with other methods. We instead turn our attention to another method.\nWe set \\(\\mathbfit{u}_0 = \\mathbfit{e}\\) as an initial value. We define the relative residual after \\(k\\) steps as\n\\[\nr_k = \\frac{||\\mathbfit{M}\\mathbfit{u}_k - \\mathbfit{e}||}{||\\mathbfit{e}||},\n\\tag{3.4}\\]\nwhere \\(||.||\\) is the 2-norm.\nIf the solver we chose is stable, then \\(||r_k|| \\to 0\\) as \\(k \\to \\infty\\). We define now the residual ratio at step \\(k\\) to be the ratio of the residuals at step \\(k\\) and \\(k-1\\). That is\n\\[\n\\rho_k = \\frac{r_k}{r_{k-1}} = \\frac{||\\mathbfit{Mu}_k - \\mathbfit{e}||}{||\\mathbfit{Mu}_{k-1}-\\mathbfit{e}||}.\n\\tag{3.5}\\]\nNote that the residual ratio depends on both the problem parameters and the solver parameters. It will be useful in future sections to make that relation evident by using the notation \\(\\rho_{k,b,n}(\\alpha, \\Delta t)\\). Figure 3.1 shows the evolution of the relative residual, as well as the residual ratio for specific parameters. After a certain number of iterations, the residual ratio stabilizes. This can be however be after a large amount of iterations, so the rate of convergence can be costly to compute.\n\n\n\n\n\n\n\n\nFigure 3.1: Evolution of the residual norm over iteration, with problem parameters \\(n = 50\\) and \\(b = 0.05\\), and RK parameters \\(\\Delta t = 1\\) and \\(\\alpha = 0.2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explicit Runge-Kutta Method</span>"
    ]
  },
  {
    "objectID": "5_solverExploration.html#sec-small_experiment",
    "href": "5_solverExploration.html#sec-small_experiment",
    "title": "3  Explicit Runge-Kutta Method",
    "section": "3.3 A small experiment",
    "text": "3.3 A small experiment\nWe are interested in finding the best parameters \\((\\Delta t, \\alpha)\\) to use for some specific problem parameters \\((b,n)\\). Ideally, we should minimize the asymptotic residual ratio \\(\\rho_\\infty\\), but this is computationally intensive, so we restrict ourselves to minimizing the residual ratio \\(\\rho_k\\) after a fixed amount of iterations.\nAs we’ve seen in Figure 3.1, \\(\\rho_k\\) can vary quite a bit depending on \\(k\\), so we decide to investigate the residual ratio after 10 iterations and 100 iterations. We set the problem parameters \\(b = 0.05\\), and \\(n = 100\\), and we plot \\(\\rho_{k,0.05,100}(\\Delta t, \\alpha)\\) for different values of \\(k\\). This is achieved by making a linear grid for parameters \\(\\Delta t\\) and \\(\\alpha\\) of size \\(100\\times 100\\), where \\(\\alpha\\) varies between \\(0\\) and \\(1\\), and \\(\\Delta t\\) varies between \\(0\\) and \\(5\\), then computing the residual ratios on that grid.\nWe wish to find the optimal parameters for this specific problem, that is, the ones that minimize \\(\\rho_{k}\\), for different values of \\(k\\). We are also interested in seeing how much the optimal parameters depend on \\(k\\).\nAfter 100 iterations, we see that we need to choose the parameters in more narrow region than after 10 iterations to get \\(\\rho_{100}&lt;1\\), suggesting that convergence of the solver may not hold even if it seems to hold for the first few iterations. However, this doesn’t seem to be the case when we consider higher values of \\(k\\). Nevertheless, we can see how the solver parameters interact with the residual ratio.\nBy doing this experiment, we motivate the following method: using a grid search, look for the solver parameters that minimize \\(\\rho_k\\), where \\(k\\) has to be chosen as low as possible to minimize computing time, but also high enough to ensure that the solver won’t diverge after more iterations. This method however need to be repeated for each individual problem parameters. We therefore explore a possible solution to this problem by using a reinforcement learning algorithm to “learn” the optimal solver parameters \\(\\alpha\\) and \\(\\Delta t\\), as a function of the problem parameters \\(b\\) and \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(\\rho_{10}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\rho_{100}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(\\rho_{200}\\)\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(\\rho_{500}\\)\n\n\n\n\n\n\n\nFigure 3.2: Contour plot of some residual ratios \\(\\rho_k\\), for different \\(k\\) after different number of iterations, for the specific problem parameters \\(n = 100\\) and \\(b=0.05\\). Note that the area in white is where \\(\\rho_k &gt; 1\\).\n\n\n\n\n\n\n\n[1] A. Iserles, A first course in the numerical analysis of differential equations /, 2. ed. in Cambridge texts in applied mathematics. Cambridge ; Cambridge University Press, 2009.\n\n\n[2] P. Birken, “Numerical methods for stiff problems.” Lecture Notes, 2022.\n\n\n[3] Wolfgang. Hackbusch and S. (Online service), Iterative solution of large sparse systems of equations /, 2nd ed. 2016. in Applied mathematical sciences,. Cham : Springer International Publishing :, 2016. Available: http://dx.doi.org/10.1007/978-3-319-28483-5\n\n\n[4] L. N. Trefethen and D. Bau, Numerical linear algebra /. Philadelphia : SIAM, Society for Industrial; Applied Mathematics, cop. 1997.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explicit Runge-Kutta Method</span>"
    ]
  },
  {
    "objectID": "9_summary.html",
    "href": "9_summary.html",
    "title": "7  Summary and Discussion",
    "section": "",
    "text": "In this thesis, we started with the idea of using numerical differential equations solver as an iterative solver for a linear system. More specifically, we turned our attention to a specific RK method, which has two parameters to chose from, which we called the solver parameters. We also chose a specific type of linear equation which arises from the discretization of the steady state, one dimensional convection-diffusion equation. This linear equation depends on two parameters, which we called the problem parameters. The goal was then to see if we could optimize for the solver parameters, as a function of the problem parameters, to maximize the convergence rate of the method. To do that, we used reinforcement learning. In particular, we applied the classical REINFORCE algorithm to our problem. Using the implementation in this thesis, we observed that the implemented solution works, with limited results. In particular, if we use the parameter that we learn, it is possible for the solver to diverge for some problem parameters. There are some avenues to improve these results, in particular:\n\nOn the technical front, the implemented algorithm is very elementary, and suffers from the issue of high sample variance, being a Monte Carlo method. This issue can be addressed by more better algorithms.\nThe policy used was a linear function of the problem parameters. We may want to explore if choosing a policy taking into account interactions between the problem parameters, or applying some transforms to them before fitting a linear policy. It is also possible to fit a neural network to the policy.\nPossible incremental improvements can also be made. This involves for example experimenting with the reward function design, or setting a decaying learning rate to improve convergence of the RL algorithm.\n\nAt last, we need not restrict ourselves to just one type of solver. We could potentially train an intelligent agent to chose which numerical solver to use, depending on the problem.\nThere is on the other hand one glaring issue with the way that reinforcement learning was applied to this problem. A core philosophy of reinforcement learning is that the states, actions and rewards are all interdependent. This interdependence was absent in this thesis, with the state transition being random, no matter the action taken. While it was possible to adapt this philosophy as presented in this thesis, this somewhat hampers the utility of using reinforcement learning over other methods. In particular, one may wonder if the implementation presented here is essentially “gradient descent, with extra steps”.\nIt is therefore preferable to change how the problem is approached. One approach could be train an agent to dynamically change the solver parameters over successive iterations for some specific set of parameters. In that case, the agent would need information about the evolution of the residual, which complicates the modeling problem. Another approach would be to make use of meta learning [1], where instead of directly finding the optimal solver parameters, we learn how to find them efficiently.\n\n\n\n\n[1] M. Andrychowicz et al., “Learning to learn by gradient descent by gradient descent,” CoRR, vol. abs/1606.04474, 2016, Available: http://arxiv.org/abs/1606.04474",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary and Discussion</span>"
    ]
  },
  {
    "objectID": "10_references.html",
    "href": "10_references.html",
    "title": "References",
    "section": "",
    "text": "[1] Bellman Richard Ernest, Stability theory of\ndifferential equations /. New York : McGraw-Hill, 1953.\n\n\n[2] S.\nZhao, “Mathematical foundations of reinforcement learning.”\n2023. https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning\n(accessed Mar. 30, 2023).\n\n\n[3] R.\nS. Sutton and A. G. Barto, Reinforcement learning: An\nintroduction, Second. The MIT Press, 2018. Available: http://incompleteideas.net/book/the-book-2nd.html\n\n\n[4] R.\nJ. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine Learning,\nvol. 8, no. 3, pp. 229–256, May 1992, doi: 10.1007/BF00992696.\n\n\n[5] T.\nP. Lillicrap et al., “Continuous control with deep\nreinforcement learning.” 2019. Available: https://arxiv.org/abs/1509.02971\n\n\n[6] J.\nSchulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” CoRR,\nvol. abs/1707.06347, 2017, Available: http://arxiv.org/abs/1707.06347\n\n\n[7] A.\nIserles, A first course in the numerical analysis of differential\nequations /, 2. ed. in Cambridge texts in applied mathematics.\nCambridge ; Cambridge University Press, 2009.\n\n\n[8] P.\nBirken, “Numerical methods for stiff problems.” Lecture\nNotes, 2022.\n\n\n[9] M.\nAndrychowicz et al., “Learning to learn by gradient\ndescent by gradient descent,” CoRR, vol. abs/1606.04474,\n2016, Available: http://arxiv.org/abs/1606.04474\n\n\n[10] A.\nFawzi et al., “Discovering faster matrix multiplication\nalgorithms with reinforcement learning,” Nature, vol.\n610, no. 7930, pp. 47–53, Oct. 2022, doi: 10.1038/s41586-022-05172-4.\n\n\n[11] C.\nMahoney, “Reinforcement learning: A review of the historic,\nmodern, and future applications of this special form of machine\nlearning.” https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6,\n2021.\n\n\n[12] D.\nSilver et al., “Mastering the game of go with deep neural\nnetworks and tree search,” Nature, vol. 529, no. 7587,\npp. 484–489, Jan. 2016, doi: 10.1038/nature16961.\n\n\n[13] A.\nAtangana, Fractional operators with constant and variable order with\napplication to geo-hydrology. Academic Press, 2018. Available: https://ludwig.lub.lu.se/login?url=https://www.sciencedirect.com/science/book/9780128096703\n\n\n[14] R.\nBellman, R. E. Bellman, and R. Corporation, Dynamic\nprogramming. in Rand corporation research study. Princeton\nUniversity Press, 1957. Available: https://books.google.se/books?id=rZW4ugAACAAJ\n\n\n[15] W.\nA. Adkins, M. G. Davidson, and S. (Online service), Ordinary\ndifferential equations. in Undergraduate texts in mathematics,. New\nYork, NY : Springer New York :, 2012. Available: http://dx.doi.org/10.1007/978-1-4614-3618-8\n\n\n[16] Wolfgang. Hackbusch and S. (Online service),\nIterative solution of large sparse systems of equations /, 2nd\ned. 2016. in Applied mathematical sciences,. Cham : Springer\nInternational Publishing :, 2016. Available: http://dx.doi.org/10.1007/978-3-319-28483-5\n\n\n[17] E.\nLudvig, M. Bellemare, and K. Pearson, “A primer on reinforcement\nlearning in the brain: Psychological, computational, and neural\nperspectives,” in Computational Neuroscience for Advancing\nArtificial Intelligence: Models, Methods and Applications, 2011,\npp. 111–144. doi: 10.4018/978-1-60960-021-1.ch006.\n\n\n[18] J.\nTromp, “Counting legal positions in go — tromp.github.io.”\nhttps://tromp.github.io/go/legal.html.\n\n\n[19] S.\nM. Ross and E. A. Peköz, A second course in probability.\nProbabilityBookstore.com, 2007. Available: https://books.google.se/books?id=g5j6DwAAQBAJ\n\n\n[20] L.\nN. Trefethen and D. Bau, Numerical linear algebra /.\nPhiladelphia : SIAM, Society for Industrial; Applied Mathematics, cop.\n1997.\n\n\n[21] S.\nCuomo, V. S. di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F.\nPiccialli, “Scientific machine learning through physics-informed\nneural networks: Where we are and what’s next.” 2022. Available:\nhttps://arxiv.org/abs/2201.05624\n\n\n[22] R.\nRombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion\nmodels.” 2022. Available: https://arxiv.org/abs/2112.10752\n\n\n[23] J.\nKober, J. Bagnell, and J. Peters, “Reinforcement learning in\nrobotics: A survey,” The International Journal of Robotics\nResearch, vol. 32, pp. 1238–1274, Sep. 2013, doi: 10.1177/0278364913495721.\n\n\n[24] B.\nHambly, R. Xu, and H. Yang, “Recent advances in reinforcement\nlearning in finance,” Mathematical Finance, vol. n/a,\nno. n/a, doi: https://doi.org/10.1111/mafi.12382.\n\n\n[25] X.\nChen, L. Yao, J. McAuley, G. Zhou, and X. Wang, “A survey of deep\nreinforcement learning in recommender systems: A systematic review and\nfuture directions.” 2021. Available: https://arxiv.org/abs/2109.03540",
    "crumbs": [
      "References"
    ]
  }
]