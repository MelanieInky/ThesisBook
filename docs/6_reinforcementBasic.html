<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mélanie Fournier">

<title>Reinforcement Learning for the Optimization of Explicit Runge Kutta Method Parameters - 4&nbsp; Basics of Reinforcement Learning (RL)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./7_policyGradient.html" rel="next">
<link href="./5_solverExploration.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./6_reinforcementBasic.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning (RL)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Reinforcement Learning for the Optimization of Explicit Runge Kutta Method Parameters</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_motivation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Motivation : Pseudo time iterations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_convecDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">A Test Problem, the Convection Diffusion Equation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_solverExploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Explicit Runge-Kutta Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_reinforcementBasic.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning (RL)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_policyGradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Policy Gradient Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Implementation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Summary and Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-non-mathematical-yet-delicious-example" id="toc-a-non-mathematical-yet-delicious-example" class="nav-link active" data-scroll-target="#a-non-mathematical-yet-delicious-example"><span class="header-section-number">4.1</span> A non mathematical, yet delicious example!</a></li>
  <li><a href="#another-example-leonardo-the-rabbit" id="toc-another-example-leonardo-the-rabbit" class="nav-link" data-scroll-target="#another-example-leonardo-the-rabbit"><span class="header-section-number">4.2</span> Another example: Leonardo the rabbit</a>
  <ul class="collapse">
  <li><a href="#states" id="toc-states" class="nav-link" data-scroll-target="#states"><span class="header-section-number">4.2.1</span> States</a></li>
  <li><a href="#actions" id="toc-actions" class="nav-link" data-scroll-target="#actions"><span class="header-section-number">4.2.2</span> Actions</a></li>
  <li><a href="#state-transitions" id="toc-state-transitions" class="nav-link" data-scroll-target="#state-transitions"><span class="header-section-number">4.2.3</span> State transitions</a></li>
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy"><span class="header-section-number">4.2.4</span> Policy</a></li>
  <li><a href="#rewards" id="toc-rewards" class="nav-link" data-scroll-target="#rewards"><span class="header-section-number">4.2.5</span> Rewards</a></li>
  <li><a href="#state-transitions-and-rewards-probabilities" id="toc-state-transitions-and-rewards-probabilities" class="nav-link" data-scroll-target="#state-transitions-and-rewards-probabilities"><span class="header-section-number">4.2.6</span> State transitions and rewards probabilities</a></li>
  </ul></li>
  <li><a href="#finite-markov-decision-process" id="toc-finite-markov-decision-process" class="nav-link" data-scroll-target="#finite-markov-decision-process"><span class="header-section-number">4.3</span> Finite Markov decision process</a></li>
  <li><a href="#state-value-and-bellman-equation" id="toc-state-value-and-bellman-equation" class="nav-link" data-scroll-target="#state-value-and-bellman-equation"><span class="header-section-number">4.4</span> State Value and Bellman Equation</a></li>
  <li><a href="#action-value" id="toc-action-value" class="nav-link" data-scroll-target="#action-value"><span class="header-section-number">4.5</span> Action Value</a></li>
  <li><a href="#optimal-policy-and-value-iteration" id="toc-optimal-policy-and-value-iteration" class="nav-link" data-scroll-target="#optimal-policy-and-value-iteration"><span class="header-section-number">4.6</span> Optimal policy and value iteration</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning (RL)</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mélanie Fournier </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>In this section, we outline the main ideas behind reinforcement learning and how they can be applied in the context of this thesis. The reader familiar with the material may skip this section.</p>
<section id="a-non-mathematical-yet-delicious-example" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="a-non-mathematical-yet-delicious-example"><span class="header-section-number">4.1</span> A non mathematical, yet delicious example!</h2>
<p>In Reinforcement Learning tasks, we are training an <em>agent</em> that interacts with its <em>environment</em> by taking decisions. In this example, we are the agent, and the environment is the kitchen. Suppose we want to cook a delicious meal. At any point in time, we are making decisions such as;</p>
<ul>
<li>Which ingredients we use. Do we use tofu or seitan? Do we add spice or more chili pepper? When do we incorporate the sauce?</li>
<li>Which cookware we use? Cast iron, or non-stick pan?</li>
<li>Whether to put the oven to <span class="math inline">\(200^\circ C\)</span> or <span class="math inline">\(220^\circ C\)</span>.</li>
<li>Or simply do nothing!</li>
</ul>
<p>All of these decisions, which we will call <em>actions</em> from now on, are taken based on the current <em>state</em> of the environment, that is the cooking process. How we decide which <em>action</em> to take given a current <em>state</em> will be called the <em>policy</em> from now on.</p>
<p>After each action, the cooking process gets to a new <em>state</em> and we taste the meal. By tasting it, we get a <em>reward</em> that depend on how well we did. Maybe the food started to burn in which case we get a negative reward, or maybe we made the food tastier, in which case we get a positive reward. In this example, there is a <em>starting state</em>, where we decide to cook something, and a <em>terminal state</em>, in which we finished cooking and get to enjoy the meal.</p>
<p>But how do we learn how to cook, how do we know what <em>action</em> to take at a specific <em>state</em>? That is, how do we learn the <em>policy</em>? We learn it by getting feedback, which is defined by the <em>reward</em> we get after each action. Some of those rewards are immediate, for example, if we add some spices to our food and it tastes better. We want to have a <em>policy</em> that maximizes the total <em>rewards</em> we get over a entire cooking session. This also mean that we have to balance how we prefer the immediate rewards against the future rewards. For example, adding a spice may make the meal taste better in the short term, for which we get a reward, but it may clash later when we add other ingredients, leading to a worse meal and worse <em>rewards</em> down the line.</p>
<p>Each time we cook, we learn what works and what doesn’t, and remember that for any future time we cook. But, if we want to get better at cooking, we must not just repeat the <em>actions</em> that worked! We also have to take some risks, and <em>explore</em> the potential actions we can take at each state! On the other hand, we still need to rely and <em>exploit</em> what we know. There is a balance to find between <em>exploitation</em> and <em>exploration</em> so as to learn as fast as possible.</p>
</section>
<section id="another-example-leonardo-the-rabbit" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="another-example-leonardo-the-rabbit"><span class="header-section-number">4.2</span> Another example: Leonardo the rabbit</h2>
<p>The last example is an intuitive way of thinking of reinforcement learning as similar to the way we animals learn about the world and its processes. The ideas behind reinforcement learning borrow a lot from the fields of psychology and neuroscience<span class="citation" data-cites="aPrimerBrain"><a href="#ref-aPrimerBrain" role="doc-biblioref">[1]</a></span>, and modelling how we learn is a gargantuan task that is, for this very reason, outside of the scope of this thesis!</p>
<p>We turn our attention to a more modest example that is much easier to model, and is an example that one can find in a lot of reinforcement learning books<span class="citation" data-cites="Sutton1998"><a href="#ref-Sutton1998" role="doc-biblioref">[2]</a></span>, <span class="citation" data-cites="reinforcementBookShiyuZhao"><a href="#ref-reinforcementBookShiyuZhao" role="doc-biblioref">[3]</a></span>. We consider the case of Leonardo the rabbit. Leonardo, the agent, is situated in the outside world, which is represented as a <span class="math inline">\(3\times 3\)</span> grid (the environment). He wants to get to the carrot at the bottom right as fast as possible. To help Leonardo get to his meal, we will use reinforcement learning.</p>
<section id="states" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="states"><span class="header-section-number">4.2.1</span> States</h3>
<p>The first thing we do is give a number to each box in the grid, from <span class="math inline">\(1\)</span> to <span class="math inline">\(9\)</span>. We call the set of all boxes number as the state set, which we denote by <span class="math inline">\(\mathcal{S}\)</span>. In this example, <span class="math inline">\(\mathcal{S} = \{1,2,\dots,9\}\)</span> (see <a href="#fig-gridworld1" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-gridworld1</span></a>). A state is defined as any element in the state set, which we denote by <span class="math inline">\(s\in\mathcal{S}\)</span>. The state is the box Leonardo is in.</p>
<div id="fig-gridworld1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gridworld1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<embed src="images/gridworld.pdf" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gridworld1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Can you help special agent Leonardo get to his carrot? The grid environment, where our fluffy friend is situated in. His state is <span class="math inline">\(s=1\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="actions" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="actions"><span class="header-section-number">4.2.2</span> Actions</h3>
<p>Leonardo, in this grid, can move in any 4 directions, that is left, right, up or down. We call this the action set <span class="math inline">\(\mathcal{A}\)</span>, and in this example <span class="math inline">\(\mathcal{A} = \{\text{left,right,up,down}\}\)</span>. An action is defined as any element in the action set, which we denote by <span class="math inline">\(a\in\mathcal{A}\)</span>.</p>
</section>
<section id="state-transitions" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="state-transitions"><span class="header-section-number">4.2.3</span> State transitions</h3>
<p>At this point, we can introduce a time variable <span class="math inline">\(t\)</span>. The initial time is set to <span class="math inline">\(t=0\)</span>, and, after Leonardo takes an action <span class="math inline">\(t\)</span> moves forward by <span class="math inline">\(1\)</span>, and he finds himself in a new state. This is what we call a state transition(see <a href="#fig-gridworld_transition" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-gridworld_transition</span></a>).</p>
<p>We want to keep track of Leonardo positions and actions over time, which is why we denote the state Leonardo is in at time <span class="math inline">\(t\)</span> by <span class="math inline">\(S_t\)</span>, and the action he takes by <span class="math inline">\(A_t\)</span>. In this example, there is the initial state <span class="math inline">\(S_0 = 1\)</span>.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span><span class="math inline">\(S_t\)</span> and <span class="math inline">\(A_t\)</span> are random variables, which is we note in uppercase. Specific observations of <span class="math inline">\(S_t\)</span> and <span class="math inline">\(A_t\)</span> will be in lowercase, that is respectively <span class="math inline">\(s_t\)</span> and <span class="math inline">\(a_t\)</span>.</p>
</div>
<div id="fig-gridworld_transition" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gridworld_transition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<embed src="images/gridworld_transition.pdf" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gridworld_transition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: An example of state transition. Leonardo, being at the state <span class="math inline">\(s_t = 4\)</span>, takes the action <span class="math inline">\(a_t = \text{right}\)</span>. After this action, he is at the state <span class="math inline">\(s_{t+1} = 5\)</span>. Leonardo gets the reward <span class="math inline">\(r_{t+1} = -5\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="policy" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="policy"><span class="header-section-number">4.2.4</span> Policy</h3>
<p>Leonardo, as the agent, only has access to his current state <span class="math inline">\(S_t\)</span>. He has to take an action <span class="math inline">\(A_t\)</span>, but how does he know which action to take? To do that, he uses a policy, which we denote by a function <span class="math inline">\(\pi\)</span>. More formally, <span class="math inline">\(\pi\)</span> is a function that defines the probability of taking the action <span class="math inline">\(A_t = a\)</span> if the state is <span class="math inline">\(S_t = s\)</span>. We denote this by <span class="math inline">\(\pi(a|s)\)</span>.</p>
<p>Suppose for example that <span class="math inline">\(S_t = 3\)</span>. Leonardo has no idea of where the carrot is, but he knows that he can not go up nor to the right, so his policy is to go down, or right at random. Then:</p>
<ul>
<li>The probability to go right is <span class="math inline">\(\pi(\text{right}, 3) = 0.5\)</span>.</li>
<li>The probability to go down is <span class="math inline">\(\pi(\text{down}, 3) = 0.5\)</span>.</li>
</ul>
<p>More specifically, for any state <span class="math inline">\(s\)</span>, we define the conditional probability mass function <span class="math inline">\(\pi(a|s) = \Pr(A_t = a | S_t = s)\)</span>, where <span class="math inline">\(\Pr\)</span> denote a probability. Hence, for any fixed state <span class="math inline">\(s\)</span>, <span class="math inline">\(\sum_{a\in\mathcal{A}} \pi(a|s) = 1\)</span>.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>We will assume that Leonardo only cares about what his current state is to take an action, and not for how long he has been in the grid. This makes the policy independent of the time <span class="math inline">\(t\)</span>.</p>
</div>
</section>
<section id="rewards" class="level3" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="rewards"><span class="header-section-number">4.2.5</span> Rewards</h3>
<p>While Leonardo only takes actions by looking at his current state, he still wants to get to the carrot as fast as possible. He knows his current state <span class="math inline">\(s_t\)</span> and takes the action <span class="math inline">\(a_t\)</span>. Doing so, he ends up in the state <span class="math inline">\(s_{t+1}\)</span> and he gets a reward.</p>
<ul>
<li>The red colored box are difficult to get in, so if he ends up on one of the red colored box, he gets a reward of <span class="math inline">\(-5\)</span>. This is for example the case in <a href="#fig-gridworld_transition" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-gridworld_transition</span></a>.</li>
<li>If he ends up on the carrot, he gets a reward of <span class="math inline">\(+5\)</span>.</li>
<li>If he ends up in any other state, he gets a reward of <span class="math inline">\(-1\)</span>, as he does not want to lose time.</li>
</ul>
<p>More formally, we denote the reward Leonardo gets after taking the action <span class="math inline">\(A_t\)</span> from the state <span class="math inline">\(S_t\)</span> by <span class="math inline">\(R_{t+1}\)</span>. The set of all possible rewards is denoted by <span class="math inline">\(\mathcal{R}\)</span>. Here <span class="math inline">\(\mathcal{R} = \{-1,5,-5\}\)</span>. <span class="math inline">\(R_t\)</span> is again a random variable and we denote an observation of the reward at time <span class="math inline">\(t\)</span> by <span class="math inline">\(r_t\)</span>.</p>
</section>
<section id="state-transitions-and-rewards-probabilities" class="level3" data-number="4.2.6">
<h3 data-number="4.2.6" class="anchored" data-anchor-id="state-transitions-and-rewards-probabilities"><span class="header-section-number">4.2.6</span> State transitions and rewards probabilities</h3>
<p>Suppose now that there is a teleporter in the 4th box. This teleporter is however unreliable. Half the time, it teleports whoever steps in the box to the <span class="math inline">\(9^{th}\)</span> box, meaning Leonardo could potentially get directly to his prize! The other half of the time, however, it teleports the user to the <span class="math inline">\(7^{th}\)</span> box.</p>
<p>Suppose now that Leonardo is at state <span class="math inline">\(s_t=1\)</span>, he takes the action <span class="math inline">\(a_t = \text{down}\)</span> to the teleporter(see <a href="#fig-gridworld_teleporter" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-gridworld_teleporter</span></a>). Then:</p>
<ul>
<li>The next state is <span class="math inline">\(s_{t+1} = 9\)</span> with probability <span class="math inline">\(0.5\)</span>.</li>
<li>The next state is <span class="math inline">\(s_{t+1} = 7\)</span> with probability <span class="math inline">\(0.5\)</span>.</li>
</ul>
<p>But now, the reward he gets is random too!</p>
<ul>
<li>If he end up in the <span class="math inline">\(9^{th}\)</span> box, <span class="math inline">\(r_{t+1} = 5\)</span>.</li>
<li>If the teleporter does not work and he ends up in the <span class="math inline">\(7^{th}\)</span> box, <span class="math inline">\(r_{t+1} = -1\)</span>.</li>
</ul>
<p>More specifically, this means that state transitions and rewards need to be modelled by a probability, more specifically, the probability of getting a reward <span class="math inline">\(r\in\mathcal{R}\)</span>, and that the next state is <span class="math inline">\(s'\in\mathcal{S}\)</span> given that the agent takes the action <span class="math inline">\(a\in\mathcal{A}\)</span> at the state <span class="math inline">\(s\in\mathcal{S}\)</span>. We formalize a state transition probability as the conditional probability defined in the sample space <span class="math inline">\(\mathcal{S}\times \mathcal{R}\)</span></p>
<p><span class="math display">\[
p(s',r|s,a) = \Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a).
\]</span></p>
<div id="fig-gridworld_teleporter" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gridworld_teleporter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<embed src="images/bunny_teleporter.pdf" class="img-fluid">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gridworld_teleporter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Will it be worth the risk? Leonardo has taken the action <span class="math inline">\(a = \text{down}\)</span> at the state <span class="math inline">\(s=1\)</span>. There is a <span class="math inline">\(50\%\)</span> chance he ends up right on his prize! The state transition and reward probability is <span class="math inline">\(p(s'=9,r=5|s=1,a=down) = 0.5\)</span>. Similarly, <span class="math inline">\(p(s'=7,r=-1|s=1,a=down) = 0.5\)</span>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="finite-markov-decision-process" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="finite-markov-decision-process"><span class="header-section-number">4.3</span> Finite Markov decision process</h2>
<p>We formalize the above example by defining a Markov decision process (MDP). This definition and the ones up until the end of this chapter are adapted from <span class="citation" data-cites="reinforcementBookShiyuZhao"><a href="#ref-reinforcementBookShiyuZhao" role="doc-biblioref">[3]</a></span>.</p>
<div id="def-Markov_decision_process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1</strong></span> <strong>(Markov decision process)</strong>. A finite Markov decision process (MDP) is defined as a discrete time process, where we have:</p>
<ul>
<li>A finite set of all states <span class="math inline">\(\mathcal{S}\)</span>.</li>
<li>A finite set of all possible actions <span class="math inline">\(\mathcal{A}\)</span>.</li>
<li>A reward set <span class="math inline">\(\mathcal{R}(s,a)\)</span>, which contains the potential rewards received after taking any action <span class="math inline">\(a\in\mathcal{A}\)</span> from any state <span class="math inline">\(s\in\mathcal{S}\)</span>.</li>
</ul>
<p>We use the notation <span class="math inline">\(S_t, A_t\)</span> as the state and action of the process at time <span class="math inline">\(t\)</span>. The reward <span class="math inline">\(R_t\)</span> is the reward received at time t. <span class="math inline">\(S_t,A_t\)</span> and <span class="math inline">\(R_t\)</span> are random variables.</p>
<p>A Markov decision process also has a model, which consists of the state and reward transition probabilities:</p>
<ul>
<li>The probability, given that the current state is <span class="math inline">\(s\)</span>, and that the action taken is <span class="math inline">\(a\)</span>, that the next state is <span class="math inline">\(s'\)</span> and the next reward is <span class="math inline">\(r\)</span>. That is <span class="math inline">\(p(s',r|s,a) = \Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)\)</span>.</li>
</ul>
<p>Furthermore, a Markov decision process has a policy that governs, for any state <span class="math inline">\(s\in\mathcal{S}\)</span>, the probability of taking action <span class="math inline">\(a\in\mathcal{A}\)</span>, that probability is <span class="math inline">\(\pi(a|s) = \Pr(A_t = a|S_t = s)\)</span>. We assume that the policy is not dependent on time.</p>
<p>Finally, a Markov decision process has the Markov property, or lack of memory. The state transition and rewards probabilities are only dependent on the current state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span>, and not the states and actions that preceeded. Mathematically, <span class="math inline">\(\Pr(S_{t+1} = s', R_{t+1} = r|S_t, A_t, S_{t-1}, A_{t-1}, \dots , S_0, A_0) = \Pr(S_{t+1} = s', R_{t+1} = r | S_t, A_t)\)</span>.</p>
</div>
<p>An example of Markov decision process with two states can be seen in <a href="#fig-MDP_example" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-MDP_example</span></a>.</p>
<div id="fig-MDP_example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MDP_example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<embed src="images/MDP.pdf" class="img-fluid" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MDP_example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: An example of a Markov decision process with two states <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> and two possible actions <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span> for each states. The dashed lines represent the model transitions. After each action, the process get to a new state and a reward is given, here in dark red.
</figcaption>
</figure>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The state space <span class="math inline">\(\mathcal{S}\)</span> and the action space <span class="math inline">\(\mathcal{A}\)</span> can be finite or not. We only consider the case of finite Markov decision process to make matters easier.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The model in a Markov decision process is often impossible to define in advance. This problem is remedied by using <em>model free</em> algorithms.</p>
</div>
</section>
<section id="state-value-and-bellman-equation" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="state-value-and-bellman-equation"><span class="header-section-number">4.4</span> State Value and Bellman Equation</h2>
<p>We have a Markov decision process, which serves as a nice mathematical formalization of an agent and its environment <span class="citation" data-cites="Sutton1998"><a href="#ref-Sutton1998" role="doc-biblioref">[2]</a></span>. Now we want to train the agent to make the best possible decisions? Answering this question is the goal of the next sections.</p>
<p>We first define a trajectory. We denote by <span class="math inline">\(S_t\)</span> the state of an agent at instant <span class="math inline">\(t\)</span>. Then, according to the policy, this agent takes the action <span class="math inline">\(A_t\)</span>. After taking this action, the agent is now at the state <span class="math inline">\(S_{t+1}\)</span>, and it gets the rewards <span class="math inline">\(R_{t+1}\)</span>. Then the agent takes action <span class="math inline">\(A_{t+1}\)</span>, and gets to a new state <span class="math inline">\(S_{t+2}\)</span> with reward <span class="math inline">\(R_{t+2}\)</span>. This continues indefinitely. We define the trajectory of an agent with starting state <span class="math inline">\(S_t = s_t\)</span> as the chain of states, actions and rewards from time <span class="math inline">\(t\)</span> onward:</p>
<p><span class="math display">\[
S_t = s_t,A_t \to R_{t+1},S_{t+1},A_{t+1} \to R_{t+2},S_{t+2},A_{t+2} \to \cdots,
\]</span></p>
<p>Note that, due to the Markov property and the fact that we assume the policy is time independent, the starting value of <span class="math inline">\(t\)</span> is not important.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>In some environments, it is natural for the agent to have a task that has a starting state and a finishing states (for example, beginning a cooking session and finishing it, or starting a game and winning/losing at it.) We call these tasks <em>episodic tasks</em> and in these cases, a finite trajectory <span class="math inline">\(S_0,A_0 \to \dots \to S_T\)</span> is also called an <em>episode</em>. In the cases where the task is such that no such state can be defined, a trajectory is not finite and we call these tasks <em>continuing tasks</em>, which will be the case in this thesis.</p>
</div>
<p>In reinforcement learning setting, we assume that we have no control of the environment model (for example, one can not change the rules of a game), but that we have control over the agent decisions (i.e the policy) and how we reward that agent. The goal of any reinforcement learning algorithm is thus to define the rewards properly and then to find a policy that maximizes the rewards the agent gets. We now define the discounted return along a trajectory,</p>
<div id="def-discount" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2</strong></span> Let <span class="math inline">\(t = 0, 1, \dots\)</span>. The (discounted) return along the trajectory <span class="math inline">\(S_t,A_t \to S_{t+1},A_{t+1}, R_{t+1} \to S_{t+2},A_{t+2}, R_{t+2} \to \dots\)</span> is the random variable given by</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{+\infty}\gamma^k R_{t+1+k},
\]</span></p>
<p>where <span class="math inline">\(\gamma \in [0,1)\)</span> is called the discount rate.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>By setting a discount rate that is less than 1 in continuing tasks, we make sure that the discounted return is well defined in the case of bounded rewards. Indeed, if, for any <span class="math inline">\(t\)</span>, <span class="math inline">\(|R_t|\leq M\)</span>, then <span class="math inline">\(\sum_{k=0}^{+\infty}|\gamma^k R_{t+1+k}| \leq  \sum_{k=0}^{+\infty}\gamma^k M = \frac{M}{1-\gamma}\)</span>, so the series is absolutely convergent.</p>
</div>
<p>The <em>discounted return</em> is thus the sum of rewards along a trajectory, with a penalty for rewards far in the future, controlled by the <em>discount rate</em>. The discount rate is chosen depending on whether we want the agent to favor short term rewards, in which case a discount rate closer to <span class="math inline">\(0\)</span> can be chosen, or long term rewards, with a discount rate closer to <span class="math inline">\(1\)</span>.</p>
<p>Since the discounted return is a random variable, we can look at its expectation, in particular, we are interested in its conditional expectation, given a starting state <span class="math inline">\(S_t = s\)</span>. This expectation is called the state value <span class="citation" data-cites="Sutton1998"><a href="#ref-Sutton1998" role="doc-biblioref">[2]</a></span>.</p>
<div id="def-state_value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3</strong></span> <strong>State value</strong> The state value of a state <span class="math inline">\(s\)</span> is the function, defined for any <span class="math inline">\(s\in\mathcal{S}\)</span> as the conditional expectation of the discounted return, given <span class="math inline">\(S_t = s\)</span>,</p>
<p><span class="math display">\[
v_\pi(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s],
\]</span></p>
<p>where <span class="math inline">\(\pi\)</span> is a given policy.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Once again, the Markov property and the time independence of the policy mean that the state value does not depend on time.</p>
</div>
<p>We remark that</p>
<p><span class="math display">\[\begin{align}   
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \nonumber\\
&amp;=R_{t+1} + \gamma \left(R_{t+2} + \gamma R_{t+3}+ \dots \right) \nonumber\\
&amp;=R_{t+1} + \gamma G_{t+1}.
\end{align}\]</span></p>
<p>This expression of the return can be used in conjunction with the definition of the state value above to get</p>
<p><span id="eq-state_value_part1"><span class="math display">\[
v_\pi(s) = E[G_t|S_t = s]= E[R_{t+1}| S_t = s] + \gamma E[G_{t+1} | S_t = s].
\tag{4.1}\]</span></span></p>
<p>The first term is the expectation of immediate reward, following a certain policy <span class="math inline">\(\pi\)</span>, the second is the expectation of future rewards. Let us expand on that formula a bit more. We now make use of the “law of total expectation”:</p>
<div id="thm-total_expectation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1</strong></span> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables, and suppose <span class="math inline">\(E[|Y|]&lt;\infty\)</span>. Then <span class="math display">\[
E[Y] = E\left[E[Y|X]\right]
\]</span></p>
</div>
<p>Using this, the expectation of immediate reward is</p>
<p><span class="math display">\[
E[R_{t+1}| S_t = s] = E\big[E[R_{t+1}|S_t = s,A_t]\big] = \sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}} \sum_{s'\in \mathcal{S}}rp(s',r|s,a).
\]</span></p>
<p>We now develop the second part in the RHS of <a href="#eq-state_value_part1" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-state_value_part1</span></a>, and use the law of total expectation again to get</p>
<p><span class="math display">\[
E[G_{t+1} | S_t = s] = E\big[E[G_{t+1} | S_t = s , S_{t+1}]\big] = \sum_{s'\in\mathcal{S}}p(s'|s)E[G_{t+1}|S_t = s, S_{t+1} = s'],
\]</span></p>
<p>where <span class="math inline">\(p(s'|s) = \sum_{a\in\mathcal{A}} \sum_{r\in\mathcal{R}}p(s',r|s,a)\pi(a|s)\)</span> is the probability of the next state being <span class="math inline">\(s'\)</span> if the current state is <span class="math inline">\(s\)</span>. Because of the Markov property of the MDP, we can remove the conditioning <span class="math inline">\(S_t = s\)</span> and thus, <span class="math inline">\(E[G_{t+1}|S_t = s, S_{t+1} = s'] = E[G_{t+1}|S_{t+1} = s] = v_\pi(s')\)</span>. Then <span id="eq-state_value_part2"><span class="math display">\[
E[G_{t+1} | S_t = s] = \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A}}\sum_{r\in\mathcal{R}}v_\pi(s')\pi(a|s)p(s',r|s,a).
\tag{4.2}\]</span></span></p>
<p>Putting <a href="#eq-state_value_part1" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-state_value_part1</span></a> and <a href="#eq-state_value_part2" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-state_value_part2</span></a> together, we get Bellman’s equation:</p>
<p><span id="eq-Bellman"><span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}}\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}\pi(a|s)p(s',r|s,a)\left[ r + \gamma v_\pi(s')\right].
\tag{4.3}\]</span></span></p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The Bellman equation depends on the given policy and gives a recursive relation for the state values. Solving this equation is called policy evaluation which involves fixed point iterations (see example below).</p>
</div>
<div id="exm-the_example" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> We can directly derive the state values in the MDP in <a href="#fig-MDP_example" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-MDP_example</span></a>. We remark that in this example, given a specific state transition, the reward we get is deterministic, which simplifies the computations.</p>
<p>In particular, for the state <span class="math inline">\(s_2\)</span>, There are two possible actions <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span> we can take. The policy is to take action <span class="math inline">\(a_0\)</span> with a probability <span class="math inline">\(0.6\)</span>, and action <span class="math inline">\(a_1\)</span> with a probability <span class="math inline">\(0.4\)</span>. When we take for example action <span class="math inline">\(a_0\)</span>, the probability of the next state being <span class="math inline">\(s_1\)</span> is <span class="math inline">\(0.3\)</span>, in which case the reward is <span class="math inline">\(5\)</span>. Proceeding similarly for all the possible actions and rewards, we get</p>
<p><span class="math display">\[\begin{align*}
v_\pi(s_2) &amp;= \sum_{a=0}^1\sum_{r\in\mathcal{R}}\sum_{s'=1}^2\pi(a|s)p(s',r|s_2,a)\left[ r + \gamma v_\pi(s')\right]\\
&amp;= 0.6 \sum_{r\in\mathcal{R}}\sum_{s'=1}^2 p(s',r|s_2,a=0)\left[ r + \gamma v_\pi(s')\right] + 0.4 \sum_{r\in\mathcal{R}}\sum_{s'=1}^2 p(s',r|s_2,a=1)\left[ r + \gamma v_\pi(s')\right]\\
&amp;= 0.6 \left[0.3(5 + \gamma v_\pi(s_1)) + 0.7(-2 + \gamma v_\pi(s_2))\right] + 0.4\left[ 0.2(-2 + \gamma v_\pi(s_2)) + 0.8(5+\gamma v_\pi(s_1) \right].
\end{align*}\]</span></p>
<p>After some computations, we end up with</p>
<p><span class="math display">\[
v_\pi(s_2) = 1.5 + \gamma (0.5,0.5)\begin{pmatrix}
v_\pi(s_1) \\
v_\pi(s_2)
\end{pmatrix}.
\]</span></p>
<p>Similarly <span class="math inline">\(v_\pi(s_1) = 4.1 + \gamma(0.9,0.1)(v_\pi(s_1),v_\pi(s_2))^\intercal\)</span>. This leads to the system:</p>
<p><span class="math display">\[
\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)
\end{pmatrix} = \begin{pmatrix}
4.1\\
1.5
\end{pmatrix} + \gamma \begin{pmatrix}
0.9 &amp; 0.1\\
0.5 &amp; 0.5
\end{pmatrix}\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)
\end{pmatrix}.
\]</span></p>
<p>We stop here to remark that this equation is of the form <span class="math inline">\(v_\pi = r_\pi + \gamma \mathbfit{P}_\pi v_\pi\)</span>. <span class="math inline">\(\mathbfit{P}_\pi\)</span> can be related to a state transition matrix in a markov chain and is row stochastic. Furthermore, since <span class="math inline">\(\gamma&lt;1\)</span>, we motivate solving the equation by using fixed point iterations. This is the main idea behind <em>dynamic programming</em> <span class="citation" data-cites="bellman1957dynamic"><a href="#ref-bellman1957dynamic" role="doc-biblioref">[4]</a></span>. In this case, we can simply solve the system directly. For example, with <span class="math inline">\(\gamma=0.5\)</span>, we get the state values <span class="math inline">\(v_\pi(s_1) = 7.875\)</span>, <span class="math inline">\(v_\pi(s_2) = 4.625\)</span>.</p>
</div>
</section>
<section id="action-value" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="action-value"><span class="header-section-number">4.5</span> Action Value</h2>
<p>The state value gives information about a specific state, however, we are also often interested in knowing how much we stand to gain by taking a particular action at a particular state. This lead to the definition of the action value.</p>
<div id="def-action_value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4</strong></span> <strong>Action value</strong> The action value is defined as the expectation of discounted return <span class="math inline">\(G_t\)</span>, given a specific action <span class="math inline">\(a\)</span>, taken at the current state <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
q_\pi(a|s) = E\left[G_t|A_t=a,S_t=s\right] = E\left[\sum_|A_t=a,S_t=s\right],
\]</span></p>
<p>where <span class="math inline">\(G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots\)</span>.</p>
</div>
<p>We also have, from <a href="#def-state_value" class="quarto-xref">Definition&nbsp;<span class="quarto-unresolved-ref">def-state_value</span></a>, and the law of total expectation,</p>
<p><span class="math display">\[
v_\pi(s) = E[G_t|S_t = s] = E\left[E[G_t|S_t = s, A_t = a]\right].
\]</span></p>
<p>Then, <span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a|s)E\left[G_t|S_t=s,A_t =a\right],
\]</span></p>
<p>and we can get the relation between state value and action value:</p>
<p><span id="eq-state_value_action_relation"><span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}} \pi(a|s)q_\pi(a|s).
\tag{4.4}\]</span></span></p>
<p>We remark that by viewing <span class="math inline">\(\pi(a|s)\)</span> as a probability mass function, we can express the state values as another expectation:</p>
<p><span class="math display">\[
v_\pi(s) = E[q_\pi(a|s)],
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is random variable with p.m.f <span class="math inline">\(\pi(a|s)\)</span>. Actions values are important in the sense that they tell us of the “value of taking an action over another”, and they appear naturally in almost all reinforcement learning algorithms. One important thing to note is that, by “comparing” <a href="#eq-state_value_action_relation" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-state_value_action_relation</span></a> and <a href="#eq-Bellman" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Bellman</span></a>, we get an equivalent definitions of the action values as</p>
<p><span id="eq-action_value_to_state_value"><span class="math display">\[
q_\pi(a|s) = \sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}p(s',r|s,a)\left[ r + \gamma v_\pi(s')\right].
\tag{4.5}\]</span></span></p>
<p><a href="#eq-action_value_to_state_value" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-action_value_to_state_value</span></a> means that if we have access to the state values, we can compute the action values, while <a href="#eq-state_value_action_relation" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-state_value_action_relation</span></a> works in the opposite way, deriving state values from the action values.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>A more rigorous approach to derive <a href="#eq-action_value_to_state_value" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-action_value_to_state_value</span></a> would be similar to how we derive Bellman’s equation.</p>
</div>
</section>
<section id="optimal-policy-and-value-iteration" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="optimal-policy-and-value-iteration"><span class="header-section-number">4.6</span> Optimal policy and value iteration</h2>
<p>Now that we have defined the state values, we want to find a policy that maximizes them, that is, find a policy which we denote by <span class="math inline">\(\pi^*(a,s)\)</span> such that, for any state <span class="math inline">\(s\)</span> and for any policy <span class="math inline">\(\pi(a|s)\)</span>, <span class="math inline">\(v_{\pi^*}(s)\geq v_\pi(s)\)</span>. It turns out that not only this optimal policy exist, but that we can find it by repeating the following steps, starting from any policy <span class="math inline">\(\pi_0\)</span>:</p>
<ul>
<li>Test the current policy, that is evaluate the state values.</li>
<li>From these state values, compute the action values.</li>
<li>Using these action values, set a new and better policy that aim to choose the best actions.</li>
</ul>
<p>More specifically, we present the pseudo code for the value iteration algorithm.</p>
<hr>
<div class="line-block"><strong>Value iteration pseudocode</strong><br>
<strong>INPUT:</strong><br>
- An initial policy <span class="math inline">\(\pi_0\)</span>.<br>
- Discount rate <span class="math inline">\(\gamma\)</span>.<br>
- A stopping criterion.<br>
<strong>OUTPUT:</strong> An approximation of the optimal policy <span class="math inline">\(\pi^*\)</span>, at an arbitrary precision;<br>
<br>
i &lt;- 0;<br>
<strong>DO</strong>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute the state values <span class="math inline">\(v_{\pi_i}(s)\)</span>, using fixed point iterations;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>FOR</strong> all state <span class="math inline">\(s\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute, for all <span class="math inline">\(a\in\mathcal{A}\)</span>,the action values <span class="math inline">\(q_{\pi_i}(a,s)\)</span> using <a href="#eq-action_value_to_state_value" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-action_value_to_state_value</span></a> and the computed state values;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Denote by <span class="math inline">\(a^*\)</span> the action with the best action value <span class="math inline">\(q_{\pi_i}(a^*,s)\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the new policy <span class="math inline">\(\pi_{i+1}(a^*,s) = 1\)</span>, and set for all the other actions <span class="math inline">\(\pi_{i+1}(a,s) = 0\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>END FOR</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i &lt;- i + 1<br>
<strong>UNTIL</strong> Stopping criterion is met.</div>
<hr>
<p>This algorithm is important in the sense that we can prove that it converges to an optimal policy, that maximizes all state values! Unfortunately, this algorithm scales poorly. In <a href="#exm-the_example" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-the_example</span></a>, we found that computing the state values is equivalent to solving a <span class="math inline">\(2\times 2\)</span> linear system. In the general case, this system as the same dimensions as the number of states. Depending on the problem, solving this linear system can become prohibitively expensive (for example, there are several orders of magnitude more legal board states in a game of go than atoms in the observable universe <span class="citation" data-cites="trompCountingLegal"><a href="#ref-trompCountingLegal" role="doc-biblioref">[5]</a></span>), and yet we can design a program that can beat the best human players handily <span class="citation" data-cites="Silver2016"><a href="#ref-Silver2016" role="doc-biblioref">[6]</a></span>! Nevertheless, the main idea of starting with an initial policy, then getting a better and better policy over time is a fundamental idea in reinforcement learning.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-aPrimerBrain" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">E. Ludvig, M. Bellemare, and K. Pearson, <span>“A primer on reinforcement learning in the brain: Psychological, computational, and neural perspectives,”</span> in <em>Computational Neuroscience for Advancing Artificial Intelligence: Models, Methods and Applications</em>, 2011, pp. 111–144. doi: <a href="https://doi.org/10.4018/978-1-60960-021-1.ch006">10.4018/978-1-60960-021-1.ch006</a>.</div>
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R. S. Sutton and A. G. Barto, <em>Reinforcement learning: An introduction</em>, Second. The MIT Press, 2018. Available: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></div>
</div>
<div id="ref-reinforcementBookShiyuZhao" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">S. Zhao, <span>“Mathematical foundations of reinforcement learning.”</span> 2023. <a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning</a> (accessed Mar. 30, 2023).</div>
</div>
<div id="ref-bellman1957dynamic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">R. Bellman, R. E. Bellman, and R. Corporation, <em>Dynamic programming</em>. in Rand corporation research study. Princeton University Press, 1957. Available: <a href="https://books.google.se/books?id=rZW4ugAACAAJ">https://books.google.se/books?id=rZW4ugAACAAJ</a></div>
</div>
<div id="ref-trompCountingLegal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">J. Tromp, <span>“Counting legal positions in go — tromp.github.io.”</span> <a href="https://tromp.github.io/go/legal.html" class="uri">https://tromp.github.io/go/legal.html</a>.</div>
</div>
<div id="ref-Silver2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">D. Silver <em>et al.</em>, <span>“Mastering the game of go with deep neural networks and tree search,”</span> <em>Nature</em>, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: <a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./5_solverExploration.html" class="pagination-link  aria-label=" &lt;span="" runge-kutta="" method&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Explicit Runge-Kutta Method</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./7_policyGradient.html" class="pagination-link" aria-label="<span class='chapter-number'>5</span>&nbsp; <span class='chapter-title'>Policy Gradient Method</span>">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Policy Gradient Method</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>