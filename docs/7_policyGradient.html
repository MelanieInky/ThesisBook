<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Reinforcement Learning for the Optimization of Explicit Runge Kutta Method Parameters - 5&nbsp; Policy Gradient Method</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./8_implementation.html" rel="next">
<link href="./6_reinforcementBasic.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./7_policyGradient.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Policy Gradient Method</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Reinforcement Learning for the Optimization of Explicit Runge Kutta Method Parameters</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_motivation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Motivation : Pseudo time iterations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_convecDiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">A Test Problem, the Convection Diffusion Equation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_solverExploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Explicit Runge-Kutta Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_reinforcementBasic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning (RL)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_policyGradient.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Policy Gradient Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Implementation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Summary and Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#modelling-the-problem-as-a-reinforcement-learning-problem" id="toc-modelling-the-problem-as-a-reinforcement-learning-problem" class="nav-link active" data-scroll-target="#modelling-the-problem-as-a-reinforcement-learning-problem"><span class="header-section-number">5.1</span> Modelling the problem as a reinforcement learning problem</a>
  <ul class="collapse">
  <li><a href="#modelling-the-states" id="toc-modelling-the-states" class="nav-link" data-scroll-target="#modelling-the-states">Modelling the states</a></li>
  <li><a href="#modelling-the-actions-and-the-policy" id="toc-modelling-the-actions-and-the-policy" class="nav-link" data-scroll-target="#modelling-the-actions-and-the-policy">Modelling the actions and the policy</a></li>
  <li><a href="#modelling-the-rewards" id="toc-modelling-the-rewards" class="nav-link" data-scroll-target="#modelling-the-rewards">Modelling the rewards</a></li>
  <li><a href="#state-transitions" id="toc-state-transitions" class="nav-link" data-scroll-target="#state-transitions">State transitions</a></li>
  <li><a href="#other-challenges" id="toc-other-challenges" class="nav-link" data-scroll-target="#other-challenges">Other challenges</a></li>
  </ul></li>
  <li><a href="#dealing-with-a-large-state-action-space." id="toc-dealing-with-a-large-state-action-space." class="nav-link" data-scroll-target="#dealing-with-a-large-state-action-space."><span class="header-section-number">5.2</span> Dealing with a large state-action space.</a></li>
  <li><a href="#model-based-model-free" id="toc-model-based-model-free" class="nav-link" data-scroll-target="#model-based-model-free"><span class="header-section-number">5.3</span> Model-based, model-free</a></li>
  <li><a href="#policy-gradient-methods" id="toc-policy-gradient-methods" class="nav-link" data-scroll-target="#policy-gradient-methods"><span class="header-section-number">5.4</span> Policy gradient methods</a>
  <ul class="collapse">
  <li><a href="#objective-function" id="toc-objective-function" class="nav-link" data-scroll-target="#objective-function"><span class="header-section-number">5.4.1</span> Objective function</a></li>
  <li><a href="#policy-gradient-theorem" id="toc-policy-gradient-theorem" class="nav-link" data-scroll-target="#policy-gradient-theorem"><span class="header-section-number">5.4.2</span> Policy gradient theorem</a></li>
  <li><a href="#reinforce-algorithm" id="toc-reinforce-algorithm" class="nav-link" data-scroll-target="#reinforce-algorithm"><span class="header-section-number">5.4.3</span> REINFORCE algorithm</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Policy Gradient Method</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Now that we have access to the main definitions used in RL, we can study the problem we had at the end of chapter 3 through the lens of RL.</p>
<p>The last chapter has been quite long as we introduced reinforcement learning, so as a reminder, we summarize the work we’ve done so far. We have a test problem, the convection diffusion equation <a href="#eq-convec_diff_steady" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-convec_diff_steady</span></a>, which we discretize, and with the following <strong>problem parameters</strong>:</p>
<ul>
<li><p>A parameter <span class="math inline">\(b \in [0,1]\)</span> in the steady-state convection diffusion equation , and</p></li>
<li><p>a discretization parameter <span class="math inline">\(n\in\mathbb{N}\)</span> defining the number of interior points in the linear grid used to discretize this equation.</p></li>
</ul>
<p>We end up with a linear system of the form <span class="math inline">\(\mathbfit{Mu} = \mathbfit{e}\)</span> (<a href="#eq-test_problem_scaled_system" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-test_problem_scaled_system</span></a>). To solve this system, we solve the ODE <span class="math inline">\(\mathbfit{u}'(t) = \mathbfit{e} - \mathbfit{Mu}(t)\)</span> using an explicit Runge-Kutta method with two <strong>solver parameters</strong> (see <a href="#sec-RK_solver_test_problem" class="quarto-xref"><span class="quarto-unresolved-ref">sec-RK_solver_test_problem</span></a>):</p>
<ul>
<li><span class="math inline">\(\Delta t\)</span>, the (pseudo) time step, and</li>
<li><span class="math inline">\(\alpha\)</span>, a parameter specific to the Runge Kutta method used.</li>
</ul>
<p>We relate this solver to a stationary iterative method of the form <span class="math inline">\(\mathbfit{u}_{n+1} = \mathbfit{Ku}_n + \mathbfit{Le}\)</span>, where <span class="math inline">\(\mathbfit{K} = \mathbfit{I}-\mathbfit{LM}\)</span>. This method is convergent if and only if the spectral radius of <span class="math inline">\(\mathbfit{K}\)</span> is strictly less than one. We could compute this spectral radius, but this is an computationally intensive task, so we use an approximation. This approximation is the residual ratio after 10 iterations of the Runge-Kutta solver, starting with <span class="math inline">\(\mathbfit{u}_0 = \mathbfit{e}\)</span>.</p>
<p>We define this ratio as <span class="math inline">\(\rho_{10, b, n}(\Delta t, \alpha)\)</span>, a function parametrized by <span class="math inline">\(b\)</span> and <span class="math inline">\(n\)</span>, with arguments <span class="math inline">\(\Delta t\)</span> and <span class="math inline">\(\alpha\)</span>. We are faced with the following optimization problem:</p>
<p>For any problem parameters <span class="math inline">\(b\)</span>, <span class="math inline">\(n\)</span>, find the optimal solver parameters <span id="eq-optimization_problem"><span class="math display">\[
(\Delta t^*, \alpha^*) =  \arg \min_{\Delta t, \alpha} \rho_{10, b , n}(\Delta t, \alpha).
\tag{5.1}\]</span></span></p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>We’ve already seen in <a href="#sec-small_experiment" class="quarto-xref"><span class="quarto-unresolved-ref">sec-small_experiment</span></a> that the optimal parameters can lead to divergence of the solver once more iterations are computed, which is problematic, so we are perfectly happy to find “good enough, but not optimal” solver parameters where this issue will not happen, hopefully. This issue can be mitigated by computing the residual ratio after more iterations, at the cost of it being more computationally expensive.</p>
</div>
<section id="modelling-the-problem-as-a-reinforcement-learning-problem" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="modelling-the-problem-as-a-reinforcement-learning-problem"><span class="header-section-number">5.1</span> Modelling the problem as a reinforcement learning problem</h2>
<p>We are interested in using reinforcement learning to solve the above problem. The last chapter provided an overview of the elements of reinforcement learning, and we can now translate our problem in a RL setting.</p>
<section id="modelling-the-states" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="modelling-the-states">Modelling the states</h3>
<p>We start by modelling the states. The most natural way of defining the states is to use the problem parameters <span class="math inline">\(b\)</span> and <span class="math inline">\(n\)</span>. We thus define a specific state as a pair of problem parameters <span class="math inline">\(s = (b,n) \in [0,1]\times \mathbb{N^*}\)</span>.</p>
</section>
<section id="modelling-the-actions-and-the-policy" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="modelling-the-actions-and-the-policy">Modelling the actions and the policy</h3>
<p>Once we know a specific state, that is the problem parameters, we need to choose the two solver parameters <span class="math inline">\(\Delta t\)</span> and <span class="math inline">\(\alpha\)</span>. A specific action is then a pair <span class="math inline">\(a = (\Delta t, \alpha) \in \mathbb{R^+}\times [0,1]\)</span>. The policy is then denoted by <span class="math inline">\(\pi(a = (\Delta t, \alpha) |s = (b,n))\)</span>. We will discuss the policy more in depth in the next chapter.</p>
</section>
<section id="modelling-the-rewards" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="modelling-the-rewards">Modelling the rewards</h3>
<p>Once a state-action pair is chosen, the residual ratio <span class="math inline">\(\rho_{10,b,n}(\Delta t, \alpha)\)</span> is computed. The reward can then be defined as a function of the computed residual ratio,</p>
<p><span class="math display">\[
r = 1 - \rho_{10,b,n}(\Delta t, \alpha).
\]</span></p>
<p>This reward is positive when the residual ratio is less than one, and negative otherwise. This mean that a reinforcement learning agent, which seek to maximize the reward it gets, will aim to minimize the residual ratio.</p>
</section>
<section id="state-transitions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="state-transitions">State transitions</h3>
<p>In the definition of a Markov decision process (<a href="#def-Markov_decision_process" class="quarto-xref">Definition&nbsp;<span class="quarto-unresolved-ref">def-Markov_decision_process</span></a>), we also have a probabilistic model of the state and rewards transition <span class="math inline">\(p(s',r|s,a)\)</span>. Right away, we can see that this model is difficult to define, as we can not now, for a specific state and action, what reward we will get.</p>
<p>On the other hand, we can still control the state transitions. In this regard, we choose a new state, at random, after an action and reward is computed. More precisely, we choose a new parameter <span class="math inline">\(b\)</span>, uniformly between 0 and 1, and a new parameter <span class="math inline">\(n\)</span>, between <span class="math inline">\(5\)</span> and <span class="math inline">\(200\)</span>, following a discrete uniform distribution as well. These values for <span class="math inline">\(n\)</span> are arbitrary, with a maximum of <span class="math inline">\(200\)</span> to spare us of long computational time when computing the rewards. We also cap the minimum value of <span class="math inline">\(n\)</span> to an arbitrary minimum of <span class="math inline">\(5\)</span> as those values are simply too low to get a acceptable discretization error, and we do not want to train an agent to solve for these states.</p>
</section>
<section id="other-challenges" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="other-challenges">Other challenges</h3>
<p>There are still several challenges that need to be addressed:</p>
<ul>
<li>In our problem, the State-Action space is continuous. We previously assumed finite spaces.</li>
<li>In the definition of a MDP, we have a model: if we know a specific state and action, we have a probabilistic model of the reward the agent get and the next state of the environment. In our case, we know the model the state transitions, but we have no way of knowing the rewards.</li>
</ul>
</section>
</section>
<section id="dealing-with-a-large-state-action-space." class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="dealing-with-a-large-state-action-space."><span class="header-section-number">5.2</span> Dealing with a large state-action space.</h2>
<p>In the last chapter, we made the assumption that every space, be it state, action, or reward is finite. This assumption, while practical to derive theoretical results from, is in practice not always followed, as some states may be continuously defined for example.</p>
<p>We take our problem as formulated before. The state is defined as the problem parameters, that is <span class="math inline">\(b\in[0,1]\)</span> and <span class="math inline">\(n = 1 , 2, \dots\)</span>. Without any adjustment, the state space is of the form <span class="math inline">\([0,1] \times \mathbb{N}\)</span>, and is not finite.</p>
<p>Similarly, the policy is defined by choosing the values <span class="math inline">\((\alpha,\Delta t) \in [0,1]\times \mathbb{R}^+\)</span>, depending on the state. Once again, the action space is continuous.</p>
<p>One approach would be to discretize the entire state <span class="math inline">\(\times\)</span> action space, and then to apply classical dynamic programming algorithms to get some results. Then, after an optimal policy is found, do some form of interpolation for problem parameters outside of the discretized space. This approach has its own merit, as there are 3 dimensions that need to be discretized, and <span class="math inline">\(n\)</span> can be chosen within a finite range.</p>
<p>Another approach is to use an approximation function. One way to do that is to approximate the value function <span class="math inline">\(v(s)\)</span> by some parametrization <span class="math inline">\(v(s) \approx \hat{v}(s,\omega)\)</span> where <span class="math inline">\(\omega \in \mathbb{R}^d\)</span> are <span class="math inline">\(d\)</span> parameters. Such methods are called <em>value based</em>. The method we use in this thesis, on the other hand, use an approximation of the policy function defined as <span class="math inline">\(\pi(a|s,\mathbfit{\theta})\)</span>, where <span class="math inline">\(\mathbfit{\theta}\in \mathbb{R}^d\)</span> is a parameter vector. Such methods are called <em>policy based</em>. The reason to chose from this class of algorithm is two-fold.</p>
<ul>
<li><p>When thinking about the test problem, a straightforward approach is to choose the solver parameters as a linear function of the problem parameters. A policy based approach allow us to do exactly this.</p></li>
<li><p>By doing so, we automatically take care of the need to interpolate between discrete states and action, which would be another headache we would have to deal with.</p></li>
</ul>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Approximation is usually done using neural networks. In the case of this thesis, a linear approximation is used.</p>
</div>
</section>
<section id="model-based-model-free" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="model-based-model-free"><span class="header-section-number">5.3</span> Model-based, model-free</h2>
<p>One problem we are faced with is the issue of the model of the state and rewards transition, that is</p>
<p><span class="math display">\[
p(s',r|s,a) = \Pr(S_{t+1} = s', R_{t+1} = a | S_t = s, A_t = a).
\]</span></p>
<p>Thankfully, we are dealing with random variables, and with random variables, Monte-Carlo methods follow.</p>
<p>In particular, we often only need to compute the expectations of functions of random variables. This can be done is the following way. Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(x_1, x_2, \dots , x_n\)</span> be independent samples of <span class="math inline">\(X\)</span>. Then, we can estimate <span class="math inline">\(E[X]\)</span> as the empirical mean of our samples, that is:</p>
<p><span class="math display">\[
\hat{X}_n = \frac{x_1 + \dots + x_n}{n}.
\]</span></p>
</section>
<section id="policy-gradient-methods" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="policy-gradient-methods"><span class="header-section-number">5.4</span> Policy gradient methods</h2>
<section id="objective-function" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="objective-function"><span class="header-section-number">5.4.1</span> Objective function</h3>
<p>Let <span class="math inline">\(\mathbfit{\theta} \in \mathbb{R}^d\)</span> be a parameter vector and <span class="math inline">\(\pi(a|s,\mathbfit{\theta}) = p(A_t = a | S_t = s , \mathbfit{\theta})\)</span> an approximate policy that is derivable w.r.t <span class="math inline">\(\theta\)</span>. We want to define an objective function <span class="math inline">\(J(\mathbfit{\theta})\)</span> that we want to maximize in order to find the best value of <span class="math inline">\(\mathbfit{\theta}\)</span>.</p>
<p>To this end, we make the following assumption, the first one being for simplicity, and the second one being specific to the problem we modelled in the former sections in this chapter.</p>
<ul>
<li>The states and action set are finite.</li>
<li>The states are uniformly distributed, and so are the state transitions. That is, for any <span class="math inline">\(s,s'\in\mathcal{S}, \Pr(S_t = s) = 1 / |\mathcal{S}| = \Pr(S_t = s | S_{t-1} = s')\)</span>, where <span class="math inline">\(|\mathcal{S}|\)</span> is the number of element of <span class="math inline">\(S\)</span>. This correspond to the idea of taking a new state at random in our problem.</li>
</ul>
<p>We define the objective function</p>
<p><span id="eq-objective_function"><span class="math display">\[
J(\mathbfit{\theta}) = \overline{v_\pi(S)} = \frac{1}{|\mathcal{S}|}\sum_{s\in S}v_\pi(s).
\tag{5.2}\]</span></span></p>
<p>That is, <span class="math inline">\(J(\mathbfit{\theta})\)</span> is the average, (non weighted, as per assumption) state value.</p>
<p>We want to maximize this objective function by changing the policy parameter <span class="math inline">\(\theta\)</span>. To this end, we use a gradient ascent algorithm of the form</p>
<p><span id="eq-gradient_ascent_algorithm"><span class="math display">\[
\mathbfit{\theta}_{t+1} = {\mathbfit{\theta}}_t + \alpha \nabla_{\mathbfit{\theta}} J(\mathbfit{\theta}),
\tag{5.3}\]</span></span></p>
<p>where <span class="math inline">\(\nabla_{\mathbfit{\theta}}\)</span> represents the gradient operator, w.r.t <span class="math inline">\(\mathbfit{\theta}\)</span>. This gradient is</p>
<p><span id="eq-objective_gradient"><span class="math display">\[
\nabla_{\mathbfit{\theta}} J(\mathbfit{\theta}) = \frac{1}{|\mathcal{S}|}\sum_{s\in S}\nabla_{\mathbfit{\theta}}v_\pi(s).
\tag{5.4}\]</span></span></p>
<p>We are faced with the immediate issue that the algorithm requires knowing this gradient.</p>
</section>
<section id="policy-gradient-theorem" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="policy-gradient-theorem"><span class="header-section-number">5.4.2</span> Policy gradient theorem</h3>
<p>We prove, using the aforementioned assumptions a specific case of the policy gradient theorem. This proof is adapted from <span class="citation" data-cites="Sutton1998"><a href="#ref-Sutton1998" role="doc-biblioref">[1]</a>,chap 13.2</span>. We also remind the reader that both the state values and the action values depend on the policy <span class="math inline">\(\pi\)</span> and thus depend on <span class="math inline">\(\mathbfit{\theta}\)</span>.</p>
<p>From the last chapter, we have the expression of the state values <span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a|s, \mathbfit{\theta})q_\pi(a,s).
\]</span></p>
<p>We take the gradient of <span class="math inline">\(v_\pi(s)\)</span> w.r.t <span class="math inline">\(\theta\)</span> to get <span id="eq-gradient_wrt_theta"><span class="math display">\[
\nabla_{\mathbfit{\theta}} v_\pi(s) = \sum_{a\in\mathcal{A}}\nabla_{\mathbfit{\theta}}\pi(a|s, {\mathbfit{\theta}})q_\pi(a,s) + \pi(a|s,{\mathbfit{\theta}})\nabla_{\mathbfit{\theta}} q_\pi(a,s).
\tag{5.5}\]</span></span></p>
<p>We now turn our attention to the <span class="math inline">\(\nabla q_\pi(a,s)\)</span> term above. We use the expression of the actions value in <a href="#eq-action_value_to_state_value" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-action_value_to_state_value</span></a>,</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} q_\pi(a,s) = \nabla_{\mathbfit{\theta}} \left[ \sum_{r}\sum_{s'} p(s',r|s,a)(r + \gamma v_\pi(s'))\right].
\]</span></p>
<p>Both <span class="math inline">\(p(s',r|s,a)\)</span> and the reward <span class="math inline">\(r\)</span> do not depend on the policy, and therefore not on <span class="math inline">\(\mathbfit{\theta}\)</span>. The gradient is thus</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} q_\pi(a,s) =  \gamma\sum_{s'}\left[\sum_{r}p(s',r|a,s)\right]\nabla_{\mathbfit{\theta}} v_\pi(s').
\]</span></p>
<p>By the assumption of the state transition probabilities and the law of total probabilities <span class="math inline">\(\sum_{r}p(s',r|a,s)= 1/|\mathcal{S}|\)</span>, and thus</p>
<p><span class="math display">\[
\nabla q_\pi(a,s) =  \gamma\sum_{s'}\frac{1}{|\mathcal{S}|}\nabla v_\pi(s').
\]</span></p>
<p>We recognize the expression of the objective function’s gradient <span class="math inline">\(\nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}})\)</span> to get <span class="math inline">\(\nabla_{\mathbfit{\theta}} q_\pi(a,s) = \gamma \nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}})\)</span>. We insert this in <a href="#eq-gradient_wrt_theta" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-gradient_wrt_theta</span></a> and we get</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} v_\pi(s) = \sum_{a\in\mathcal{A}}\nabla_{\mathbfit{\theta}}\pi(a|s, {\mathbfit{\theta}})q_\pi(a,s) + \gamma\pi(a|s, {\mathbfit{\theta}})\nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}}).
\]</span></p>
<p>Since the policy <span class="math inline">\(\pi(a|s)\)</span> is a probability over the action space, it sums to <span class="math inline">\(1\)</span> and we can get the second part of the RHS out of the sum</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} v_\pi(s) = \gamma J({\mathbfit{\theta}}) + \sum_{a\in\mathcal{A}}\nabla_{\mathbfit{\theta}}\pi(a|s,{\mathbfit{\theta}})q_\pi(a,s).
\]</span></p>
<p>Using <span class="math inline">\(\nabla J({\mathbfit{\theta}}) = \frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}\nabla_{\mathbfit{\theta}} v_\pi(s)\)</span>, we get</p>
<p><span class="math display">\[\begin{align}
\nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}}) &amp;= \frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}\left[\gamma J({\mathbfit{\theta}}) + \sum_{a\in\mathcal{A}}\nabla_{\mathbfit{\theta}}\pi(a|s, {\mathbfit{\theta}})q_\pi(a,s)\right],\\
&amp;= \gamma \nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}}) + \sum_{s\in\mathcal{S}}\frac{1}{|\mathcal{S}|}\sum_{a\in\mathcal{A}}\nabla_{\mathbfit{\theta}}\pi(a|s, {\mathbfit{\theta}})q_\pi(a,s).
\end{align}\]</span></p>
<p>And after a small rearrangement of the terms,</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} J(\mathbfit{\theta}) = \frac{1}{1-\gamma}\sum_{s\in \mathcal{S}}\frac{1}{|\mathcal{S}|}\sum_{a\in\mathcal{A}}\nabla\pi(a|s,{\mathbfit{\theta}})q_\pi(a,s,{\mathbfit{\theta}}).
\]</span></p>
<p>This is a special case of the policy gradient theorem. The reason to put the fraction <span class="math inline">\(1/|\mathcal{S}|\)</span> inside the first sum is to get a parallel with the more general expression, where in general, we have a weighted sum with different weight depending on the state. Depending on the objective function used, this can be for example the stationary distribution of the states for a given policy.</p>
<p>We state the policy gradient theorem in a more general form.</p>
<div id="thm-policyGradient" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1</strong></span> <strong>Policy gradient theorem</strong> (For continuing cases, with discount factor <span class="math inline">\(\gamma &lt; 1\)</span>.)</p>
<p>Let <span class="math inline">\(\pi(a|s,\mathbfit{\theta})\)</span> be a stochastic policy that is derivable w.r.t <span class="math inline">\(\mathbfit{\theta}\)</span>.</p>
<p>Let <span class="math inline">\(\mu(s)\)</span> be the probability mass function of the stationary distribution of the states, following the policy <span class="math inline">\(\pi\)</span>.</p>
<p>Define the objective function <span class="math inline">\(J(\mathbfit{\theta}) = \overline{v_\pi(S)} = \sum_{s\in \mathcal{S}}\mu(s)v_\pi(s)\)</span>. The gradient of <span class="math inline">\(J\)</span> w.r.t <span class="math inline">\(\mathbfit{\theta}\)</span> is then proportional to the weighted sum</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}}) \propto \sum_s \mu(s) \sum_a q_\pi(a,s, {\mathbfit{\theta}})\nabla \pi(a|s, {\mathbfit{\theta}}).
\]</span></p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>With our assumptions <span class="math inline">\(\mu(s) = \frac{1}{|S|}\)</span>. A proper treatment of the problem would involve properly defining Markov chains and stationary distributions, which is out of the scope of this thesis. We’ve seen in example <a href="#exm-the_example" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-the_example</span></a> that the state transition matrix <span class="math inline">\(P_\pi\)</span> appears. This relation between Markov chains and MDP is explored in <span class="citation" data-cites="reinforcementBookShiyuZhao"><a href="#ref-reinforcementBookShiyuZhao" role="doc-biblioref">[2]</a></span>, as well as the policy gradient theorem. For more information on Markov chains, see <span class="citation" data-cites="ross2007second"><a href="#ref-ross2007second" role="doc-biblioref">[3]</a></span>.</p>
</div>
<p>The policy gradient theorem is powerful in the sense that we can derive the gradient of the objective function, something that is tied to the environment, to establishing the gradient of the parametrized policy function, which we have more control over.</p>
</section>
<section id="reinforce-algorithm" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="reinforce-algorithm"><span class="header-section-number">5.4.3</span> REINFORCE algorithm</h3>
<p>Here, we introduce reinforce the classic REINFORCE algorithm <span class="citation" data-cites="Williams1992"><a href="#ref-Williams1992" role="doc-biblioref">[4]</a></span>. Even with the policy gradient theorem, we are still faced with the problem of estimating the action values <span class="math inline">\(q_\pi\)</span>. We remark that the formula in the policy gradient is an expectation,</p>
<p><span class="math display">\[
\nabla_{\mathbfit{\theta}} J(\mathbfit{\theta}) \propto E\left[\sum_a q_\pi(a,S)\nabla \pi(a|S, \mathbfit{\theta})\right],
\]</span></p>
<p>where <span class="math inline">\(S\)</span> is the random variable given by the probability mass function <span class="math inline">\(\mu(s)\)</span>. By using the identity <span class="math inline">\(\frac{\nabla f}{f} = \nabla \ln f\)</span>, we can also rewrite the inner term as</p>
<p><span class="math display">\[
\sum_a q_\pi(a,S)\nabla_{\mathbfit{\theta}} \pi(a|S, {\mathbfit{\theta}}) = \sum_a \pi(a|S)q_\pi(a,S)\nabla_{\mathbfit{\theta}} \ln \pi(a|S, {\mathbfit{\theta}}),
\]</span></p>
<p>which is also an expectation, and thus</p>
<p><span class="math display">\[
\nabla J(\theta) \propto E\left[q_\pi(A,S)\nabla_{\mathbfit{\theta}} \ln \pi(A|S, {\mathbfit{\theta}})\right].
\]</span></p>
<p>We also know from before that the action value is also the conditional expectation of the return <span class="math inline">\(q_\pi(s,a) = E[G_t|S_t = s, A_t = a]\)</span>. Thus,</p>
<p><span id="eq-policy_gradient_expectation"><span class="math display">\[
\nabla_{\mathbfit{\theta}} J({\mathbfit{\theta}}) \propto E\left[G_t\nabla \ln \pi(A_t|S_t, {\mathbfit{\theta}})\right].
\tag{5.6}\]</span></span></p>
<p>Note that the variable <span class="math inline">\(t\)</span> has been introduced Since this is an expectation, we can estimate it by using samples. Retracing our steps, the <span class="math inline">\(k\)</span>’th sample, which we note as <span class="math inline">\(e_k\)</span> have to be chosen as follow.</p>
<ul>
<li>Chose a state <span class="math inline">\(S_0 = s\)</span> at random, following its stationary distribution.</li>
<li>Chose an action <span class="math inline">\(A_0 = a\)</span> according to the policy <span class="math inline">\(\pi(A_0 = a|S_0 = s, {\mathbfit{\theta}})\)</span>.</li>
<li>Compute the log policy gradient. Then, get the return <span class="math inline">\(G_0 = g\)</span> for the state-action pair <span class="math inline">\((s,a)\)</span>. The sample is then <span class="math inline">\(e_k = g\nabla_{\mathbfit{\theta}} \ln \pi(a|s, {\mathbfit{\theta}})\)</span>.</li>
</ul>
<p>Then, the estimator for the RHS in <a href="#eq-policy_gradient_expectation" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-policy_gradient_expectation</span></a> is given by</p>
<p><span class="math display">\[
\hat{E}_n = \frac{1}{n}\sum_{k=1}^n e_k,
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of samples we have. Using a gradient ascent algorithm, we can update the parameters <span class="math inline">\(\mathbfit{\theta}\)</span>,</p>
<p><span class="math display">\[
\mathbfit{\theta}_{t+1} = \mathbfit{\theta}_t + \alpha\frac{1}{n}\sum_{k=1}^n e_k.
\]</span></p>
<p>This method has three problems:</p>
<ul>
<li>The states need to be chosen according to the stationary distribution <span class="math inline">\(\mu(s)\)</span>, which is not trivial. Thankfully, with our assumption of random state transitions, <span class="math inline">\(\mu(s) = \frac{1}{|\mathcal{S}|}\)</span>.</li>
<li>To get each sample <span class="math inline">\(e_k\)</span>, we need to compute a return. Doing so, we end up visiting a lot of different states and gathering a lot of information that we end up discarding. This issue is an issue of low sample efficiency and is usually best handled via temporal difference based methods, where one estimate the returns after a finite number of steps. These methods are out of scope of the thesis.</li>
<li>For continuing cases(where the are no final states), the return is an infinite sum of random variable, which we can not sample. We will have to stop after <span class="math inline">\(\tau\)</span> transitions and use the estimate <span class="math inline">\(G_t \approx \sum_{t=0}^\tau \gamma^t R_{t+1}\)</span>. This introduces some bias, in particular when <span class="math inline">\(\gamma \approx 1\)</span> and <span class="math inline">\(\tau\)</span> is small. Once again, this can be resolved by temporal difference based methods.</li>
</ul>
<p>Let us forget about the truncations issues for now. When we sample the expectation in <a href="#eq-policy_gradient_expectation" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-policy_gradient_expectation</span></a>, we get a trajectory</p>
<p><span class="math display">\[
s_0,a_0 \to s_{1}, a_{1} \to s_{2},a_{2} \dots .
\]</span></p>
<p>Then, we can estimate, via Monte Carlo estimation, the return <span class="math inline">\(G_0\)</span>. Doing this, we also have access to the trajectory</p>
<p><span class="math display">\[
s_{1}, a_{1} \to s_{2},a_{2} \dots,
\]</span></p>
<p>and thus we can also estimate the return <span class="math inline">\(G_{1}\)</span>! Therefore, we can use a single episode to estimate multiple samples! Using this idea, we can generate an episode of length <span class="math inline">\(\tau +1\)</span>:</p>
<p><span class="math display">\[
s_0,a_0 \to s_1,a_1,r_1 \to s_2,a_2,r_2 \to \dots \to s_{\tau +1}, r_{\tau +1}.
\]</span></p>
<p>For any <span class="math inline">\(t = 0,\dots ,T\)</span>, the estimated return is then defined as</p>
<p><span class="math display">\[
\hat{G}_t = \sum_{k=t}^\tau \gamma^{t-k}r_{k+1}.
\]</span></p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Because the initial state is chosen following a stationary distribution, we also ensure that the subsequent states are chosen following this same distribution.</p>
</div>
<p>We can now state the REINFORCE algorithm <span class="citation" data-cites="Williams1992"><a href="#ref-Williams1992" role="doc-biblioref">[4]</a></span>, also called policy gradient Monte-Carlo in pseudo code format:</p>
<hr>
<div class="line-block"><strong>REINFORCE algorithm pseudocode</strong><br>
<strong>INPUT:</strong><br>
- A parameter vector <span class="math inline">\(\mathbfit{\theta} \in\mathbb{R}^d\)</span>, and a parametrized policy <span class="math inline">\(\pi(a|s,\mathbfit{\theta})\)</span> with computable gradient <span class="math inline">\(\nabla_{\mathbfit{\theta}} \pi(a|s,\mathbfit{\theta})\)</span>;<br>
- Learning rate <span class="math inline">\(\alpha\)</span>;<br>
- Discount rate <span class="math inline">\(\gamma\)</span>;<br>
- Episode length <span class="math inline">\(\tau+1\)</span>;<br>
- Number of episode to iterate for <span class="math inline">\(n\)</span>;<br>
<strong>OUTPUT:</strong> The updated parameter <span class="math inline">\(\mathbfit{\theta}\)</span>;<br>
<br>
<strong>FOR</strong> <span class="math inline">\(n\)</span> episodes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generate an episode, following <span class="math inline">\(\pi(a|s,\mathbfit{\theta})\)</span>, of length T+2 the form <span class="math inline">\(s_0,a_0 \to s_1,a_1,r_1 \to s_2,a_2,r_2 \to \dots \to s_{\tau+1}, r_{\tau+1}\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>FOR</strong> t=0 … :<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute the estimated return <span class="math inline">\(\hat{G_t} = \sum_{k=t}^\tau \gamma^{t-k}r_{k+1}\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute the log gradient <span class="math inline">\(\nabla \ln \pi(a_t|s_t,\theta)\)</span>;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update <span class="math inline">\(\theta \leftarrow \theta + \alpha \hat{G_t} \nabla \ln \pi(a_t|s_t,\mathbfit{\theta})\)</span>;<br>
</div>
<hr>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Because of the finite episode length, the REINFORCE algorithm is more suited for episodic tasks, but it is also usable for continuing tasks, if we accept some bias. Another alternative to reduce bias would be to discard the last few estimated returns <span class="math inline">\(\hat{G}_\tau, \hat{G}_{\tau-1}, \dots\)</span> as they are the most biased.</p>
</div>
<p>The REINFORCE update can be interpreted as updating the parameters to make it more likely to take an action if the estimated sample return is good, and the opposite otherwise. Furthermore, by looking at the term <span class="math inline">\(\nabla \ln \pi = \frac{\nabla \pi}{\pi}\)</span>, we can see that if the probability of taking the action is low, then the gradient becomes bigger! That way, this gradient act as a balance between exploration and exploitation. Otherwise we would update the parameters as much for a rare action than a common one, and the common action is taken more often which lead to the common action having much more sway in the process.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R. S. Sutton and A. G. Barto, <em>Reinforcement learning: An introduction</em>, Second. The MIT Press, 2018. Available: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></div>
</div>
<div id="ref-reinforcementBookShiyuZhao" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">S. Zhao, <span>“Mathematical foundations of reinforcement learning.”</span> 2023. <a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning</a> (accessed Mar. 30, 2023).</div>
</div>
<div id="ref-ross2007second" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">S. M. Ross and E. A. Peköz, <em>A second course in probability</em>. ProbabilityBookstore.com, 2007. Available: <a href="https://books.google.se/books?id=g5j6DwAAQBAJ">https://books.google.se/books?id=g5j6DwAAQBAJ</a></div>
</div>
<div id="ref-Williams1992" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">R. J. Williams, <span>“Simple statistical gradient-following algorithms for connectionist reinforcement learning,”</span> <em>Machine Learning</em>, vol. 8, no. 3, pp. 229–256, May 1992, doi: <a href="https://doi.org/10.1007/BF00992696">10.1007/BF00992696</a>.</div>
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./6_reinforcementBasic.html" class="pagination-link  aria-label=" &lt;span="" of="" reinforcement="" learning="" (rl)&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basics of Reinforcement Learning (RL)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./8_implementation.html" class="pagination-link" aria-label="<span class='chapter-number'>6</span>&nbsp; <span class='chapter-title'>Implementation</span>">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Implementation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>