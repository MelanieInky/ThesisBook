<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mélanie Fournier">

<title>ThesisBook - 7&nbsp; Implementation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./policyGradient.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ThesisBook</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./motivation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Motivation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testEqIntro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A test problem - Convection diffusion equation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solverExploration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Runge-Kutta solver applied to the test problem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcementBasic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Basics of reinforcement learning(RL).</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./policyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiment.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-linear-approximation-of-the-policy." id="toc-a-linear-approximation-of-the-policy." class="nav-link active" data-scroll-target="#a-linear-approximation-of-the-policy."><span class="toc-section-number">7.1</span>  A linear approximation of the policy.</a></li>
  <li><a href="#setting-up-the-reward." id="toc-setting-up-the-reward." class="nav-link" data-scroll-target="#setting-up-the-reward."><span class="toc-section-number">7.2</span>  Setting up the reward.</a></li>
  <li><a href="#application-of-the-reinforce-algorithm." id="toc-application-of-the-reinforce-algorithm." class="nav-link" data-scroll-target="#application-of-the-reinforce-algorithm."><span class="toc-section-number">7.3</span>  Application of the REINFORCE algorithm.</a></li>
  <li><a href="#exploration-vs-exploitation-tradeoff." id="toc-exploration-vs-exploitation-tradeoff." class="nav-link" data-scroll-target="#exploration-vs-exploitation-tradeoff."><span class="toc-section-number">7.4</span>  Exploration vs exploitation tradeoff.</a></li>
  <li><a href="#gradient-ascent-with-different-learning-parameters." id="toc-gradient-ascent-with-different-learning-parameters." class="nav-link" data-scroll-target="#gradient-ascent-with-different-learning-parameters."><span class="toc-section-number">7.5</span>  Gradient ascent with different learning parameters.</a></li>
  <li><a href="#impact-of-initial-condition." id="toc-impact-of-initial-condition." class="nav-link" data-scroll-target="#impact-of-initial-condition."><span class="toc-section-number">7.6</span>  Impact of initial condition.</a></li>
  <li><a href="#moving-beyond-the-basics." id="toc-moving-beyond-the-basics." class="nav-link" data-scroll-target="#moving-beyond-the-basics."><span class="toc-section-number">7.7</span>  Moving beyond the basics.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mélanie Fournier </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="a-linear-approximation-of-the-policy." class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="a-linear-approximation-of-the-policy."><span class="header-section-number">7.1</span> A linear approximation of the policy.</h2>
<p>We want to have a policy of the form <span class="math inline">\((\Delta t, \alpha) =A(b,n)' + \text{c}\)</span>, where <span class="math inline">\(A\)</span> is a two by two matrix and <span class="math inline">\(c\)</span> a 2-vector.</p>
<p>We define the following stochastic policy, define first</p>
<p><span class="math display">\[
\begin{pmatrix}
\mu_\alpha \\
\mu_{\Delta t}
\end{pmatrix} =
\begin{pmatrix}
\theta_0 &amp; \theta_1\\
\theta_2 &amp; \theta_3
\end{pmatrix}
\begin{pmatrix}
b\\
n
\end{pmatrix} +
\begin{pmatrix}
\theta_4\\
\theta_5
\end{pmatrix}
\]</span></p>
<p>Then we chose the random policy <span class="math inline">\(\alpha \sim \mathcal{N}(\mu_\alpha,\sigma^2)\)</span>,and similarly <span class="math inline">\(\Delta t \sim \mathcal{N}(\mu_{\Delta t}, \sigma^2)\)</span>. The term <span class="math inline">\(\sigma^2\)</span>, which is the variance of the policy is chosen fixed, and will help us balance exploration vs exploitation. Since <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\Delta t\)</span> are chosen independently, the joint probability density of both parameters is the product of both marginal pdf, that is</p>
<p><span class="math display">\[
f(\alpha,\Delta t) = f_{1}(\alpha)\cdot f_{2}(\Delta t)
\]</span></p>
<p>where <span class="math display">\[
f_1(\alpha) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}\right)
\]</span></p>
<p>and similarly,</p>
<p><span class="math display">\[
f_2(\Delta t) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)^2}{2\sigma^2}\right)
\]</span>.</p>
<p>Taking the logarithm, we get <span class="math inline">\(\ln(f(\alpha,\Delta t) = \ln(f_1(\alpha)) + \ln(f_2(\Delta t))\)</span>. Thus,</p>
<p><span class="math display">\[
\ln(f_1(\alpha)) = \ln(\frac{1}{\sqrt{2\pi}\sigma}) - \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)^2}{2\sigma^2}
\]</span>.</p>
<p>We now take the gradient w.r.t <span class="math inline">\(\theta\)</span> to get</p>
<p><span class="math display">\[
\nabla_\theta \ln(f_1(\alpha)) = \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T
\]</span></p>
<p>where <span class="math inline">\(\xi_\alpha = \frac{(\alpha - \theta_0b-\theta_1n-\theta_4)}{\sigma^2}\)</span>.</p>
<p>Doing a similar thing with <span class="math inline">\(\Delta t\)</span>, we get the gradient,</p>
<p><span class="math display">\[
\nabla_\theta \ln(f_2(\Delta t)) = \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\]</span></p>
<p>where <span class="math inline">\(\xi_{\Delta t} = \frac{(\Delta t - \theta_2b-\theta_3n-\theta_5)}{\sigma^2}\)</span> We now add both gradients together to we get the gradient of the policy, for a specific action <span class="math inline">\(a = (\alpha, \Delta t)\)</span> and state <span class="math inline">\(s=(b,n)\)</span>,</p>
<p><span id="eq-policyGradient"><span class="math display">\[
\nabla_\theta \ln\pi(a|s,\theta) =  \xi_\alpha (b\theta_0,n\theta_1,0,0,\theta_4,0)^T+ \xi_{\Delta t}(0,0,b\theta_2,n\theta_3,0,\theta_5)^T.
\tag{7.1}\]</span></span></p>
<p>We used here the standard notation. That is, <span class="math inline">\(\pi(a|s,\theta) = f(\alpha,\Delta t)\)</span>.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>One may remark that the REINFORCE algorithm uses a discrete policy space. This is not an issue. Instead of using the probability mass function of the policy, we will instead use the probability density function as a substitute(<span class="citation" data-cites="lillicrap2019continuous">Lillicrap et al. (<a href="references.html#ref-lillicrap2019continuous" role="doc-biblioref">2019</a>)</span>). The fact that the pdf admits values <span class="math inline">\(&gt;1\)</span> is not an issue as we adjust the learning rate accordingly.</p>
</div>
</section>
<section id="setting-up-the-reward." class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="setting-up-the-reward."><span class="header-section-number">7.2</span> Setting up the reward.</h2>
<p>Once a state and action is chosen, the reward need to be computed. We said before that we compute the residual ratio after 10 iterations <span class="math inline">\(c_{10}\)</span> for each state. With that ratio, we need to define an appropriate reward metrics. The reward is</p>
<p><span class="math display">\[
r(c_{10}) = \begin{cases}
100\times (1-c_{10}) \;\;\;\;\;\qquad \text{ if } c_{10}&lt;1\\
\max(-10,1 - c_{10}) \qquad \text{ if } c_{10}\geq1
\end{cases}
\]</span></p>
<p>When <span class="math inline">\(c_{10}&lt;1\)</span>, the reward is positive as we are currently converging, and the lower the ratio, the better the convergence and thus we want a better reward. Because the ratio tends to be very close to <span class="math inline">\(1\)</span>, we multiply everything by <span class="math inline">\(100\)</span>, adding more contrast to the rewards.</p>
<p>When, on the other hand <span class="math inline">\(c_{10}\geq 1\)</span>, the reward is negative as we are diverging. The higher the ratio, the lower the reward. As the ratio can get very big with very bad parameters, we cap the negative reward at <span class="math inline">\(-10\)</span>.</p>
</section>
<section id="application-of-the-reinforce-algorithm." class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="application-of-the-reinforce-algorithm."><span class="header-section-number">7.3</span> Application of the REINFORCE algorithm.</h2>
<p>The Reinforce algorithm is used in Here are the results, (Experiments will be rerun, those are placeholder graph)</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="experiment_files/figure-html/cell-2-output-1.png" width="589" height="429" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Evolution of the theta parameters with episode number. The hyperparameters are sigma=0.1, learning rate 5e-6….</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="experiment_files/figure-html/cell-3-output-1.png" width="587" height="429" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Evolution of the average reward inside the episode with episode number. The hyperparameters are sigma=0.1, learning rate 5e-6…. The variance is high! The trendline is a polynomial fit not to be trusted too much.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>there are problem with the results, namely</p>
<ul>
<li>The learning rate must be kept low, otherwise things explode nastily.</li>
<li>The convergence is extremely slow, to the point were we may be wondering if it converges at all</li>
</ul>
<p>(The next sections are things I’ve done to mitigate these problem, with varying degrees of successes, the log are there, but also I need to clean up my code and rerun it for reproducibility sake as I was tinkering a lot at the time, the goal is to have a program were each hyperparameter can be chosen at the beginning/as environment variables, and making all the experiments easily reproducible).</p>
</section>
<section id="exploration-vs-exploitation-tradeoff." class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="exploration-vs-exploitation-tradeoff."><span class="header-section-number">7.4</span> Exploration vs exploitation tradeoff.</h2>
<p>Changing <span class="math inline">\(\sigma^2\)</span> to a higher value makes for a flatter gradient, so not only are we taking more risks, we can actually afford a higher learning rate without exploding!</p>
</section>
<section id="gradient-ascent-with-different-learning-parameters." class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="gradient-ascent-with-different-learning-parameters."><span class="header-section-number">7.5</span> Gradient ascent with different learning parameters.</h2>
<p>Here is how we can get better convergence by changing the learning parameters.</p>
<p>One problem is that if we look at the gradient directions, how steep they are depend on the problem parameters. And since <span class="math inline">\(n\)</span> can vary between 5 and 300, this make for a muuuuuch steeper gradient than the direction associated with <span class="math inline">\(b\)</span>, which only varies between 0 and 1. As a result, the direction associated with <span class="math inline">\(n\)</span> tend to converge but not the other. This can be remedied by applying varying learning rate in each direction. It actually works quite well! ’</p>
</section>
<section id="impact-of-initial-condition." class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="impact-of-initial-condition."><span class="header-section-number">7.6</span> Impact of initial condition.</h2>
<p>Gradient based algorithm have a tendency to converge to local minima. (in our case maxima, but same thing really), therefore, it would be interesting to see how initial policy impacts the learned policy now that we have convergence. (Not done yet, but I suspect some nasty surprises!)</p>
</section>
<section id="moving-beyond-the-basics." class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="moving-beyond-the-basics."><span class="header-section-number">7.7</span> Moving beyond the basics.</h2>
<p>(Some discussion on what to do next, etc… ) Example include.</p>
<ul>
<li>Better algorithm, with less sample variance and more sample efficiency.</li>
<li>Moving beyond a linear policy. (approximation using NN).</li>
<li>Meta learning, maybe?</li>
<li>Applying the concept learned here to a more varied set of problem, instead of just confining ourselves to steady state diffusion-convection equation.</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-lillicrap2019continuous" class="csl-entry" role="doc-biblioentry">
Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. <span>“Continuous Control with Deep Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./policyGradient.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>