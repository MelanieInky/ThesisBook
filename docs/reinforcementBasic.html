<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mélanie Fournier">

<title>ThesisBook - 5&nbsp; Reinforcement learning - Basic</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./policyGradient.html" rel="next">
<link href="./solverExploration.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reinforcement learning - Basic</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ThesisBook</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./motivation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Motivation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testEqIntro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A test problem - Convection diffusion equation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solverExploration.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Runge Kutta solver applied to the test problem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcementBasic.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reinforcement learning - Basic</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./policyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiment.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Implementation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-non-mathematical-but-delicious-example." id="toc-a-non-mathematical-but-delicious-example." class="nav-link active" data-scroll-target="#a-non-mathematical-but-delicious-example."><span class="toc-section-number">5.1</span>  A non mathematical, but delicious example.</a></li>
  <li><a href="#finite-markov-decision-process" id="toc-finite-markov-decision-process" class="nav-link" data-scroll-target="#finite-markov-decision-process"><span class="toc-section-number">5.2</span>  Finite Markov decision process</a>
  <ul class="collapse">
  <li><a href="#bellman-equation" id="toc-bellman-equation" class="nav-link" data-scroll-target="#bellman-equation"><span class="toc-section-number">5.2.1</span>  Bellman equation</a></li>
  </ul></li>
  <li><a href="#bellman-optimality-equation" id="toc-bellman-optimality-equation" class="nav-link" data-scroll-target="#bellman-optimality-equation"><span class="toc-section-number">5.3</span>  Bellman optimality equation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Reinforcement learning - Basic</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mélanie Fournier </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>In this section, we outline the main ideas behind reinforcement learning and how they can be applied in the context of this thesis.</p>
<section id="a-non-mathematical-but-delicious-example." class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="a-non-mathematical-but-delicious-example."><span class="header-section-number">5.1</span> A non mathematical, but delicious example.</h2>
<p>Suppose we want to cook a delicious meal. At any point in time, we are making decisions such as</p>
<ul>
<li>which ingredients we use. Do we use tofu or seitan? Do we add spice more chili pepper? When do we incorporate the sauce?</li>
<li>which cookware we use? Cast iron, or non-stick pan?</li>
<li>whether to stir or not. It may stick and burn at the bottom of the pan if we don’t, but we are lazy and our laziness has to be weighted in.</li>
<li>Or simply do nothing!</li>
</ul>
<p>All of these decisions, which we will call <em>actions</em> from now on, are taken in reaction to the current <em>state</em> of the cooking process, following a certain <em>policy</em>, which is shaped by our previous cooking experience.</p>
<p>After each action, the cooking process get to a new <em>state</em> and we get a <em>reward</em> that depend on how we did. Maybe the food started to burn in which case we get a negative reward, or maybe we made the food better, in which case we get a positive reward. In this example, there is also a terminal state, in which we finished cooking and get to eat the meal.</p>
<p>But how do we learn how to cook, that is, how do we learn the <em>policy</em>? We learn it by trying to make the food as good as possible, which is defined by the <em>reward</em> we get after each action. Some of those rewards are immediate. For example, if we add some spices to our food and it tastes better, we may be inclined to do it again the next time we cook a meal. We want to have a <em>policy</em> that maximize the total <em>rewards</em> we get, which also mean that we have to balance our decision between the immediate reward and the future rewards. Adding a spice may make the meal taste better in the short term, but it may clash later when we add other ingredients, leading to a worse meal and bad <em>rewards</em>.</p>
<p>Each time we cook, we learn what works and what doesn’t, and remember that for the future time we cook. But, if we want to get better at cooking, we must not just repeat the <em>actions</em> that worked! We also have to take some risks, and <em>explore</em> the potential actions we can take at each state! On the other hand, we still need to rely and <em>exploit</em> what we know, so there is a balance between <em>exploitation</em> and <em>exploration</em> to find so we can learn as fast as possible.</p>
</section>
<section id="finite-markov-decision-process" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="finite-markov-decision-process"><span class="header-section-number">5.2</span> Finite Markov decision process</h2>
<p>Before introducing reinforcement learning, we first need to define a Markov decision process (MDP).</p>
<div id="def-Markov_decision_process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 </strong></span>(Markov decision process). A finite Markov decision process is defined as a discrete time process, where we have</p>
<ul>
<li>a state set <span class="math inline">\(\mathbfcal{S}\)</span>,</li>
<li>an action set <span class="math inline">\(\mathcal{A}\)</span>, containing all possible actions,</li>
<li>for each state and each action, we have a reward set <span class="math inline">\(\mathcal{R}(s,a)\)</span>, which contain the potential rewards received after taking action <span class="math inline">\(a\in\mathcal{A}\)</span> from the state <span class="math inline">\(s\in\mathcal{S}\)</span>.</li>
</ul>
<p>A Markov decision process has a model, which consist of</p>
<ul>
<li>the probability of getting from state <span class="math inline">\(s\)</span> to the state <span class="math inline">\(s'\)</span> by taking action <span class="math inline">\(a\)</span>, which we call the state transition probability <span class="math inline">\(p(s'|s,a) = P(S_{t+1} = s' | s_t = s, a_t = a)\)</span>.</li>
<li>the probability of getting reward <span class="math inline">\(r\)</span> by taking the action <span class="math inline">\(a\)</span> at a state <span class="math inline">\(s\)</span> <span class="math inline">\(p(r|s,a) = P(R_{t+1} = r | S_t = s, A_t = a)\)</span>.</li>
</ul>
<p>Furthermore, a policy function is also given that governs for a given state <span class="math inline">\(s\in\mathcal{S}\)</span>, the probability of taking action <span class="math inline">\(a\in\mathcal{A}\)</span>, that probability is <span class="math inline">\(\pi(a|s) = Pr(A_{t+1} = a|S_t = s)\)</span>.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>We have implicitly defined the random variables designing the state, action, reward at a time <span class="math inline">\(t\)</span>, those are respectively <span class="math inline">\(S_t,A_t,R_t\)</span>. A diagram of the process is as follow</p>
</div>
<p>(Here is a shiny diagram, I should learn tikz..)</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>The state space <span class="math inline">\(\mathcal{S}\)</span> and the action space <span class="math inline">\(\mathcal{A}\)</span> can be finite or not. We only consider the case of finite Markov decision process to make matter easier, with generalization only if necessary. This also mean that the model is finite.</p>
<p>The model in a MDP can be in practice impossible to define in advance. This is remedied by using so called <em>model free</em> reinforcement learning algorithms.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>Markov property applies, in particular lack of memory. (TODO)</p>
</div>
<div id="exm-fmdpex" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 </strong></span>(A more mathematical example, adorable)</p>
<p>(More or less a gridworld example to write about)</p>
</div>
<p>At this point, we may be able to roughly define how to translate the problem (TODO, should formulate it better so I can reference it).</p>
<ul>
<li>The state space is defined by the problem parameters <span class="math inline">\((b,n)\)</span>. A simplification that could be possible would be to discretize the state space.</li>
<li>Actions would be choosing the solver parameters <span class="math inline">\((\Delta t, \alpha)\)</span>. This can also be discretized.</li>
<li>Reward can be defined to be proportional to convergence rate after a certain number of iterations, where better convergence rate leads to better rewards.</li>
<li>The model can be partially described, in the sense that while we can’t model rewards, we can define the state transition probabilities by simply choosing a new state at random.</li>
</ul>
<section id="bellman-equation" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="bellman-equation"><span class="header-section-number">5.2.1</span> Bellman equation</h3>
<p>We first define a trajectory. We note as <span class="math inline">\(S_t\)</span> the state of an agent at instant <span class="math inline">\(t\)</span>. Then, according to the policy, this agent takes the action <span class="math inline">\(A_t\)</span>. After taking this action, the agent is now at the state <span class="math inline">\(S_{t+1}\)</span>, and it gets the rewards <span class="math inline">\(R_{t+1}\)</span>. Then the agent takes action <span class="math inline">\(A_{t+1}\)</span>, and gets to a new state <span class="math inline">\(S_{t+2}\)</span> with reward <span class="math inline">\(R_{t+2}\)</span>. This can continues indefinitely. We define the trajectory of an agent with starting state <span class="math inline">\(S_t = s\)</span> as the states-rewards pairs <span class="math inline">\({(S_{t+1},A_{t+1}),(S_{t+2},A_{t+2} ),\dots}\)</span>.</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display" data-execution_count="1">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="reinforcementBasic_files/figure-html/cell-2-output-1.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A helpful diagram showing trajectory, to be remade with tikz.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Ideally, we would like to chose a policy that aim to maximize rewards along any trajectory, given any starting state. This is the goal of any reinforcement learning algorithm. We now define the discounted return along a trajectory.</p>
<div id="def-discount" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 </strong></span>Let <span class="math inline">\(t = 0, 1, \dots\)</span>. The discounted return along the trajectory <span class="math inline">\({(S_{t+1},A_{t+1}),(S_{t+2},A_{t+2} ),\dots}\)</span> is the random variable given by</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\]</span></p>
<p>where <span class="math inline">\(\gamma \in (0,1)\)</span> is called the discount rate.</p>
</div>
<p>The discounted return is thus the sum of rewards along a trajectory. The discount rate is chosen depending on whether we want the agent to favor short term rewards, in which case a discount rate closer to <span class="math inline">\(0\)</span> can be chosen, or long term rewards, with a discount rate closer to <span class="math inline">\(1\)</span>.</p>
<p>Since the discount rate is a random variable, we can look at its expectation, in particular, we are interested in its conditional expectation, given a starting state <span class="math inline">\(S_t = s\)</span>. This expectation is called the state value.</p>
<div id="def-state_value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 </strong></span>(State value) The state value of a state <span class="math inline">\(s\)</span> is the function, defined for any <span class="math inline">\(s\in\mathcal{S}\)</span> as</p>
<p><span class="math display">\[
v_\pi(s) = E[G_t|S_t = s] = E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t]
\]</span></p>
<p>where <span class="math inline">\(\pi\)</span> is a given policy.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>The Markov property of the MDP means that the state value does not depend on time.</p>
</div>
<p>The objective is thus to find a policy <span class="math inline">\(\pi\)</span> that maximizes the state values. We next derive the Bellman equation.</p>
<p>It is first apparent that</p>
<p><span class="math display">\[\begin{align}   
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \nonumber\\
&amp;=R_{t+1} + \gamma \left(R_{t+2} + \gamma R_{t+3}+ \dots \right) \nonumber\\
&amp;=R_{t+1} + \gamma G_{t+1}
\end{align}\]</span></p>
<p>Inputting this into the state value yields</p>
<p><span class="math display">\[
v_\pi(s) = E[G_t|S_t = s]= E[R_{t+1}| S_t = s] + \gamma E[G_{t+1} | S_t = s]
\]</span></p>
<p>The first term is the expectation of immediate reward, following a certain policy <span class="math inline">\(\pi\)</span>, the second is the expectation of future rewards. Let us expand on that formula a bit more. We use the law of total expectation on the first part of the RHS to get</p>
<p><span class="math display">\[
E[R_{t+1}| S_t = s] = E[E[R_{t+1}|S,A]] = \sum_{a\in\mathcal{A}}\pi(a,s)\sum_{r\in\mathcal{R}}rp(r|s,a)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{R} = \mathcal{R}(s,a)\)</span> is the set of possible rewards one can get by taking action <span class="math inline">\(a\)</span> at state <span class="math inline">\(s\)</span>.</p>
<p>We now develop the second part of the RHS of the equation to get,</p>
<p><span class="math display">\[
E[G_{t+1} | S_t = s] = E[E[G_{t+1} | S_t = s , S_{t+1}]] = \sum_{s'\in\mathcal{S}}E[G_{t+1}|S_t = s, S_{t+1} = s']p(s'|s)
\]</span></p>
<p>where <span class="math inline">\(p(s'|s) = \sum_{a\in\mathcal{A}} p(s'|s,a)\pi(a,s)\)</span> is the probability of the next state being <span class="math inline">\(s'\)</span> if the current state is <span class="math inline">\(s\)</span>. Because of the Markov property of the MDP, we can remove the conditioning <span class="math inline">\(S_t = s\)</span> and thus, <span class="math inline">\(E[G_{t+1}|S_t = s, S_{t+1} = s'] = E[G_{t+1}|S_{t+1} = s] = v_\pi(s')\)</span>. Then <span class="math display">\[
E[G_{t+1} | S_t = s] = \sum_{s'\in\mathcal{S}}\sum_{a\in\mathcal{A}}v_\pi(s')\pi(a|s)p(s'|s,a).
\]</span></p>
<p>Putting everything together, we get a first form of Bellman equation for (finite) MDP.</p>
<p><span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a,s)\left[\sum_{r\in\mathcal{R}}rp(r|s,a) + \gamma\sum_{s'\in\mathcal{S}}v_\pi(s')p(s'|s,a) \right]
\]</span></p>
<p>Some remarks</p>
<ul>
<li>The Bellman equation gives a recursive relation for the state values. Solving this equation is called policy evaluation and involves fixed point iterations, which we will not get into details here.</li>
<li>This equation is valid for a given policy.</li>
</ul>
<p>The expression between the brackets is called the action value <span class="math inline">\(q_\pi(s,a)\)</span>. Bellman’s equation is then simply</p>
<p><span class="math display">\[
v_\pi(s) = \sum_{a\in\mathcal{A}}\pi(a,s)q_\pi(s,a) = E_\pi[q]
\]</span></p>
<p>(///TODO)</p>
</section>
</section>
<section id="bellman-optimality-equation" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="bellman-optimality-equation"><span class="header-section-number">5.3</span> Bellman optimality equation</h2>
<p>Given some state value, one may ask the fundamental question.</p>
<p>(test <span class="citation" data-cites="reinforcementBookShiyuZhao">Zhao (<a href="references.html#ref-reinforcementBookShiyuZhao" role="doc-biblioref">2023</a>)</span> , <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="references.html#ref-Sutton1998" role="doc-biblioref">2018</a>)</span> , <span class="citation" data-cites="Williams1992">Williams (<a href="references.html#ref-Williams1992" role="doc-biblioref">1992</a>)</span>, <span class="citation" data-cites="lillicrap2019continuous">Lillicrap et al. (<a href="references.html#ref-lillicrap2019continuous" role="doc-biblioref">2019</a>)</span>).</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-lillicrap2019continuous" class="csl-entry" role="doc-biblioentry">
Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. <span>“Continuous Control with Deep Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="doc-biblioentry">
Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press. <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-Williams1992" class="csl-entry" role="doc-biblioentry">
Williams, Ronald J. 1992. <span>“Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.”</span> <em>Machine Learning</em> 8 (3): 229–56. <a href="https://doi.org/10.1007/BF00992696">https://doi.org/10.1007/BF00992696</a>.
</div>
<div id="ref-reinforcementBookShiyuZhao" class="csl-entry" role="doc-biblioentry">
Zhao, Shiyu. 2023. <span>“Mathematical Foundations of Reinforcement Learning.”</span> 2023. <a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./solverExploration.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Runge Kutta solver applied to the test problem</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./policyGradient.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy gradient methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>