---
author: "MÃ©lanie Fournier"
jupyter: python3
---

# Explicit Runge-Kutta Method

## A small introdution to explicit Runge-Kutta methods

This section aim to introduce explicit Runge-Kutta methods[@NumDiffEqBook, chap. 3], and the particular Runge-Kutta method used in this thesis. We consider solving a generic initial value problem of the form

$$
y'(t) = f(t,y(t)), \quad y(0) = y_0.
$$

If we know, for an instant $t_n$ the value for $y(t_n)$, we can compute the value of $y$ at instant $t_{n+1} = t_n +\Delta t$ by integrating

$$
y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}}f(u,y(u))\; du
$$

and with the change of variable $u = t_n + \Delta t\tau$, we have

$$
y(t_{n+1}) = y(t_n) + \Delta t\int_0^1f(t_n+h\tau,y(t_n+h\tau)) \; d\tau
$$
The problem is finding a suitable way to compute the integral above. An elementary approach is to use the current value of $f(t_n,y(t_n))$ and to treat $f$ as constant, thus defining the sequence

$$
y_{n+1} = y_n + \Delta tf(t_n,y_n)
$$

where $y_{n} \approx y(t_{n})$, $y_0 = y(0)$. This is the explicit Euler's method. We now want to exploit quadrature formulas for integration. Let $c_j \in [0,1], j=1,2,\nu$ be the nodes in the quadrature formula, with their associated weight $b_j, j=1,2,\dots$. A quadrature formula for the integral is then of the form

$$
\int_0^1 f(t_n+h\tau,y(t_n+\Delta t\tau))\; d\tau = \sum_{j=1}^\nu b_j f(t_n + \Delta t c_j,y(t_n+\Delta t c_j)).
$$

This is all well and good, except that we have to know the values $y(t_n+hc_j)$, which we do not know. We can however, play pretend and compute an approximation of these values $\xi_j \approx y(t_n+\Delta t c_j), j=1,\dots, \nu$. The $\xi_j$ are called *stage values*. [@BirkenNotes], and we compute them using a linear combination of the values $f(t_n + \Delta t c_j, \xi_j)$. That is 

$$
\xi_i = y_n + \Delta t \sum_{j=1}^\nu a_{ij}f(t_n+\Delta t c_j, \xi_j),
$$

for $i = 1,\dots, \nu$, where the $a_{ij}$ are some well chosen values, which is not in scope of this thesis. To simplify notation, we note $A$ as the square array containing the $a_{ij}$ parameters, that is $A_{ij} = a_{ij}$, $c = (c_1,\dots,c_\nu)^\intercal$ the vector of nodes, and $b = (b_1,\dots, b_\nu)^\intercal$ the vector of weights. A RK methods is then written in the form of the following array, also called Butcher tableau.  

$$
\renewcommand\arraystretch{1.2}
\begin{array}
{c|c}
c & A\\
\hline & b^\intercal
\end{array}
$$



We remark that if, for for any $j\geq i$,  $a_{ij} \neq 0$, then we will need to know $\xi_j$ to compute $\xi_i$, which involves solving an equation, making the method *implicit*. We consider here *explicit* methods, where we can compute $\xi_{i+1}$ if we know $\xi_j, j = 1 , \dots , i-1$. Let $a_{11} = 0$ and $c_1 = 0$. An explicit RK method is then of the form 

$$
y_{n+1} = y_n + h\sum_{j=1}^\nu b_j f(t_n+\Delta t c_j,\xi_j),
$$

where the stage derivative $\xi_j$ are computed sequentially as follow


\begin{align*}
\xi_1 &= y_n\\
\xi_2 &= y_n + \Delta t a_{2,1}f(t_n,\xi_1)\\
\xi_3 &= y_n + \Delta t a_{3,1}f(t_n,\xi_1) +  \Delta t a_{3,2}f(t_n+\Delta t c_2,\xi_2)\\
\vdots\\
\xi_\nu &= y_n + \Delta t \sum_{j=1}^{\nu-1} a_{\nu,j}f(t_n + \Delta t c_j,\xi_j)
\end{align*}



## Application to the test problem


We now have to solve the ODE $u'(t) = e - Mu(t)$ where $M$ depends on the problem parameters $b$ and $\Delta x = 1 / (n+1)$, and $n$ is the chosen number of subdivisions of $[0,1]$. Since we are only interested on the asymptotic behavior of $u$, we only need to care about the stability of the numerical solver we wish to use. We consider in this thesis the following RK method with two stages.

$$
\begin{array}
{c|cc}
0 & &\\
 \alpha & \alpha & \\
\hline & 0 & 1
\end{array}
$$




This solver has two parameters, namely the (pseudo) time step $\Delta t$ and $\alpha$, where $\alpha \in [0,1]$. 

The goal is for the solver to converge to a steady state solution as fast as possible. We set $u_0 = u(0) = e$ as an initial value. We define the relative residual after $k$ steps as 

$$
r_k = ||Mu_k - e_s||/ ||e||.
$$

where $||.||$ is the 2-norm.

If the solver we chose is stable, then $||r_k|| \to 0$ as $k \to \infty$. We define now the convergence at step $n$ to be the ratio of residual at step $k$ and $k-1$. That is 

$$
\rho_k = \frac{||r_k||}{||r_{k-1}||} = \frac{||Mu_k - e||}{||Mu_{k-1}-e||}
$${#eq-residual_ratio_equation}

where $||.||$ is the 2-norm. @fig-residual_ratio_evolution shows the evolution of the relative residual, as well as the residual ratio for specifics parameters. 




```{python}
#| echo: false
#| output: false

import numpy as np
import matplotlib.pyplot as plt
from testProblemClass import TestProblem
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import plotly.graph_objects as go
import os

if not os.path.exists("images"):
    os.mkdir("images")

```



```{python}
#| fig-cap: Evolution of the residual norm over iteration, with problem parameters $n = 50$ and $b = 0.05$, and RK parameters $\Delta t = 1$ and $\alpha = 0.2$. 
#| echo: false
#| label: fig-residual_ratio_evolution

n = 50
b = 0.05
alpha = 0.2
deltaT = 1
problem = TestProblem(b = b,n = n)

y ,  resNormList = problem.main_solver2(alpha,deltaT,1000)

e_norm = np.linalg.norm(problem.e,2)
relRes = resNormList / e_norm

resRatio = resNormList[1:]/resNormList[:-1]

fig, (ax1, ax2) = plt.subplots(2,1)
ax1.plot(relRes)
ax1.set_yscale('log')
ax1.set_ylabel('Relative Residual $r_n$')
ax1.set_xlabel('Iteration')

ax2.plot(resRatio)
ax2.set_ylabel('Residual ratio')
ax2.set_xlabel('Iteration')
plt.show()
```


:::{.remark}
Using the same notation as before for the stage values, for the equation $u'(t) = f(t,u(t))$, we have $\xi_1 = u_n$

$$
\xi_2 = u_n + \Delta t \alpha f(t_n,\xi_1) = u_n + \Delta t \alpha f(t_n,u_n)
$$

The update is thus

$$
u_{n+1} = u_n + \Delta t f(t_n+\alpha \Delta t, \xi_2)
$$

In the test problem case, $f(t_n,u_n) = e - Mu_n$, and we get the update

$$
u_{n+1} = u_n + \Delta t\left[e-M(u_n+\alpha(e-Mu_n))\right].
$$

After a few lines of computation, we get the following iteration

$$
u_{n+1} = \left[I-\Delta t M - \alpha(\Delta t M)^2\right]u_n + \Delta t e - \alpha \Delta t Me.
$$

We recognize a fixed point iteration of the form $u_{n+1} = g(u_n)$, which converges to a unique fixed point $u^*$ if $g$ is a contraction mapping, and the convergence rate depends on its Lipschitz constant. From this point, it is apparent that optimizing the solver parameters involves minimizing this Lipschitz constant. Furthermore, at the potential fixed point

$$
u^* = \left[I-\Delta t M - \alpha(\Delta t M)^2\right]u^* + \Delta t e - \alpha \Delta t Me.
$$

This is equivalent after rearranging the terms to 

$$
(1+ \alpha \Delta t M)(e-Mu^*) = 0.
$$

Clearly, if $M$ is non singular, we can simply chose $u^* = M^{-1}e$ to solve this equation. Since the fixed point is unique, this is what the solver converges to. 
:::

## A small experiment

We are interested in finding the best parameters $(\Delta t, \alpha)$ to use for some specific problem parameters $(b, n)$. Since the residual ratio vary quite a bit depending on the number of iteration, we decide to investigate the residual ratio after 10 iterations and 100 iterations. So, for the problem parameters $b = 0.05$, and $n = 100$, we plot $\rho_{10 }= f(\Delta t, \alpha)$ and $\rho_{100} = g(\Delta t, \alpha)$. We wish to answer the following questions

- Where are the optimal parameters for this specific problem, that is, the ones that minimize $\rho_{10}$ and $\rho_{100}$, and do they also depend on the iteration number or not. 
- What do these functions look like. In particular, we may be interested in the function convexity.


In both cases, we use a contour plot. In @fig-resRatio10 and @fig-resRatio100, the residual ratio is clipped when it is $\geq 1$ so as to maximize contrast.

```{python}
#| echo: false
#| output: false
#| eval: false


def resRatio(resNormList):
    return resNormList[-1] / resNormList[-2]


b = 0.05
n = 100
## Remake the problem, just in case
problem = TestProblem(b = b,n = n)

##Make an l*l grid to search in
l = 100
deltaTgrid = np.linspace(0.1,5,l)
alphaGrid = np.linspace(0,1,l)

deltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)

resRatioGrid2 = np.zeros((l,l))

#Version 10th residual ratio
for i in range(l):
    for j in range(l):
        #print('alpha', alphaGrid[j,0])
        #print('deltaT', deltaTgrid[0,i])
        y , resNormList = problem.main_solver2(alphaGrid[j,i],deltaTgrid[j,i],10)
        ratio = resRatio(resNormList)
        #print('ratio', ratio)
        resRatioGrid2[j,i] = resRatio(resNormList)


##VISUALISATION

###Removing the margins
layout = go.Layout(
  margin=go.layout.Margin(
        l=0, #left margin
        r=0, #right margin
        b=0, #bottom margin
        t=0, #top margin
    )
)

fig10 = go.Figure(data = 
    go.Contour(
        z = np.clip(resRatioGrid2,0.5,1),
        y = alphaGrid[:,0],
        x = deltaTgrid[0,:],
        colorbar=dict(
            title='Residual ratio, after 10 iter.', # title here
            titleside='right',
            titlefont=dict(
            size=14,
            family='Arial, sans-serif'))
    ), layout = layout)

fig10 = fig10.update_layout(
    xaxis_title = 'Delta t',
    yaxis_title = 'alpha'
)


###NOT RUN because it takes a while to compile otherwise
#### Version 100 #####
for i in range(l):
    for j in range(l):
        #Solve using the parameters
        y , resNormList = problem.main_solver2(alphaGrid[j,i],deltaTgrid[j,i],100)
        ratio = resRatio(resNormList)
        #print('ratio', ratio)
        resRatioGrid2[j,i] = resRatio(resNormList)

fig100 = go.Figure(data = 
    go.Contour(
        z = np.clip(resRatioGrid2,0.5,1),
        y = alphaGrid[:,0],
        x = deltaTgrid[0,:],
        colorbar=dict(
            title='Residual ratio, after 100 iter.', # title here
            titleside='right',
            titlefont=dict(
            size=14,
            family='Arial, sans-serif'))
    ), layout = layout)

fig100.update_layout(
    xaxis_title = 'Delta t',
    yaxis_title = 'alpha'
)

#Saving
fig10.write_image("images/res_ratio10.pdf", width = 350, height = 200)
fig100.write_image("images/res_ratio100.pdf", width=350, height = 200)
```


::: {#fig-elephants layout-ncol=2}

![$\rho_{10}$](images/res_ratio10.pdf){#fig-res_ratio10 height=200}

![$\rho_{100}$](images/res_ratio100.pdf){#fig-res_ratio100 height=200}

Contour plot of two residual ratio $\rho_k$ after different number of iterations, for specifics problem parameters. The parameters are $n = 100$ and $b=0.05$. 

:::


The stability region after 100 iterations is more narrow, suggesting that convergence may not hold even if it seems to hold for the first few iterations. Nevertheless, we can see how the solver parameters interact with the residual ratio. 

This is of course an exploration of particular problem parameters, and it is not advisable in practice to compute the optimal parameters with a grid search. We thus explore a possible solution to this problem by using a reinforcement learning algorithm to "learn" these optimal parameters.

