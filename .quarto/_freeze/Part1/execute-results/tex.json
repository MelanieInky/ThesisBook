{
  "hash": "c21f679ce08e10ee6d431057327d0c47",
  "result": {
    "markdown": "---\ntitle: Testing ground for bachelor thesis\nauthor: MÃ©lanie Fournier\nformat:\n  html:\n    code-fold: true\n  pdf:\n    geometry:\n      - top=30mm\n      - left=20mm\n    number-sections: true\n    include-in-header:\n      text: |\n        \\usepackage{amsmath}\n        \\usepackage{easy-todo}\n        \\usepackage{amsthm}\n    documentclass: article\n    fontsize: '14'\n---\n\n# Explicit RK2 and stability function\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass testProblem:\n## Define it as \n    def __init__(self,b,n) -> None:\n        self.n = n\n        self.b = b\n        self.deltaX = 1 / (n+1)\n        self.M = self.buildM(b,n,self.deltaX)\n        self.e = self.buildE(n, self.deltaX)\n\n\n    def buildM(self,b,n,deltaX):\n        \"\"\"\n        we go from u0 to u(n+1).\n        \"\"\"\n        deltaX = 1 / (n+1)\n        A = deltaX *(np.eye(n) -1 * np.eye(n,k = -1))\n        B = b* (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))\n        return A-B\n    \n    def buildE(self,n,deltaX):\n        return deltaX**2 *np.ones(n)\n    \n    def f(self,y):\n        return self.e - self.M@y\n    \n    def oneStepSmoother(self,y,t,deltaT,alpha):\n        \"\"\"\n        Perform one pseudo time step deltaT of the solver for the diff eq\n        y' = e - My = f(y). .\n        \"\"\"\n        k1 = self.f(y)\n        k2 = self.f(y + alpha*deltaT*k1)\n        yNext = y + deltaT*k2\n        return yNext\n    \n    def findOptimalParameters(self):\n        #This is where the reinforcement learning algorithm \n        #take place in\n        return 0 , 0\n    \n\n    def mainSolver(self,n_iter = 10):\n        \"\"\" Main solver for the problem, calculate the approximated solution\n        after n_iter pseudo time steps. \"\"\"\n        resNormList = np.zeros(n_iter+1)\n        t = 0\n        #Initial guess y = e\n        y = np.ones(e)\n        resNormList[0] = np.linalg.norm(self.M@y-self.e)\n        ##Finding the optimal params\n        alpha, deltaT = self.findOptimalParameters()\n        ##Will need to be removed, just for debugging\n        alpha = 0.5\n        deltaT = 0.00006\n        #For now, we use our best guess\n        for i in range(n_iter):\n            y = self.oneStepSmoother(y,t,deltaT,alpha)\n            t += deltaT\n            resNorm = np.linalg.norm(self.M@y - self.e)\n            resNormList[i+1] = resNorm\n        return y , resNormList\n\n    def mainSolver2(self,alpha, deltaT, n_iter = 10):\n        \"\"\" Like the main solver, except we give \n        the parameters explicitely \"\"\"\n        resNormList = np.zeros(n_iter+1)\n        t = 0\n        #Initial guess y = e\n        y = np.ones(n)\n        resNormList[0] = np.linalg.norm(self.M@y-self.e)\n        #For now, we use our best guess\n        for i in range(n_iter):\n            y = self.oneStepSmoother(y,t,deltaT,alpha)\n            t += deltaT\n            resNorm = np.linalg.norm(self.M@y - self.e)\n            resNormList[i+1] = resNorm\n        return y , resNormList\n\n```\n:::\n\n\nWe now have everything we need to get going, let's plot the residual norm over iteration as a first test\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#Create the object\nb = 0.5\nn = 100\n\nalpha = 0.13813813813813813\ndeltaT = 3.5143143143143143\nconvDiffProb = testProblem(b,n)\ny, resNormList = convDiffProb.mainSolver2(0.093093,5.6003,20)\n\nx = np.linspace(0,1,n+2) #Create space\nyTh = np.zeros(n+2)\nyTh[1:n+1] = np.linalg.solve(convDiffProb.M,convDiffProb.e)\n\nyApprox = np.zeros(n+2)\nyApprox[1:n+1] = y\nfig, (ax1,ax2) = plt.subplots(1,2)\n\nax1.plot(resNormList)\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Residual norm\")\nax1.set_yscale('log')\n\nax2.plot(x,yTh,label = 'Discretised solution')\nax2.plot(x,yApprox,label = \"iterative solution\")\nax2.legend()\n\nfig.show()\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_5673/2529022355.py:27: UserWarning:\n\nMatplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Evolution of the residual norm over a number of iteration.](Part1_files/figure-pdf/cell-3-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\ndef resRatio(resNormList):\n    return resNormList[-1] / resNormList[-2]\n\n\n\nl = 100\ndeltaTgrid = np.linspace(0.000001,0.00001,l)\nalphaGrid = np.linspace(0,1,l)\n\ndeltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)\n\nresRatioGrid2 = np.zeros((l,l))\n\nfor i in range(l):\n    for j in range(l):\n        #print('alpha', alphaGrid[j,0])\n        #print('deltaT', deltaTgrid[0,i])\n        y , resNormList = convDiffProb.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)\n        ratio = resRatio(resNormList)\n        #print('ratio', ratio)\n        resRatioGrid2[j,i] = resRatio(resNormList)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n\nclippedRatio = np.clip(resRatioGrid2,0.8,0.9)\nsurf = ax.contour(deltaTgrid,alphaGrid,clippedRatio,levels = [0.8,0.85,0.9])\n\ntransformedContour = np.log(1/(1+np.exp(-clippedRatio+1)))\n\n\n\nprint(np.nanmin(resRatioGrid2))\nprint(np.argmin(resRatioGrid2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9999898995194059\n99\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_5673/1941845570.py:30: UserWarning:\n\nNo contour levels were found within the data range.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-4-output-3.pdf){fig-pos='H'}\n:::\n:::\n\n\nContour plot\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\ncp = ax.contour(deltaTgrid,alphaGrid,resRatioGrid2,levels = [0.83,0.86,0.88,0.9,1], cmap=cm.coolwarm, linewidth=0)\nax.clabel(cp)\n#ax.view_init(elev = 90,azim = 150)\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_5673/383841379.py:2: UserWarning:\n\nNo contour levels were found within the data range.\n\n/tmp/ipykernel_5673/383841379.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'linewidth'\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-5-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nSurface plot\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(deltaTgrid,alphaGrid,np.clip(resRatioGrid2,0.5,1), cmap=cm.coolwarm, linewidth=0)\nax.view_init(elev = 90,azim = 180)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-6-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n# Make data.\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y)\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R)\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\n# Customize the z axis.\nax.set_zlim(-1.01, 1.01)\nax.zaxis.set_major_locator(LinearLocator(10))\n# A StrMethodFormatter is used automatically\nax.zaxis.set_major_formatter('{x:.02f}')\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-7-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport sys\nprint(sys.executable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/home/melanie/anaconda3/bin/python\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nNecessary functions go here.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef RK2(f,y,t,deltaT,alpha,**args):\n    \"\"\"Second order family of Rk2\n    c = [0,alpha], bT = [1-1/(2alpha), 1/(2alpha)] , a2,1 = alpha \"\"\"\n    k1 = f(t,y,**args)\n    k2 = f(t + alpha*deltaT, y + alpha*deltaT*k1,**args)\n    yNext = y + deltaT*(k1*(1-1/(2*alpha)) + k2 * 1/(2*alpha))\n    return yNext\n    \ndef buildM(b,n):\n    \"\"\"\n    we go from u0 to u(n+1).\n    \"\"\"\n    deltaX = 1 / (n+1)\n    A = 1/deltaX *(np.eye(n) -1 * np.eye(n,k = -1))\n    B = b/deltaX**2 * (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))\n    return A-B\n\ndef buildE(n):\n    return np.ones(n)\n\ndef f(t,y,M,e):\n    return e - M@y\n\n\ndef mainSolver(deltaT, alpha,b,f = f,n_iter = 10,n_points=100):\n    t = 0\n    e = buildE(n_points)\n    M = buildM(b,n_points)\n    #First guess\n    y = np.copy(e)\n    resNorm = np.linalg.norm(M@y -e)\n\n    for i in range(n_iter):\n        y = RK2(f,y,t,deltaT,alpha,M = M,e = e)\n        t += deltaT\n        lastResNorm , resNorm = resNorm ,  np.linalg.norm(M@y - e)\n    return resNorm / lastResNorm\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmainSolver(0.0001,0.5,0.5)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.9678775609609744\n```\n:::\n:::\n\n\nTo facilitate everything, we discretise the space with 100 interior points only, and with parameter $b = 0.5$.\n\n\n\nThis is how the solution looks like with the discretisation\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nb = 0.5\nn = 100\n\nM = buildM(b,n)\ne = buildE(n)\n\nx = np.linspace(0,1,n+2)\nx2 = np.linspace(0,1,n)\n\nanalyticSol = x - (np.exp(-(1-x)/b)-np.exp(-1/b))/(1-np.exp(-1/b))\nu = np.linalg.solve(M,e)\n\nplt.plot(x,analyticSol,label = 'Analytical solution')\nplt.plot(x2,u,label = 'Discretised solution')\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<matplotlib.legend.Legend at 0x7f69d6f22700>\n```\n\nDiscretised solution vs analytical solution\n:::\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-12-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nHow would changing the parameters affect the residuals ratio after 10 iterations?\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndeltaTGrid = np.linspace(0.00001,0.0001,100)\n\nratio = np.zeros(100)\ni = 0\nfor deltaT in deltaTGrid:\n    ratio[i] = mainSolver(deltaT,0.6,0.5)\n    i+=1\n\nplt.plot(deltaTGrid,ratio)\nplt.xlabel('Delta T')\nplt.ylabel('Ratio')\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nText(0, 0.5, 'Ratio')\n```\n\nImpact of the choice of time step with the residual ratios.\n:::\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-13-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nHow would changing the RK parameter change the residual ratio after 10 iterations? Here we take the optimal delta T we found earlier.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n#fig-cap: Changing alpha does not do much...\nalphaGrid = np.linspace(0.01,0.99,100)\n\nratio = np.zeros(100)\ni = 0\nfor alpha in alphaGrid:\n    ratio[i] = mainSolver(0.00007,alpha,0.5)\n    i+=1\n\nplt.plot(alphaGrid,ratio)\nplt.xlabel('alpha')\nplt.ylabel('Ratio')\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nText(0, 0.5, 'Ratio')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Part1_files/figure-pdf/cell-14-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nPendulum test\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef f(t,y):\n    g = 9.81\n    l = 1\n    f1 = y[1]\n    f2 = -g/l* np.sin(y[0])\n    return np.array([f1,f2])\n\n\n\n#Pendulum\ndeltaT = 0.01\nt_min = 0\nn = 1000\nt = t_min\ntArray = np.zeros(n+1)\ntArray[0] = t\ny = np.array([np.pi/2,0])\nyArray = np.zeros((n+1,2))\nyArray[0] = y\nfor i in range(n):\n    y = RK2(f,y,t,deltaT,0.9)\n    t+=deltaT\n    tArray[i+1] = t\n    yArray[i+1] = y\n\n\nplt.plot(tArray,yArray[:,0])\n```\n:::\n\n\n",
    "supporting": [
      "Part1_files/figure-pdf"
    ],
    "filters": []
  }
}