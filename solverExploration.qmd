---
title: "Runge Kutta solver applied to the test problem"
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
        include-in-header: 
            text: |
                \usepackage{amsmath}
                \usepackage{easy-todo}
                \usepackage{amsthm}
        documentclass: article
        fontsize: "14"
jupyter: python3
---


We now have to solve the ODE $u' = b - Nu$ where $M$ depends on the problem parameters $b$ and $\Delta x = 1 / (n+1)$, where $n$ is the chosen number of subdivisions of $[0,1]$. Since we are only interested on the asymptotic behavior of $u$, we only need to care about the stability of the numerical solver we wish to use. We consider the following RK scheme with two stages.


blablbal


This solver has two parameters $\Delta t$ and $\alpha$. The objective is for the solver to converge to a steady state solution as fast as possible. Set $u_0 = u(0) = e$ as an initial value. We define the relative residual after $k$ steps as 

$$
r_k = ||Nu_k - b||/ ||b||.
$$

where $||.||$ is the 2-norm.

If the solver we chose is stable, then $||r_k|| \to 0$ as $k \to \infty$. We define now the convergence at step $n$ to be the ratio of residual at step $k$ and $k-1$. That is 

$$
c_k = \frac{||r_k||}{||r_{k-1}||} = \frac{||Mu_k - e||}{||Mu_{k-1}-e||}
$$

where $||.||$ is the 2-norm. 


```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from testProblemClass import testProblem
```



```{python}
#| fig-cap: Evolution of the residual norm over iteration, with problem parameters $n = 100$ and $b = 0.05$, and RK parameters $\Delta t = 1.6$ and $\alpha = 0.3$. 
#| echo: false

n = 100
b = 0.05
alpha = 0.3
deltaT = 1.65
problem = testProblem(b,n)

y ,  resNormList = problem.mainSolver2(alpha,deltaT,2000)

e_norm = np.linalg.norm(problem.e,2)
relRes = resNormList / e_norm

resRatio = resNormList[1:]/resNormList[:-1]

fig, (ax1, ax2) = plt.subplots(2,1)
ax1.plot(relRes)
ax1.set_yscale('log')
ax1.set_ylabel('Relative Residual $r_n$')
ax1.set_xlabel('Iteration')

ax2.plot(resRatio)
ax2.set_ylabel('Residual ratio')
ax2.set_xlabel('Iteration')
plt.show()
```


 # A small experiment.

We are interested in finding the best parameters $(\Delta t, \alpha)$ to use for some specific problem parameters $(b, n)$. Since the residual ratio vary quite a bit depending on the number of iteration, we decide to investigate the residual ratio after 10 iterations and 100 iterations. So, for the problem parameters $b = 0.05$, and $n = 100$, we plot $c_{10 }= f(\Delta t, \alpha)$ and $c_{100} = g(\Delta t, \alpha)$. We wish to answer the following questions

- Where are the optimal parameters for this specific problem, that is, the ones that minimize $c_{10}$ and $c_{100}$, and do they also depend on the iteration number or not. 
- What do these functions look like. In particular, we may be interested in the function convexity.


In both cases, we use a contour plot. In @fig-resRatio10 and @fig-resRatio100, the residual ratio is clipped when it is $\geq 1$ so as to maximize contrast.

```{python}
#| echo: false

from matplotlib import cm
from matplotlib.ticker import LinearLocator

def resRatio(resNormList):
    return resNormList[-1] / resNormList[-2]



l = 100
deltaTgrid = np.linspace(0.1,4.3,l)
alphaGrid = np.linspace(0,1,l)

deltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)

resRatioGrid2 = np.zeros((l,l))

#Version 10
for i in range(l):
    for j in range(l):
        #print('alpha', alphaGrid[j,0])
        #print('deltaT', deltaTgrid[0,i])
        y , resNormList = problem.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)
        ratio = resRatio(resNormList)
        #print('ratio', ratio)
        resRatioGrid2[j,i] = resRatio(resNormList)




```


```{python}
#| fig-cap: Residual ratio at 10 iterations, for problem parameters $b = 0.05$ and $n = 100$, and with varying solver parameters $\alpha$ and $\Delta t$
#| label: fig-resRatio10
from IPython.display import Image
Image('images/c10.png')

```

```{python}
#| fig-cap: Residual ratio at 100 iterations, for problem parameters $b = 0.05$ and $n = 100$, and with varying solver parameters $\alpha$ and $\Delta t$
#| label: fig-resRatio100
Image('images/c100.png')
```
```{python}
#| echo: false
#| eval: false
import plotly.graph_objects as go

fig = go.Figure(data = 
    go.Contour(
        z = np.clip(resRatioGrid2,0.5,1),
        y = alphaGrid[:,0],
        x = deltaTgrid[0,:],
        colorbar=dict(
            title='Residual ratio, after 10 iter.', # title here
            titleside='right',
            titlefont=dict(
            size=14,
            family='Arial, sans-serif'))
    ))

fig.update_layout(
    xaxis_title = 'Delta t',
    yaxis_title = 'alpha'
)

```


```{python}

#| echo: false
#| eval: false

#Version 100
for i in range(l):
    for j in range(l):
        #print('alpha', alphaGrid[j,0])
        #print('deltaT', deltaTgrid[0,i])
        y , resNormList = problem.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],100)
        ratio = resRatio(resNormList)
        #print('ratio', ratio)
        resRatioGrid2[j,i] = resRatio(resNormList)

fig = go.Figure(data = 
    go.Contour(
        z = np.clip(resRatioGrid2,0.5,1),
        y = alphaGrid[:,0],
        x = deltaTgrid[0,:],
        colorbar=dict(
            title='Residual ratio, after 100 iter.', # title here
            titleside='right',
            titlefont=dict(
            size=14,
            family='Arial, sans-serif'))
    ))

fig.update_layout(
    xaxis_title = 'Delta t',
    yaxis_title = 'alpha'
)

```

The stability region after 100 iterations is more narrow, suggesting that convergence may not hold even if it seems to hold for the first few iterations. Nevertheless, we can see how the parameters act on the function. 

This is of course an exploration of a particular problem, and it makes no sense in practice to compute the optimal parameters with a grid search. We thus explore a possible solution by using a reinforcement learning algorithm to "learn" these optimal parameters. 

