@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}


@book{BellmanStab,
	author = {Bellman, Richard Ernest,},
	title = {Stability theory of differential equations /},
	publisher = {McGraw-Hill,},
	year = {1953},
	address = {New York :}
}

@online{reinforcementBookShiyuZhao,
  author = {Shiyu Zhao},
  title = {Mathematical Foundations of Reinforcement Learning.},
  year = 2023,
  url = {https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning},
  urldate = {2023-03-30}
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}


﻿@Article{Williams1992,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={229-256},
abstract={This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}


@misc{lillicrap2019continuous,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}




@article{DBLP:journals/corr/SchulmanWDRK17,
  author       = {John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov},
  title        = {Proximal Policy Optimization Algorithms},
  journal      = {CoRR},
  volume       = {abs/1707.06347},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.06347},
  eprinttype    = {arXiv},
  eprint       = {1707.06347},
  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{NumDiffEqBook,
	author = {Iserles, Arieh},
	title = {A first course in the numerical analysis of differential equations /},
	publisher = {Cambridge University Press,},
	year = {2009},
	series = {Cambridge texts in applied mathematics},
	address = {Cambridge ;},
	edition = {2. ed.}
}


@misc{BirkenNotes,
  author = {Birken, Philipp},
  title = {Numerical methods for stiff problems},
  year = {2022},
  howpublished = {Lecture Notes}
}

@article{gradient_descent_gradient_descent,
  author       = {Marcin Andrychowicz and
                  Misha Denil and
                  Sergio Gomez Colmenarejo and
                  Matthew W. Hoffman and
                  David Pfau and
                  Tom Schaul and
                  Nando de Freitas},
  title        = {Learning to learn by gradient descent by gradient descent},
  journal      = {CoRR},
  volume       = {abs/1606.04474},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.04474},
  eprinttype    = {arXiv},
  eprint       = {1606.04474},
  timestamp    = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/AndrychowiczDGH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{Fawzi2022,
author={Fawzi, Alhussein
and Balog, Matej
and Huang, Aja
and Hubert, Thomas
and Romera-Paredes, Bernardino
and Barekatain, Mohammadamin
and Novikov, Alexander
and R. Ruiz, Francisco J.
and Schrittwieser, Julian
and Swirszcz, Grzegorz
and Silver, David
and Hassabis, Demis
and Kohli, Pushmeet},
title={Discovering faster matrix multiplication algorithms with reinforcement learning},
journal={Nature},
year={2022},
month={Oct},
day={01},
volume={610},
number={7930},
pages={47-53},
abstract={Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems---from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4{\thinspace}{\texttimes}{\thinspace}4 matrices in a finite field, where AlphaTensor's algorithm improves on Strassen's two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor's ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
issn={1476-4687},
doi={10.1038/s41586-022-05172-4},
url={https://doi.org/10.1038/s41586-022-05172-4}
}



@misc{towardsdatascienceReinforcementLearning,
	author = {Chris Mahoney},
	title = {Reinforcement Learning},
  subtitle = {A Review of the Historic, Modern, and Future Applications of this Special Form of Machine Learning},
	howpublished = {\url{https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6}},
	year = {2021},
	note = {[Accessed 03-May-2023]}
}

﻿@Article{Silver2016,
author={Silver, David
and Huang, Aja
and Maddison, Chris J.
and Guez, Arthur
and Sifre, Laurent
and van den Driessche, George
and Schrittwieser, Julian
and Antonoglou, Ioannis
and Panneershelvam, Veda
and Lanctot, Marc
and Dieleman, Sander
and Grewe, Dominik
and Nham, John
and Kalchbrenner, Nal
and Sutskever, Ilya
and Lillicrap, Timothy
and Leach, Madeleine
and Kavukcuoglu, Koray
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go with deep neural networks and tree search},
journal={Nature},
year={2016},
month={Jan},
day={01},
volume={529},
number={7587},
pages={484-489},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
issn={1476-4687},
doi={10.1038/nature16961},
url={https://doi.org/10.1038/nature16961}
}


@book{convection_diffusion_book,
	author = {Abdon Atangana},
	title = {Fractional Operators with Constant and Variable Order with Application to Geo-hydrology},
	publisher = {Academic Press,},
	year = {2018},
	note = {Book},
	url = {https://ludwig.lub.lu.se/login?url=https://www.sciencedirect.com/science/book/9780128096703}
}


@book{bellman1957dynamic,
  title={Dynamic Programming},
  author={Bellman, R. and Bellman, R.E. and Rand Corporation},
  lccn={lc57005444},
  series={Rand Corporation research study},
  url={https://books.google.se/books?id=rZW4ugAACAAJ},
  year={1957},
  publisher={Princeton University Press}
}

@book{ODE_book,
	author = {Adkins, William A. and Davidson, Mark G. and SpringerLink (Online service)},
	title = {Ordinary Differential Equations},
	publisher = {Springer New York :},
	year = {2012},
	series = {Undergraduate Texts in Mathematics,},
	address = {New York, NY :},
	url = {http://dx.doi.org/10.1007/978-1-4614-3618-8}
}


@book{iterative_method_book,
	author = {Hackbusch, Wolfgang. and SpringerLink (Online service)},
	title = {Iterative Solution of Large Sparse Systems of Equations /},
	publisher = {Springer International Publishing :},
	year = {2016},
	series = {Applied Mathematical Sciences,},
	address = {Cham :},
	edition = {2nd ed. 2016.},
	url = {http://dx.doi.org/10.1007/978-3-319-28483-5}
}

@book{Rl_finance_book,
	author = {Rao, Ashwin and Jelvis, Tikhon},
	title = {Foundations of Reinforcement Learning with Applications in Finance},
	year = {2022},
	note = {Book},
	url = {https://ludwig.lub.lu.se/login?url=https://www.taylorfrancis.com/books/9781003229193}
}
