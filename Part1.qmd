---
title: "Testing ground for bachelor thesis"
author: "MÃ©lanie Fournier"
format: 
    html:
        code-fold: true
    pdf:
        geometry: 
        - top=30mm
        - left=20mm
        number-sections: true
        include-in-header: 
            text: |
                \usepackage{amsmath}
                \usepackage{easy-todo}
                \usepackage{amsthm}
        documentclass: article
        fontsize: "14"
jupyter: python3
---


# Explicit RK2 and stability function


```{python}
import numpy as np
import matplotlib.pyplot as plt

class testProblem:
## Define it as 
    def __init__(self,b,n) -> None:
        self.n = n
        self.b = b
        self.deltaX = 1 / (n+1)
        self.M = self.buildM(b,n,self.deltaX)
        self.e = self.buildE(n, self.deltaX)


    def buildM(self,b,n,deltaX):
        """
        we go from u0 to u(n+1).
        """
        deltaX = 1 / (n+1)
        A = deltaX *(np.eye(n) -1 * np.eye(n,k = -1))
        B = b* (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))
        return A-B
    
    def buildE(self,n,deltaX):
        return deltaX**2 *np.ones(n)
    
    def f(self,y):
        return self.e - self.M@y
    
    def oneStepSmoother(self,y,t,deltaT,alpha):
        """
        Perform one pseudo time step deltaT of the solver for the diff eq
        y' = e - My = f(y). .
        """
        k1 = self.f(y)
        k2 = self.f(y + alpha*deltaT*k1)
        yNext = y + deltaT*k2
        return yNext
    
    def findOptimalParameters(self):
        #This is where the reinforcement learning algorithm 
        #take place in
        return 0 , 0
    

    def mainSolver(self,n_iter = 10):
        """ Main solver for the problem, calculate the approximated solution
        after n_iter pseudo time steps. """
        resNormList = np.zeros(n_iter+1)
        t = 0
        #Initial guess y = e
        y = np.ones(e)
        resNormList[0] = np.linalg.norm(self.M@y-self.e)
        ##Finding the optimal params
        alpha, deltaT = self.findOptimalParameters()
        ##Will need to be removed, just for debugging
        alpha = 0.5
        deltaT = 0.00006
        #For now, we use our best guess
        for i in range(n_iter):
            y = self.oneStepSmoother(y,t,deltaT,alpha)
            t += deltaT
            resNorm = np.linalg.norm(self.M@y - self.e)
            resNormList[i+1] = resNorm
        return y , resNormList

    def mainSolver2(self,alpha, deltaT, n_iter = 10):
        """ Like the main solver, except we give 
        the parameters explicitely """
        resNormList = np.zeros(n_iter+1)
        t = 0
        #Initial guess y = e
        y = np.ones(n)
        resNormList[0] = np.linalg.norm(self.M@y-self.e)
        #For now, we use our best guess
        for i in range(n_iter):
            y = self.oneStepSmoother(y,t,deltaT,alpha)
            t += deltaT
            resNorm = np.linalg.norm(self.M@y - self.e)
            resNormList[i+1] = resNorm
        return y , resNormList
        
```

We now have everything we need to get going, let's plot the residual norm over iteration as a first test

```{python}
#| fig-cap: Evolution of the residual norm over a number of iteration.

#Create the object
b = 0.5
n = 100

alpha = 0.13813813813813813
deltaT = 3.5143143143143143
convDiffProb = testProblem(b,n)
y, resNormList = convDiffProb.mainSolver2(0.093093,5.6003,20)

x = np.linspace(0,1,n+2) #Create space
yTh = np.zeros(n+2)
yTh[1:n+1] = np.linalg.solve(convDiffProb.M,convDiffProb.e)

yApprox = np.zeros(n+2)
yApprox[1:n+1] = y
fig, (ax1,ax2) = plt.subplots(1,2)

ax1.plot(resNormList)
ax1.set_xlabel("Iteration")
ax1.set_ylabel("Residual norm")
ax1.set_yscale('log')

ax2.plot(x,yTh,label = 'Discretised solution')
ax2.plot(x,yApprox,label = "iterative solution")
ax2.legend()

fig.show()


```

```{python}
from matplotlib import cm
from matplotlib.ticker import LinearLocator

def resRatio(resNormList):
    return resNormList[-1] / resNormList[-2]



l = 100
deltaTgrid = np.linspace(0.000001,0.00001,l)
alphaGrid = np.linspace(0,1,l)

deltaTgrid, alphaGrid = np.meshgrid(deltaTgrid,alphaGrid)

resRatioGrid2 = np.zeros((l,l))

for i in range(l):
    for j in range(l):
        #print('alpha', alphaGrid[j,0])
        #print('deltaT', deltaTgrid[0,i])
        y , resNormList = convDiffProb.mainSolver2(alphaGrid[j,i],deltaTgrid[j,i],10)
        ratio = resRatio(resNormList)
        #print('ratio', ratio)
        resRatioGrid2[j,i] = resRatio(resNormList)

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})


clippedRatio = np.clip(resRatioGrid2,0.8,0.9)
surf = ax.contour(deltaTgrid,alphaGrid,clippedRatio,levels = [0.8,0.85,0.9])

transformedContour = np.log(1/(1+np.exp(-clippedRatio+1)))



print(np.nanmin(resRatioGrid2))
print(np.argmin(resRatioGrid2))

```
Contour plot
```{python}
fig, ax = plt.subplots()
cp = ax.contour(deltaTgrid,alphaGrid,resRatioGrid2,levels = [0.83,0.86,0.88,0.9,1], cmap=cm.coolwarm, linewidth=0)
ax.clabel(cp)
#ax.view_init(elev = 90,azim = 150)
plt.show()


```
Surface plot
```{python}
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
ax.plot_surface(deltaTgrid,alphaGrid,np.clip(resRatioGrid2,0.5,1), cmap=cm.coolwarm, linewidth=0)
ax.view_init(elev = 90,azim = 180)
plt.show()
```
```{python}

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
```

```{python}
import sys
print(sys.executable)
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
```


Necessary functions go here.


```{python}
def RK2(f,y,t,deltaT,alpha,**args):
    """Second order family of Rk2
    c = [0,alpha], bT = [1-1/(2alpha), 1/(2alpha)] , a2,1 = alpha """
    k1 = f(t,y,**args)
    k2 = f(t + alpha*deltaT, y + alpha*deltaT*k1,**args)
    yNext = y + deltaT*(k1*(1-1/(2*alpha)) + k2 * 1/(2*alpha))
    return yNext
    
def buildM(b,n):
    """
    we go from u0 to u(n+1).
    """
    deltaX = 1 / (n+1)
    A = 1/deltaX *(np.eye(n) -1 * np.eye(n,k = -1))
    B = b/deltaX**2 * (-2*np.eye(n) + np.eye(n, k = -1) + np.eye(n,k=1))
    return A-B

def buildE(n):
    return np.ones(n)

def f(t,y,M,e):
    return e - M@y


def mainSolver(deltaT, alpha,b,f = f,n_iter = 10,n_points=100):
    t = 0
    e = buildE(n_points)
    M = buildM(b,n_points)
    #First guess
    y = np.copy(e)
    resNorm = np.linalg.norm(M@y -e)

    for i in range(n_iter):
        y = RK2(f,y,t,deltaT,alpha,M = M,e = e)
        t += deltaT
        lastResNorm , resNorm = resNorm ,  np.linalg.norm(M@y - e)
    return resNorm / lastResNorm
```

```{python}
mainSolver(0.0001,0.5,0.5)
```

To facilitate everything, we discretise the space with 100 interior points only, and with parameter $b = 0.5$.



This is how the solution looks like with the discretisation
```{python}
#| fig-cap: Discretised solution vs analytical solution

b = 0.5
n = 100

M = buildM(b,n)
e = buildE(n)

x = np.linspace(0,1,n+2)
x2 = np.linspace(0,1,n)

analyticSol = x - (np.exp(-(1-x)/b)-np.exp(-1/b))/(1-np.exp(-1/b))
u = np.linalg.solve(M,e)

plt.plot(x,analyticSol,label = 'Analytical solution')
plt.plot(x2,u,label = 'Discretised solution')
plt.legend()
```

How would changing the parameters affect the residuals ratio after 10 iterations?


```{python}
#| fig-cap: Impact of the choice of time step with the residual ratios. 

deltaTGrid = np.linspace(0.00001,0.0001,100)

ratio = np.zeros(100)
i = 0
for deltaT in deltaTGrid:
    ratio[i] = mainSolver(deltaT,0.6,0.5)
    i+=1

plt.plot(deltaTGrid,ratio)
plt.xlabel('Delta T')
plt.ylabel('Ratio')

```

How would changing the RK parameter change the residual ratio after 10 iterations? Here we take the optimal delta T we found earlier.

```{python}
#fig-cap: Changing alpha does not do much...
alphaGrid = np.linspace(0.01,0.99,100)

ratio = np.zeros(100)
i = 0
for alpha in alphaGrid:
    ratio[i] = mainSolver(0.00007,alpha,0.5)
    i+=1

plt.plot(alphaGrid,ratio)
plt.xlabel('alpha')
plt.ylabel('Ratio')
```



Pendulum test
```{python}
#| output: false
def f(t,y):
    g = 9.81
    l = 1
    f1 = y[1]
    f2 = -g/l* np.sin(y[0])
    return np.array([f1,f2])



#Pendulum
deltaT = 0.01
t_min = 0
n = 1000
t = t_min
tArray = np.zeros(n+1)
tArray[0] = t
y = np.array([np.pi/2,0])
yArray = np.zeros((n+1,2))
yArray[0] = y
for i in range(n):
    y = RK2(f,y,t,deltaT,0.9)
    t+=deltaT
    tArray[i+1] = t
    yArray[i+1] = y


plt.plot(tArray,yArray[:,0])
```